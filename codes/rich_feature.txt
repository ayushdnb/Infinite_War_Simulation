# Aggregated Python sources
# Base: C:\Kishan\RL_Project\rich_feature
# Generated: 2026-02-15 13:09:06
# Total files: 35
--------------------------------------------------------------------------------


====[ 1/35 | war_simulation\__init__.py ]====================================================

====[ END war_simulation\__init__.py ]============================================================


====[ 2/35 | war_simulation\agent\__init__.py ]==============================================
# war_simulation/agent/__init__.py
from .transformer_brain import TransformerBrain, scripted_transformer_brain
from .tron_brain import TronBrain
from .mirror_brain import MirrorBrain

__all__ = [
    "TransformerBrain",
    "scripted_transformer_brain",
    "TronBrain",
    "MirrorBrain",
]

====[ END war_simulation\agent\__init__.py ]============================================================


====[ 3/35 | war_simulation\agent\ensemble.py ]==============================================
# final_war_sim/agent/ensemble.py
from __future__ import annotations
from typing import List, Tuple, Optional, Any
import torch
import torch.nn as nn
import config

try:
    # PyTorch 2.x path
    from torch.func import functional_call, vmap, stack_module_state
except Exception:
    functional_call = None
    vmap = None
    stack_module_state = None


class _DistWrap:
    """Lightweight container to mimic a torch.distributions object with logits."""
    def __init__(self, logits: torch.Tensor) -> None:
        self.logits = logits


_VMAP_WARNED: bool = False

def _is_torchscript_module(m: nn.Module) -> bool:
    # TorchScript modules don't reliably work with torch.func.functional_call/stack_module_state.
    return isinstance(m, torch.jit.ScriptModule) or isinstance(m, torch.jit.RecursiveScriptModule)

def _maybe_debug(msg: str) -> None:
    if bool(getattr(config, "VMAP_DEBUG", False)):
        print(msg)

def _maybe_warn_once(msg: str) -> None:
    global _VMAP_WARNED
    if _VMAP_WARNED:
        return
    _VMAP_WARNED = True
    _maybe_debug(msg)


@torch.no_grad()
def _ensemble_forward_loop(models: List[nn.Module], obs: torch.Tensor) -> Tuple[_DistWrap, torch.Tensor]:
    """
    Original safe loop implementation (kept as canonical fallback).
    """
    device = obs.device
    K = int(obs.size(0)) if obs.dim() > 0 else 0
    if K == 0:
        return _DistWrap(logits=torch.empty((0, 0), device=device)), torch.empty((0,), device=device)

    logits_out: List[torch.Tensor] = []
    values_out: List[torch.Tensor] = []
    for i, model in enumerate(models):
        o = obs[i].unsqueeze(0)  # (1,F)
        out = model(o)
        if not (isinstance(out, tuple) and len(out) == 2):
            raise RuntimeError("Brain.forward must return (logits_or_dist, value)")
        head, val = out
        logits = head.logits if hasattr(head, "logits") else head  # (1,A)
        v = val.view(-1)  # (1,)
        logits_out.append(logits)
        values_out.append(v)

    logits_cat = torch.cat(logits_out, dim=0)  # (K,A)
    values_cat = torch.cat(values_out, dim=0)  # (K,)
    if values_cat.dim() == 0:
        values_cat = values_cat.unsqueeze(0)
    # Invariants
    if logits_cat.dim() != 2 or logits_cat.size(0) != K:
        raise RuntimeError(f"ensemble_forward loop: logits shape invalid: got {tuple(logits_cat.shape)}, K={K}")
    if values_cat.dim() != 1 or values_cat.size(0) != K:
        raise RuntimeError(f"ensemble_forward loop: values shape invalid: got {tuple(values_cat.shape)}, K={K}")
    return _DistWrap(logits=logits_cat), values_cat


@torch.no_grad()
def _ensemble_forward_vmap(models: List[nn.Module], obs: torch.Tensor) -> Tuple[_DistWrap, torch.Tensor]:
    """
    vmap-based inference across *independent* parameter sets.
    - NO parameter sharing (each agent has its own weights).
    - NO optimizer sharing (inference-only path).
    """
    if functional_call is None or vmap is None or stack_module_state is None:
        raise RuntimeError("torch.func is not available in this PyTorch build")

    K = int(obs.size(0))
    if K == 0:
        return _DistWrap(logits=torch.empty((0, 0), device=obs.device)), torch.empty((0,), device=obs.device)

    # For safety, avoid TorchScript modules
    if any(_is_torchscript_module(m) for m in models):
        raise RuntimeError("TorchScript module in bucket (vmap disabled)")

    # stack_module_state expects identical module structure across the list.
    base = models[0]
    # Create batched params/buffers where leading dim is K
    params_batched, buffers_batched = stack_module_state(models)

    # Ensure obs has a batch dimension aligned with K
    x = obs
    if x.dim() != 2 or x.size(0) != K:
        raise RuntimeError(f"vmap: obs must be (K,F). got {tuple(x.shape)} expected K={K}")

    # Define per-model forward: takes (params_i, buffers_i, x_i) and returns (logits_i, value_i)
    def _f(params_i: Any, buffers_i: Any, x_i: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        # model expects (1,F)
        out = functional_call(base, (params_i, buffers_i), (x_i.unsqueeze(0),))
        if not (isinstance(out, tuple) and len(out) == 2):
            raise RuntimeError("Brain.forward must return (logits_or_dist, value)")
        head, val = out
        logits = head.logits if hasattr(head, "logits") else head  # (1,A)
        logits = logits.squeeze(0)  # (A,)
        # Return scalar value (so vmap stacks to (K,))
        v = val.view(-1)[0]
        return logits, v

    logits_KA, values_K = vmap(_f, in_dims=(0, 0, 0))(params_batched, buffers_batched, x)

    # Invariants
    if logits_KA.dim() != 2 or logits_KA.size(0) != K:
        raise RuntimeError(f"vmap: logits shape invalid: got {tuple(logits_KA.shape)}, K={K}")
    if values_K.dim() != 1 or values_K.size(0) != K:
        raise RuntimeError(f"vmap: values shape invalid: got {tuple(values_K.shape)}, K={K}")

    return _DistWrap(logits=logits_KA), values_K


@torch.no_grad()
def ensemble_forward(models: List[nn.Module], obs: torch.Tensor) -> Tuple[_DistWrap, torch.Tensor]:
    """
    Fuses per-agent models for a bucket into one batched tensor of outputs.
    Args:
      models: list of nn.Module, length K
      obs:    (K, F) observation batch aligned with models ordering
    Returns:
      - dist-like object with .logits -> (K, A)
      - values tensor -> (K,)    (NEVER 0-dim)
    Contract:
      Each model.forward(x: (1,F)) -> (logits: (1,A)) or (dist_with_logits, value)
    """
    K = int(obs.size(0)) if obs.dim() > 0 else 0
    if K == 0:
        return _DistWrap(logits=torch.empty((0, 0), device=obs.device)), torch.empty((0,), device=obs.device)

    use_vmap = bool(getattr(config, "USE_VMAP", False))
    min_bucket = int(getattr(config, "VMAP_MIN_BUCKET", 8))

    # vmap path is inference-only; this function is already @no_grad
    if use_vmap and K >= min_bucket:
        # Quick structural checks before attempting vmap:
        # - requires torch.func
        # - avoid TorchScript modules
        # - bucket should be homogeneous architecture (already intended by build_buckets)
        if functional_call is None or vmap is None or stack_module_state is None:
            _maybe_warn_once("[vmap] torch.func not available; falling back to loop")
        elif any(_is_torchscript_module(m) for m in models):
            _maybe_debug("[vmap] TorchScript module detected; falling back to loop")
        else:
            try:
                return _ensemble_forward_vmap(models, obs)
            except Exception as e:
                _maybe_debug(f"[vmap] vmap path failed ({type(e).__name__}: {e}); falling back to loop")

    return _ensemble_forward_loop(models, obs)
====[ END war_simulation\agent\ensemble.py ]============================================================


====[ 4/35 | war_simulation\agent\mirror_brain.py ]==========================================
from __future__ import annotations

from typing import Dict, Tuple, List, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

import config
from . import obs_spec


# ---------------------------------------------------------------------
# MirrorBrain: two-pass (propose -> reflect/edit) policy/value network.
#
# Hard invariants:
# - Input obs is the SAME flat vector and split layout as TronBrain.
# - Output contract matches PPO runtime: (logits, value) with shapes:
#     logits: (B, NUM_ACTIONS), value: (B, 1)
# ---------------------------------------------------------------------


class _SelfAttnBlock(nn.Module):
    """Pre-LN self-attention block (Transformer encoder style)."""

    def __init__(self, d_model: int, n_heads: int) -> None:
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.norm2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # LN in fp32 for stability, then cast back for projections (AMP-friendly).
        x1 = self.norm1(x.float()).to(dtype=x.dtype)
        a, _ = self.attn(x1, x1, x1, need_weights=False)
        x = x + a
        x2 = self.norm2(x.float()).to(dtype=x.dtype)
        x = x + self.ff(x2)
        return x


class _CrossAttnBlock(nn.Module):
    """Pre-LN cross-attention block: queries from x attend to memory m."""

    def __init__(self, d_model: int, n_heads: int) -> None:
        super().__init__()
        self.norm_q = nn.LayerNorm(d_model)
        self.norm_m = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.norm2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model),
        )

    def forward(self, x: torch.Tensor, m: torch.Tensor) -> torch.Tensor:
        q = self.norm_q(x.float()).to(dtype=x.dtype)
        kv = self.norm_m(m.float()).to(dtype=m.dtype)
        a, _ = self.attn(q, kv, kv, need_weights=False)
        x = x + a
        x2 = self.norm2(x.float()).to(dtype=x.dtype)
        x = x + self.ff(x2)
        return x


class MirrorBrain(nn.Module):
    """
    MirrorBrain = Tron-family tokenization + propose logits/value + small reflection edit.

    PASS 1 (PROPOSE):
      - Encode rays (self-attn over ray tokens)
      - Build plan tokens: 3 decision + 5 semantic + 1 instinct + 1 memory = 10
      - Self-attn over plan tokens
      - Cross-attn fusion: plan tokens attend to rays
      - Readout from decision tokens -> logits_proposal, value_proposal

    PASS 2 (REFLECT + EDIT):
      - Build 1 reflection token REF from internal state only:
          mean(decision tokens), entropy/margin(logits_proposal), value_proposal
      - REF cross-attends to plan tokens and ray tokens (small)
      - Output delta_logits and delta_value (both initialized ~0)
      - logits_final = logits_proposal + delta_logits
        value_final  = value_proposal  + delta_value
    """

    def __init__(self, obs_dim: int, act_dim: int) -> None:
        super().__init__()
        self.obs_dim = int(obs_dim)
        self.act_dim = int(act_dim)

        # --- Observation layout MUST match obs_spec / TronBrain exactly ---
        self.num_rays = int(getattr(config, "RAY_TOKEN_COUNT", 32))
        self.ray_feat_dim = int(getattr(config, "RAY_FEAT_DIM", 4))
        self.rays_flat_dim = self.num_rays * self.ray_feat_dim

        self.rich_base_dim = int(getattr(config, "RICH_BASE_DIM", 64))
        self.instinct_dim = int(getattr(config, "INSTINCT_DIM", 4))

        expected_obs_dim = self.rays_flat_dim + self.rich_base_dim + self.instinct_dim
        cfg_obs_dim = int(getattr(config, "OBS_DIM", expected_obs_dim))
        if cfg_obs_dim != expected_obs_dim:
            raise RuntimeError(
                f"[MirrorBrain] OBS layout mismatch: expected {expected_obs_dim} from "
                f"RAY_TOKEN_COUNT*RAY_FEAT_DIM + RICH_BASE_DIM + INSTINCT_DIM, but config.OBS_DIM={cfg_obs_dim}"
            )
        if self.obs_dim != expected_obs_dim:
            raise RuntimeError(
                f"[MirrorBrain] obs_dim mismatch: ctor obs_dim={self.obs_dim} but expected {expected_obs_dim} (must match Tron/obs_spec)."
            )

        # --- Model hyperparams (reuse Tron defaults for consistent compute budget) ---
        d_model = int(getattr(config, "MIRROR_D_MODEL", getattr(config, "TRON_D_MODEL", 64)))
        n_heads = int(getattr(config, "MIRROR_HEADS", getattr(config, "TRON_HEADS", 4)))

        # Depth controls
        ray_layers = int(getattr(config, "MIRROR_RAY_LAYERS", getattr(config, "TRON_RAY_LAYERS", 4)))
        plan_layers = int(getattr(config, "MIRROR_SEM_LAYERS", getattr(config, "TRON_SEM_LAYERS", 3)))
        fusion_layers = int(getattr(config, "MIRROR_FUSION_LAYERS", getattr(config, "TRON_FUSION_LAYERS", 2)))

        # Head MLP capacity
        mlp_hidden = int(getattr(config, "MIRROR_MLP_HIDDEN", getattr(config, "TRON_MLP_HIDDEN", 256)))

        self.d_model = d_model

        if d_model % n_heads != 0:
            raise RuntimeError(f"[MirrorBrain] d_model must be divisible by n_heads (d_model={d_model}, n_heads={n_heads}).")

        # --- PASS 1: Embeddings ---
        # Rays: (B, N, RAY_FEAT_DIM) -> (B, N, D)
        self.ray_in_norm = nn.LayerNorm(self.ray_feat_dim)
        self.ray_in_proj = nn.Linear(self.ray_feat_dim, d_model)

        # Optional learnable ray direction embedding (kept identical to Tron for parity)
        self.ray_dir_embed = nn.Parameter(torch.randn(1, self.num_rays, d_model) * 0.02)

        # Semantic partitions from rich_base (5 tokens)
        self.sem_in_norm = nn.ModuleDict()
        self.sem_in_proj = nn.ModuleDict()

        sem_keys = ["own_context", "world_context", "zone_context", "team_context", "combat_context"]
        sem_idx_map: Dict[str, List[int]] = dict(getattr(config, "SEMANTIC_RICH_BASE_INDICES", {}))
        for k in sem_keys:
            idxs = sem_idx_map.get(k, None)
            if idxs is None or len(idxs) == 0:
                raise RuntimeError(f"[MirrorBrain] Missing/empty semantic index list for '{k}' in SEMANTIC_RICH_BASE_INDICES.")
            din = int(len(idxs))
            self.sem_in_norm[k] = nn.LayerNorm(din)
            self.sem_in_proj[k] = nn.Linear(din, d_model)

        # Instinct embedding (INSTINCT_DIM -> D)
        self.instinct_in_norm = nn.LayerNorm(self.instinct_dim)
        self.instinct_in_proj = nn.Linear(self.instinct_dim, d_model)

        # Learnable plan tokens
        self.memory_token = nn.Parameter(torch.zeros(1, 1, d_model))
        self.decision_tokens = nn.Parameter(torch.randn(1, 3, d_model) * 0.02)

        # --- PASS 1: Encoders / Fusion ---
        self.ray_encoder = nn.ModuleList([_SelfAttnBlock(d_model, n_heads) for _ in range(ray_layers)])
        self.plan_encoder = nn.ModuleList([_SelfAttnBlock(d_model, n_heads) for _ in range(plan_layers)])
        self.fusion = nn.ModuleList([_CrossAttnBlock(d_model, n_heads) for _ in range(fusion_layers)])

        # --- PASS 1: Readout (decision tokens only) ---
        self.read_fc0 = nn.Linear(3 * d_model, mlp_hidden)
        self.read_fc1 = nn.Linear(mlp_hidden, mlp_hidden)
        self.actor = nn.Linear(mlp_hidden, self.act_dim)
        self.critic = nn.Linear(mlp_hidden, 1)

        # --- PASS 2: Reflection token builder (internal-state only) ---
        # REF input: mean(decision tokens) + [entropy, top1_margin, value]
        self.ref_in_norm = nn.LayerNorm(d_model + 3)
        self.ref_in_proj = nn.Linear(d_model + 3, d_model)

        # REF attends to plan tokens and ray tokens (keep this small: 2 cross-attn blocks)
        self.ref_attend_plan = _CrossAttnBlock(d_model, n_heads)
        self.ref_attend_rays = _CrossAttnBlock(d_model, n_heads)

        # Delta heads (residual edits) â€” init near zero for stable start.
        self.delta_fc0 = nn.Linear(d_model, mlp_hidden)
        self.delta_fc1 = nn.Linear(mlp_hidden, mlp_hidden)
        self.delta_actor = nn.Linear(mlp_hidden, self.act_dim)
        self.delta_critic = nn.Linear(mlp_hidden, 1)

        self._init_weights()

        # Make PASS2 edits start at ~0 so behavior initially matches propose-only.
        nn.init.zeros_(self.delta_actor.weight)
        if self.delta_actor.bias is not None:
            nn.init.zeros_(self.delta_actor.bias)
        nn.init.zeros_(self.delta_critic.weight)
        if self.delta_critic.bias is not None:
            nn.init.zeros_(self.delta_critic.bias)

    def _init_weights(self) -> None:
        """Xavier-uniform Linear weights, zero biases (match TronBrain convention)."""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    # ---- embedding helpers (stable + AMP-friendly) ----
    def _embed_rays(self, rays_raw: torch.Tensor) -> torch.Tensor:
        x = self.ray_in_norm(rays_raw.float())
        x = self.ray_in_proj(x.to(dtype=self.ray_in_proj.weight.dtype))
        return x

    def _embed_sem(self, x_raw: torch.Tensor, key: str) -> torch.Tensor:
        n = self.sem_in_norm[key]
        p = self.sem_in_proj[key]
        x = n(x_raw.float())
        x = p(x.to(dtype=p.weight.dtype))
        return x

    def _embed_instinct(self, inst_raw: torch.Tensor) -> torch.Tensor:
        x = self.instinct_in_norm(inst_raw.float())
        x = self.instinct_in_proj(x.to(dtype=self.instinct_in_proj.weight.dtype))
        return x

    @staticmethod
    def _logits_entropy_and_margin(logits: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Cheap uncertainty summary from logits (no env inputs). Returns (entropy, margin) as (B,1) each."""
        logits32 = logits.to(torch.float32)
        logp = F.log_softmax(logits32, dim=-1)
        p = logp.exp()
        ent = -(p * logp).sum(dim=-1, keepdim=True)  # (B,1)

        # Top1 margin: (top1 - top2). Larger = more confident.
        top2 = torch.topk(logits32, k=2, dim=-1).values  # (B,2)
        margin = (top2[:, 0:1] - top2[:, 1:2])          # (B,1)
        return ent, margin

    def _build_ref_token(
        self,
        dec_tokens: torch.Tensor,          # (B,3,D) in current dtype
        logits_proposal: torch.Tensor,     # (B,A)
        value_proposal: torch.Tensor,      # (B,1)
    ) -> torch.Tensor:
        """Build REF token as (B,1,D) from internal state only (fp32 LN, AMP-friendly projections)."""
        if dec_tokens.dim() != 3 or dec_tokens.size(1) != 3 or dec_tokens.size(2) != self.d_model:
            raise RuntimeError(f"[MirrorBrain] bad dec_tokens shape for REF: got {tuple(dec_tokens.shape)}, expected (B,3,{self.d_model})")

        dec_mean = dec_tokens.mean(dim=1).to(torch.float32)  # (B,D)
        ent, margin = self._logits_entropy_and_margin(logits_proposal)  # (B,1), (B,1)

        v = value_proposal.to(torch.float32)  # (B,1)
        if v.dim() != 2 or v.size(1) != 1:
            raise RuntimeError(f"[MirrorBrain] bad value_proposal shape for REF: got {tuple(v.shape)}, expected (B,1)")

        feat = torch.cat([dec_mean, ent, margin, v], dim=-1)  # (B, D+3)
        x = self.ref_in_norm(feat)  # fp32
        x = self.ref_in_proj(x.to(dtype=self.ref_in_proj.weight.dtype))  # AMP-friendly
        return x.unsqueeze(1)  # (B,1,D)

    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            obs: Flat observation tensor of shape (B, OBS_DIM). Layout must match obs_spec / TronBrain.

        Returns:
            (logits_final, value_final)
            logits_final: (B, act_dim)
            value_final:  (B, 1)
        """
        if obs.dim() != 2:
            raise RuntimeError(f"[MirrorBrain] expected obs to be 2D (B,OBS_DIM); got shape {tuple(obs.shape)}")
        B, D = obs.shape
        if D != self.obs_dim:
            raise RuntimeError(f"[MirrorBrain] obs feature dim mismatch: got {D}, expected {self.obs_dim}")

        # Split obs (strict checks live in obs_spec; do NOT change layout)
        rays_flat, rich_base, instinct = obs_spec.split_obs_flat(obs)

        # ----- PASS 1: PROPOSE -----
        # Rays -> tokens
        rays_raw = rays_flat.view(B, self.num_rays, self.ray_feat_dim)
        ray_tok = self._embed_rays(rays_raw)
        ray_tok = ray_tok + self.ray_dir_embed.to(device=obs.device, dtype=ray_tok.dtype)
        for blk in self.ray_encoder:
            ray_tok = blk(ray_tok)

        # Semantic partitions (uses cached index tensors; no layout changes)
        sem_raw = obs_spec.build_semantic_tokens(rich_base, instinct)

        sem_keys = ["own_context", "world_context", "zone_context", "team_context", "combat_context"]
        sem_tokens = []
        for k in sem_keys:
            sem_tokens.append(self._embed_sem(sem_raw[k], k).unsqueeze(1))
        sem = torch.cat(sem_tokens, dim=1)  # (B,5,D)

        # Instinct token (same tail, embedded)
        inst_tok = self._embed_instinct(sem_raw["instinct_context"]).unsqueeze(1)  # (B,1,D)

        # Learnable decision + memory tokens
        dec = self.decision_tokens.expand(B, -1, -1).to(device=obs.device)  # (B,3,D)
        mem = self.memory_token.expand(B, -1, -1).to(device=obs.device)     # (B,1,D)

        # Plan token sequence: 10 tokens total (must match Tron-family)
        tok = torch.cat([dec, sem, inst_tok, mem], dim=1)  # (B,10,D)
        if tok.size(1) != 10:
            raise RuntimeError(f"[MirrorBrain] plan token length must be 10; got {int(tok.size(1))}")

        for blk in self.plan_encoder:
            tok = blk(tok)

        for blk in self.fusion:
            tok = blk(tok, ray_tok)

        # Readout from decision tokens only
        dec_out = tok[:, :3, :].reshape(B, 3 * self.d_model)
        h = F.gelu(self.read_fc0(dec_out))
        h = F.gelu(self.read_fc1(h))
        logits_prop = self.actor(h)  # (B,A)
        value_prop = self.critic(h)  # (B,1)

        # ----- PASS 2: REFLECT + EDIT -----
        # Build REF from internal summaries only (no new env inputs)
        ref = self._build_ref_token(tok[:, :3, :], logits_prop, value_prop).to(dtype=tok.dtype, device=tok.device)

        # REF attends to plan tokens and ray tokens (small, 2 blocks)
        ref = self.ref_attend_plan(ref, tok)
        ref = self.ref_attend_rays(ref, ray_tok)

        ref_h = ref.squeeze(1)  # (B,D)
        dh = F.gelu(self.delta_fc0(ref_h))
        dh = F.gelu(self.delta_fc1(dh))
        delta_logits = self.delta_actor(dh)    # (B,A) ~0 at init
        delta_value = self.delta_critic(dh)    # (B,1) ~0 at init

        logits = logits_prop + delta_logits
        value = value_prop + delta_value

        # ----- Final shape asserts (critical invariants) -----
        if logits.dim() != 2 or logits.size(0) != B or logits.size(1) != self.act_dim:
            raise RuntimeError(f"[MirrorBrain] bad logits shape: got {tuple(logits.shape)}, expected ({B},{self.act_dim})")
        if value.dim() != 2 or value.size(0) != B or value.size(1) != 1:
            raise RuntimeError(f"[MirrorBrain] bad value shape: got {tuple(value.shape)}, expected ({B},1)")

        return logits, value

====[ END war_simulation\agent\mirror_brain.py ]============================================================


====[ 5/35 | war_simulation\agent\obs_spec.py ]==============================================
from __future__ import annotations
from typing import Dict, Tuple
import torch
import config

# Cache index tensors per (device, name) to avoid per-step allocations.
_IDX_CACHE: Dict[Tuple[torch.device, str], torch.Tensor] = {}

def _idx(name: str, device: torch.device) -> torch.Tensor:
    """Get cached index tensor for semantic token by name."""
    key = (device, name)
    t = _IDX_CACHE.get(key)
    if t is not None:
        return t
    if name not in config.SEMANTIC_RICH_BASE_INDICES:
        raise KeyError(f"Unknown semantic token name: {name}")
    idx = torch.tensor(config.SEMANTIC_RICH_BASE_INDICES[name], dtype=torch.long, device=device)
    _IDX_CACHE[key] = idx
    return idx

def split_obs_flat(obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Split flat observation into three components:
      rays_flat  : (B, RAYS_FLAT_DIM)
      rich_base  : (B, RICH_BASE_DIM)
      instinct   : (B, INSTINCT_DIM)
    
    Performs strict shape checking and fails loudly on mismatch.
    """
    if obs.dim() != 2:
        raise RuntimeError(f"obs must be rank-2 (B,F). got shape={tuple(obs.shape)}")
    
    B, F = int(obs.shape[0]), int(obs.shape[1])
    if F != int(config.OBS_DIM):
        raise RuntimeError(f"obs_dim mismatch: got F={F}, expected config.OBS_DIM={int(config.OBS_DIM)}")

    rays_dim = int(config.RAYS_FLAT_DIM)
    rich_total = int(config.RICH_TOTAL_DIM)
    if rays_dim + rich_total != F:
        raise RuntimeError(f"layout mismatch: rays_dim({rays_dim}) + rich_total({rich_total}) != F({F})")

    # Split observation into rays and rich tail
    rays_flat = obs[:, :rays_dim]
    rich_tail = obs[:, rays_dim:]

    # Split rich tail into base and instinct
    base_dim = int(config.RICH_BASE_DIM)
    inst_dim = int(config.INSTINCT_DIM)
    if base_dim + inst_dim != int(rich_tail.shape[1]):
        raise RuntimeError(
            f"rich_tail mismatch: got {int(rich_tail.shape[1])}, expected {base_dim}+{inst_dim}"
        )

    rich_base = rich_tail[:, :base_dim]
    instinct = rich_tail[:, base_dim:base_dim + inst_dim]
    return rays_flat, rich_base, instinct

def build_semantic_tokens(
    rich_base: torch.Tensor,
    instinct: torch.Tensor,
) -> Dict[str, torch.Tensor]:
    """
    Build semantic token tensors from rich_base and instinct components.
    
    Returns dictionary containing:
      own_context, world_context, zone_context, team_context, combat_context, instinct_context
    
    Each token tensor has shape (B, token_dim) and resides on same device as inputs.
    """
    if rich_base.dim() != 2:
        raise RuntimeError(f"rich_base must be (B,D). got {tuple(rich_base.shape)}")
    if instinct.dim() != 2:
        raise RuntimeError(f"instinct must be (B,4). got {tuple(instinct.shape)}")

    B = int(rich_base.shape[0])
    if int(rich_base.shape[1]) != int(config.RICH_BASE_DIM):
        raise RuntimeError(
            f"rich_base dim mismatch: got {int(rich_base.shape[1])}, expected {int(config.RICH_BASE_DIM)}"
        )
    if int(instinct.shape[0]) != B or int(instinct.shape[1]) != int(config.INSTINCT_DIM):
        raise RuntimeError(
            f"instinct shape mismatch: got {tuple(instinct.shape)}, expected ({B},{int(config.INSTINCT_DIM)})"
        )

    device = rich_base.device
    out: Dict[str, torch.Tensor] = {}

    # Extract each semantic token from rich_base using pre-defined indices
    for name in ("own_context", "world_context", "zone_context", "team_context", "combat_context"):
        idx = _idx(name, device)
        tok = torch.index_select(rich_base, dim=1, index=idx)
        out[name] = tok

    # Add instinct token directly
    out["instinct_context"] = instinct
    return out
====[ END war_simulation\agent\obs_spec.py ]============================================================


====[ 6/35 | war_simulation\agent\transformer_brain.py ]=====================================
from typing import Tuple
import torch
import torch.nn as nn
import torch.nn.functional as F
import config
class CrossAttentionBlock(nn.Module):
    """
    A single block of Cross-Attention followed by a Feed-Forward network.
    Includes residual connections and layer normalization.
    """
    def __init__(self, embed_dim: int, num_heads: int = 1):
        super().__init__()
        self.embed_dim = embed_dim
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 2),
            nn.GELU(),
            nn.Linear(embed_dim * 2, embed_dim),
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, query: torch.Tensor, key_value: torch.Tensor) -> torch.Tensor:
        attn_output, _ = self.attn(query, key_value, key_value, need_weights=False)
        x = self.norm1(query + attn_output)
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)
        return x

class SelfAttentionBlock(nn.Module):
    """
    A single block of Self-Attention followed by a Feed-Forward network.
    Includes residual connections and layer normalization.
    """
    def __init__(self, embed_dim: int, num_heads: int = 1):
        super().__init__()
        self.embed_dim = embed_dim
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 2),
            nn.GELU(),
            nn.Linear(embed_dim * 2, embed_dim),
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        attn_output, _ = self.attn(x, x, x, need_weights=False)
        x = self.norm1(x + attn_output)
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)
        return x


# --- The Main Transformer Brain ---

class TransformerBrain(nn.Module):
    """
    A transformer-based brain that processes observations by treating raycasts
    as a sequence of tokens and enriching them with the agent's state via attention.
    """
    def __init__(self, obs_dim: int, act_dim: int, embed_dim: int = 32, mlp_hidden: int = 128):
        super().__init__()
        self.obs_dim = int(obs_dim)
        self.act_dim = int(act_dim)
        self.embed_dim = int(embed_dim)

        self.num_rays = 32
        self.ray_feat_dim = 8
        self.rich_feat_dim = self.obs_dim - (self.num_rays * self.ray_feat_dim)
        
        if self.rich_feat_dim <= 0:
            raise ValueError(f"obs_dim ({obs_dim}) is not large enough for {self.num_rays} rays.")

        self.ray_embed_norm = nn.LayerNorm(self.ray_feat_dim)
        self.ray_embed_proj = nn.Linear(self.ray_feat_dim, self.embed_dim)

        self.rich_embed_norm = nn.LayerNorm(self.rich_feat_dim)
        self.rich_embed_proj = nn.Linear(self.rich_feat_dim, self.embed_dim)

        self.positional_encoding = nn.Parameter(torch.randn(1, self.num_rays, self.embed_dim))

        self.cross_attention = CrossAttentionBlock(self.embed_dim)
        self.self_attention = SelfAttentionBlock(self.embed_dim)

        mlp_input_dim = self.embed_dim * 2
        self.fc_in = nn.Linear(mlp_input_dim, mlp_hidden)
        self.fc1 = nn.Linear(mlp_hidden, mlp_hidden)
        self.actor = nn.Linear(mlp_hidden, self.act_dim)
        self.critic = nn.Linear(mlp_hidden, 1)

        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    # --- NEW: Specific, JIT-compatible helper methods ---
    # This replaces the generic '_norm_then_linear' which caused scripting errors.
    def _embed_rays(self, rays_raw: torch.Tensor) -> torch.Tensor:
        """Embeds ray features using a JIT-safe sequence."""
        x_norm = self.ray_embed_norm(rays_raw.float())
        return self.ray_embed_proj(x_norm.to(dtype=self.ray_embed_proj.weight.dtype))

    def _embed_rich(self, rich_raw: torch.Tensor) -> torch.Tensor:
        """Embeds rich features using a JIT-safe sequence."""
        x_norm = self.rich_embed_norm(rich_raw.float())
        return self.rich_embed_proj(x_norm.to(dtype=self.rich_embed_proj.weight.dtype))
    # --- END NEW ---

    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        B = obs.shape[0]

        rays_raw = obs[:, :self.num_rays * self.ray_feat_dim].view(B, self.num_rays, self.ray_feat_dim)
        rich_raw = obs[:, self.num_rays * self.ray_feat_dim:]

        # Use the new, specific helper methods
        ray_tokens = self._embed_rays(rays_raw)
        rich_token = self._embed_rich(rich_raw).unsqueeze(1)

        ray_tokens = ray_tokens + self.positional_encoding
        contextual_ray_tokens = self.cross_attention(query=ray_tokens, key_value=rich_token)
        processed_ray_tokens = self.self_attention(contextual_ray_tokens)

        pooled_ray_summary = processed_ray_tokens.mean(dim=1)
        mlp_input = torch.cat([pooled_ray_summary, rich_token.squeeze(1)], dim=-1)

        h = F.gelu(self.fc_in(mlp_input))
        h = F.gelu(self.fc1(h))

        logits = self.actor(h)
        value = self.critic(h)

        return logits, value

    def param_count(self) -> int:
        """Utility to count the total number of trainable parameters."""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

def scripted_transformer_brain(obs_dim: int, act_dim: int) -> torch.jit.ScriptModule:
    """TorchScript brain for non-PPO runs."""
    model = TransformerBrain(obs_dim, act_dim)
    return torch.jit.script(model)# 
====[ END war_simulation\agent\transformer_brain.py ]============================================================


====[ 7/35 | war_simulation\agent\tron_brain.py ]============================================
from __future__ import annotations

from typing import Dict, List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F

import config


class _SelfAttnBlock(nn.Module):
    """Pre-LN style transformer block: Self-Attention + Feed-Forward Network, with residual connections.
    
    Architecture:
        x -> MultiHeadAttention -> LayerNorm(x + attn) -> FFN -> LayerNorm(x + ffn) -> output
    """
    def __init__(self, d_model: int, n_heads: int, ffn_mult: int = 4) -> None:
        """Initialize self-attention block.
        
        Args:
            d_model: Model dimension (must be divisible by n_heads)
            n_heads: Number of attention heads
            ffn_mult: Multiplier for FFN hidden dimension (default: 4)
        """
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, ffn_mult * d_model),
            nn.GELU(),
            nn.Linear(ffn_mult * d_model, d_model),
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Apply self-attention + FFN with pre-LN residuals.
        
        Args:
            x: Input tensor of shape (batch_size, seq_len, d_model)
            
        Returns:
            Output tensor of same shape as input
        """
        # Self-attention with residual
        a, _ = self.attn(x, x, x, need_weights=False)
        x = self.norm1(x + a)
        
        # FFN with residual
        f = self.ffn(x)
        x = self.norm2(x + f)
        return x


class _CrossAttnBlock(nn.Module):
    """Cross-Attention block: Query attends to Key-Value pairs, with FFN and residuals.
    
    Architecture:
        q, kv -> CrossAttn -> LayerNorm(q + attn) -> FFN -> LayerNorm(q + ffn) -> output
    """
    def __init__(self, d_model: int, n_heads: int, ffn_mult: int = 4) -> None:
        """Initialize cross-attention block.
        
        Args:
            d_model: Model dimension (must be divisible by n_heads)
            n_heads: Number of attention heads
            ffn_mult: Multiplier for FFN hidden dimension (default: 4)
        """
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, ffn_mult * d_model),
            nn.GELU(),
            nn.Linear(ffn_mult * d_model, d_model),
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, q: torch.Tensor, kv: torch.Tensor) -> torch.Tensor:
        """Apply cross-attention where q attends to kv, followed by FFN.
        
        Args:
            q: Query tensor of shape (batch_size, q_seq_len, d_model)
            kv: Key/Value tensor of shape (batch_size, kv_seq_len, d_model)
            
        Returns:
            Output tensor of same shape as q
        """
        # Cross-attention with residual
        a, _ = self.attn(q, kv, kv, need_weights=False)
        q = self.norm1(q + a)
        
        # FFN with residual
        f = self.ffn(q)
        q = self.norm2(q + f)
        return q


class TronBrain(nn.Module):
    """
    TRON v1 Transformer-based brain for per-agent control.
    
    Architecture has 4 stages:
      1. Ray encoder: Self-attention over ray tokens to extract spatial features
      2. Semantic encoder: Self-attention over semantic tokens (own, world, zone, team, combat, instinct)
      3. Fusion: Semantic+decision tokens cross-attend to processed ray tokens
      4. Readout: Decision tokens only -> logits and value
    
    Forward contract (MUST stay): forward(obs) -> (logits, value)
    
    Observation layout (from config):
      [rays_flat (num_rays * ray_feat_dim) | rich_base(23) | instinct(4)]
    """

    def __init__(self, obs_dim: int, act_dim: int) -> None:
        """Initialize TronBrain with observation and action dimensions.
        
        Args:
            obs_dim: Total observation dimension (must match config.OBS_DIM)
            act_dim: Action dimension (number of discrete actions)
            
        Raises:
            ValueError: If obs_dim doesn't match expected layout or config invalid
            RuntimeError: If semantic indices missing from config
        """
        super().__init__()
        self.obs_dim = int(obs_dim)
        self.act_dim = int(act_dim)

        # --- Observation layout invariants (must match Phase 1-3 pipeline) ---
        # These constants are loaded from config and define the observation structure
        self.num_rays = int(getattr(config, "RAY_TOKEN_COUNT", 32))
        self.ray_feat_dim = int(getattr(config, "RAY_FEAT_DIM", 8))
        self.rich_base_dim = int(getattr(config, "RICH_BASE_DIM", 23))
        self.instinct_dim = int(getattr(config, "INSTINCT_DIM", 4))

        # Compute expected dimensions for validation
        self.rays_flat_dim = self.num_rays * self.ray_feat_dim
        self.expected_obs_dim = self.rays_flat_dim + self.rich_base_dim + self.instinct_dim

        # Validate observation dimension against config layout
        if self.obs_dim != self.expected_obs_dim:
            raise ValueError(
                f"TronBrain obs_dim mismatch: got obs_dim={self.obs_dim}, "
                f"expected {self.expected_obs_dim} = ({self.num_rays}*{self.ray_feat_dim})"
                f"+{self.rich_base_dim}+{self.instinct_dim}."
            )

        # Semantic partition indices must exist (Phase 3 output)
        # These indices define how to slice rich_base into semantic tokens
        idx_map = getattr(config, "SEMANTIC_RICH_BASE_INDICES", None)
        if idx_map is None:
            raise RuntimeError(
                "SEMANTIC_RICH_BASE_INDICES missing in config. "
                "Apply Phase 3 (semantic partitioning) before using TronBrain."
            )
        self._idx_map: Dict[str, List[int]] = dict(idx_map)

        # --- TRON hyperparams (config-driven, safe defaults) ---
        d_model = int(getattr(config, "TRON_D_MODEL", 64))
        n_heads = int(getattr(config, "TRON_HEADS", 4))
        ray_layers = int(getattr(config, "TRON_RAY_LAYERS", 4))
        sem_layers = int(getattr(config, "TRON_SEM_LAYERS", 2))
        fusion_layers = int(getattr(config, "TRON_FUSION_LAYERS", 2))
        mlp_hidden = int(getattr(config, "TRON_MLP_HIDDEN", 128))

        # Validate transformer configuration
        if d_model <= 0 or n_heads <= 0:
            raise ValueError("Invalid TRON config: TRON_D_MODEL and TRON_HEADS must be positive.")
        if d_model % n_heads != 0:
            raise ValueError(f"Invalid TRON config: TRON_D_MODEL ({d_model}) must be divisible by TRON_HEADS ({n_heads}).")

        self.d_model = d_model
        self.n_heads = n_heads

        # --- Ray token embedding (8 features -> d_model) + learned direction embedding ---
        # Each ray has 8 features, projected to d_model
        self.ray_in_norm = nn.LayerNorm(self.ray_feat_dim)
        self.ray_in_proj = nn.Linear(self.ray_feat_dim, d_model)
        # Learned positional embedding for ray directions (1, num_rays, d_model)
        self.ray_dir_embed = nn.Parameter(torch.randn(1, self.num_rays, d_model) * 0.02)

        # --- Semantic token embeddings (variable input dims -> d_model) ---
        # Expected semantic groups from Phase 3:
        #   own_context, world_context, zone_context, team_context, combat_context
        # Each group has different dimensionality, projected to common d_model
        sem_keys = ["own_context", "world_context", "zone_context", "team_context", "combat_context"]
        self._sem_keys = sem_keys

        # Create LayerNorm and Linear layers for each semantic group
        self.sem_in_norm = nn.ModuleDict()
        self.sem_in_proj = nn.ModuleDict()
        for k in sem_keys:
            idxs = self._idx_map.get(k, None)
            if idxs is None or len(idxs) == 0:
                raise RuntimeError(f"Missing/empty semantic index list for '{k}' in SEMANTIC_RICH_BASE_INDICES.")
            din = int(len(idxs))
            self.sem_in_norm[k] = nn.LayerNorm(din)
            self.sem_in_proj[k] = nn.Linear(din, d_model)

        # Instinct embedding (4 features -> d_model)
        self.instinct_in_norm = nn.LayerNorm(self.instinct_dim)
        self.instinct_in_proj = nn.Linear(self.instinct_dim, d_model)

        # Memory token (learnable, not from observation)
        # Acts as a persistent memory across time steps
        self.memory_token = nn.Parameter(torch.zeros(1, 1, d_model))

        # Decision tokens (learnable): 3 tokens for different decision-making roles
        # D0: tactical decisions, D1: objective decisions, D2: safety decisions
        self.decision_tokens = nn.Parameter(torch.randn(1, 3, d_model) * 0.02)

        # --- Stage blocks ---
        # Stage 1: Ray encoder - processes ray tokens independently
        self.ray_encoder = nn.ModuleList([_SelfAttnBlock(d_model, n_heads) for _ in range(ray_layers)])
        
        # Stage 2: Semantic encoder - processes combined token sequence
        self.sem_encoder = nn.ModuleList([_SelfAttnBlock(d_model, n_heads) for _ in range(sem_layers)])
        
        # Stage 3: Fusion - semantic tokens attend to ray tokens
        self.fusion = nn.ModuleList([_CrossAttnBlock(d_model, n_heads) for _ in range(fusion_layers)])

        # --- Readout from decision tokens only ---
        # MLP that takes concatenated decision tokens (3 * d_model) -> logits and value
        self.read_fc0 = nn.Linear(3 * d_model, mlp_hidden)
        self.read_fc1 = nn.Linear(mlp_hidden, mlp_hidden)
        self.actor = nn.Linear(mlp_hidden, self.act_dim)
        self.critic = nn.Linear(mlp_hidden, 1)

        self._init_weights()

    def _init_weights(self) -> None:
        """Initialize weights using Xavier uniform for linear layers and zeros for biases."""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    # ---- embedding helpers (stable + AMP-friendly) ----
    def _embed_rays(self, rays_raw: torch.Tensor) -> torch.Tensor:
        """Embed raw ray features to d_model dimension.
        
        Args:
            rays_raw: Raw ray features of shape (B, num_rays, ray_feat_dim)
            
        Returns:
            Embedded rays of shape (B, num_rays, d_model)
        """
        # Use float32 for LayerNorm stability, then cast to model dtype for projection
        x = self.ray_in_norm(rays_raw.float())
        x = self.ray_in_proj(x.to(dtype=self.ray_in_proj.weight.dtype))
        return x

    def _embed_sem(self, x_raw: torch.Tensor, key: str) -> torch.Tensor:
        """Embed raw semantic features for a specific group to d_model dimension.
        
        Args:
            x_raw: Raw semantic features of shape (B, group_dim)
            key: Semantic group name (e.g., "own_context")
            
        Returns:
            Embedded semantic features of shape (B, d_model)
        """
        n = self.sem_in_norm[key]
        p = self.sem_in_proj[key]
        x = n(x_raw.float())
        x = p(x.to(dtype=p.weight.dtype))
        return x

    def _embed_instinct(self, inst_raw: torch.Tensor) -> torch.Tensor:
        """Embed raw instinct features to d_model dimension.
        
        Args:
            inst_raw: Raw instinct features of shape (B, instinct_dim)
            
        Returns:
            Embedded instinct features of shape (B, d_model)
        """
        x = self.instinct_in_norm(inst_raw.float())
        x = self.instinct_in_proj(x.to(dtype=self.instinct_in_proj.weight.dtype))
        return x

    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass through TronBrain.
        
        Args:
            obs: Flat observation tensor of shape (batch_size, OBS_DIM)
                 Layout: [rays_flat | rich_base | instinct]
                 
        Returns:
            Tuple of (logits, value):
                logits: Action logits of shape (batch_size, act_dim)
                value: Value prediction of shape (batch_size, 1)
                
        Raises:
            RuntimeError: If observation shape doesn't match expected dimensions
                         or if output shapes are incorrect
        """
        # ----- hard invariants -----
        # Verify observation is 2D (batch, features)
        if obs.dim() != 2:
            raise RuntimeError(f"TronBrain.forward expects obs 2D [B,F], got shape={tuple(obs.shape)}")
        B, Fdim = int(obs.size(0)), int(obs.size(1))
        # Verify feature dimension matches expected layout
        if Fdim != self.expected_obs_dim:
            raise RuntimeError(
                f"TronBrain.forward obs dim mismatch: got F={Fdim}, expected {self.expected_obs_dim}."
            )

        # ----- split observation into components -----
        # Split according to layout: [rays_flat | rich_base | instinct]
        rays_flat = obs[:, : self.rays_flat_dim]
        rich_base = obs[:, self.rays_flat_dim : self.rays_flat_dim + self.rich_base_dim]
        instinct = obs[:, self.rays_flat_dim + self.rich_base_dim : self.expected_obs_dim]

        # ----- Stage 1: Ray Processing -----
        # Reshape flat rays to (B, num_rays, ray_feat_dim)
        rays_raw = rays_flat.view(B, self.num_rays, self.ray_feat_dim)
        # Embed rays to d_model and add directional embeddings
        ray_tok = self._embed_rays(rays_raw)
        ray_tok = ray_tok + self.ray_dir_embed.to(dtype=ray_tok.dtype, device=ray_tok.device)

        # Apply self-attention blocks to process ray tokens
        for blk in self.ray_encoder:
            ray_tok = blk(ray_tok)

        # ----- Stage 2: Semantic Token Construction -----
        # Build semantic tokens from rich_base partitions
        sem_tokens: List[torch.Tensor] = []
        for k in self._sem_keys:
            # Extract indices for this semantic group
            idxs = self._idx_map[k]
            # Gather features using indices (maintains order from config)
            xk = rich_base.index_select(dim=1, index=torch.tensor(idxs, device=rich_base.device, dtype=torch.long))
            # Embed and add sequence dimension
            sem_tokens.append(self._embed_sem(xk, k).unsqueeze(1))  # (B, 1, D)

        # Embed instinct token from tail
        inst_tok = self._embed_instinct(instinct).unsqueeze(1)  # (B, 1, D)

        # Prepare decision and memory tokens
        dec = self.decision_tokens.expand(B, -1, -1).to(device=obs.device)  # (B, 3, D)
        mem = self.memory_token.expand(B, -1, -1).to(device=obs.device)    # (B, 1, D)
        
        # Concatenate all tokens: decisions (3), semantics (5), instinct (1), memory (1)
        sem = torch.cat(sem_tokens, dim=1)  # (B, 5, D)
        tok = torch.cat([dec, sem, inst_tok, mem], dim=1)  # (B, 3+5+1+1=10, D)

        # Apply self-attention to token sequence
        for blk in self.sem_encoder:
            tok = blk(tok)

        # ----- Stage 3: Fusion (Cross-Attention) -----
        # Semantic+decision tokens attend to processed ray tokens
        for blk in self.fusion:
            tok = blk(tok, ray_tok)

        # ----- Stage 4: Readout -----
        # Extract only decision tokens (first 3) for readout
        dec_out = tok[:, :3, :].reshape(B, 3 * self.d_model)
        
        # MLP head
        h = F.gelu(self.read_fc0(dec_out))
        h = F.gelu(self.read_fc1(h))

        # Final projections
        logits = self.actor(h)              # (B, act_dim)
        value = self.critic(h)              # (B, 1)

        # ----- Final shape asserts (critical "DO NOT BREAK" invariants) -----
        if logits.dim() != 2 or logits.size(0) != B or logits.size(1) != self.act_dim:
            raise RuntimeError(f"Bad logits shape from TronBrain: got {tuple(logits.shape)}, expected ({B},{self.act_dim})")
        if value.dim() != 2 or value.size(0) != B or value.size(1) != 1:
            raise RuntimeError(f"Bad value shape from TronBrain: got {tuple(value.shape)}, expected ({B},1)")

        return logits, value
====[ END war_simulation\agent\tron_brain.py ]============================================================


====[ 8/35 | war_simulation\config.py ]======================================================
from __future__ import annotations

import os
import math
import torch

# =============================================================================
# ðŸ› ï¸ ENV-PARSING UTILITIES (PRODUCTION-GRADE SECRETS)
# =============================================================================
# These helper functions safely parse environment variables. This architecture 
# allows you to run hyperparameter sweeps from bash scripts without ever 
# touching this Python file. 
# Example: FWS_MAX_AGENTS=2000 python main.py

def _env_bool(key: str, default: bool) -> bool:
    v = os.getenv(key)
    if v is None: return default
    return v.strip().lower() in ("1", "true", "yes", "y", "on")

def _env_float(key: str, default: float) -> float:
    v = os.getenv(key)
    if v is None: return float(default)
    try: return float(v)
    except Exception: return float(default)

def _env_int(key: str, default: int) -> int:
    v = os.getenv(key)
    if v is None: return int(default)
    try: return int(v)
    except Exception: return int(default)

def _env_str(key: str, default: str) -> str:
    v = os.getenv(key)
    return default if v is None else str(v)

def _env_is_set(key: str) -> bool:
    return os.getenv(key) is not None

# =============================================================================
# ðŸ§ª EXPERIMENT TRACKING & SEEDING
# =============================================================================
# Profiles act as master macros to override batches of settings at once.
PROFILE: str = _env_str("FWS_PROFILE", "default").strip().lower()

# Tag used by ResultsWriter to organize output directories for research logging.
EXPERIMENT_TAG: str = _env_str("FWS_EXPERIMENT_TAG", "god_level_run").strip()

# Master Random Seed. 
# 0 = Stochastic (True chaos, best for emergent behavior).
# 42 = Deterministic (Best for debugging specific crashes).
RNG_SEED: int = _env_int("FWS_SEED", 0)

RESULTS_DIR: str = _env_str("FWS_RESULTS_DIR", "results").strip()

# =============================================================================
# ðŸ’» HARDWARE ACCELERATION & TENSOR COMPILER
# =============================================================================
# These settings determine how the PyTorch backend talks to your silicon.

USE_CUDA = _env_bool("FWS_CUDA", True) and torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_CUDA else "cpu")
TORCH_DEVICE = DEVICE 

# Automatic Mixed Precision (AMP). 
# Uses FP16 for neural network matrix multiplications. This literally doubles 
# your throughput and halves VRAM usage. CRITICAL for 1000+ agents.
AMP_ENABLED = _env_bool("FWS_AMP", True)

def amp_enabled() -> bool: return AMP_ENABLED

TORCH_DTYPE = torch.float16 if (USE_CUDA and AMP_ENABLED) else torch.float32

# PyTorch VMAP (Vectorized Map).
# Instead of a python 'for' loop over 1000 agents, vmap fuses all agent forward 
# passes into a single C++ batched operation. This is the secret sauce to 
# pushing the PC to the edge without bottlenecking the Python GIL.
USE_VMAP = _env_bool("FWS_USE_VMAP", True)
VMAP_MIN_BUCKET = _env_int("FWS_VMAP_MIN_BUCKET", 8)
VMAP_DEBUG = _env_bool("FWS_VMAP_DEBUG", False)

# =============================================================================
# ðŸŒ WORLD SCALE & MEMORY ALLOCATION
# =============================================================================
# Scaled up for epic, screen-filling battles. 

# 160x160 is massively larger than 120x120 (25,600 cells vs 14,400 cells).
# This gives the agents room to execute flanking maneuvers rather than just 
# crashing into each other in a center death-ball.
GRID_WIDTH  = _env_int("FWS_GRID_W", 120)
GRID_HEIGHT = _env_int("FWS_GRID_H", 120)

# START_AGENTS defines the initial drop. 500 per team = 1000 agents actively 
# computing raycasts and attention matrices simultaneously. 
START_AGENTS_PER_TEAM = _env_int("FWS_START_PER_TEAM", 150)

# MAX_AGENTS must be larger than START to accommodate the SoA (Struct of Arrays) 
# memory pre-allocation for reinforcements/respawns.
MAX_AGENTS  = _env_int("FWS_MAX_AGENTS", 500)

# 0 = run simulation math as fast as the CPU/GPU allows.
TICK_LIMIT = _env_int("FWS_TICK_LIMIT", 0)
TARGET_TPS = _env_int("FWS_TARGET_TPS", 0) 

# Strict schema width for the AgentRegistry. DO NOT CHANGE.
AGENT_FEATURES = 10 

# =============================================================================
# ðŸ—ºï¸ TOPOGRAPHY & STRATEGIC OBJECTIVES
# =============================================================================
# Topography defines the RL meta. Walls force pathfinding, zones force conflict.

# Increased walls to accommodate the massive 160x160 map. Creates distinct 
# "lanes" and "choke points" for tactical combat.
RANDOM_WALLS = _env_int("FWS_RAND_WALLS", 21)
WALL_SEG_MIN = _env_int("FWS_WALL_SEG_MIN", 10)
WALL_SEG_MAX = _env_int("FWS_WALL_SEG_MAX", 80)
WALL_AVOID_MARGIN = _env_int("FWS_WALL_MARGIN", 3)

MAP_WALL_STRAIGHT_PROB = _env_float("FWS_MAP_WALL_STRAIGHT_PROB", 0.75)
MAP_WALL_GAP_PROB      = _env_float("FWS_MAP_WALL_GAP_PROB", 0.08)

# Heal Zones (The "Water holes"). Scaled up in count to support 1000 agents.
HEAL_ZONE_COUNT      = _env_int("FWS_HEAL_COUNT", 16)
HEAL_ZONE_SIZE_RATIO = _env_float("FWS_HEAL_SIZE_RATIO", 8/128)
HEAL_RATE            = _env_float("FWS_HEAL_RATE", 0.002)

# Capture Points (The "King of the Hill" objective).
CP_COUNT           = _env_int("FWS_CP_COUNT", 11)
CP_SIZE_RATIO      = _env_float("FWS_CP_SIZE_RATIO", 10/160)
CP_REWARD_PER_TICK = _env_float("FWS_CP_REWARD", 0.50)

# =============================================================================
# âš”ï¸ COMBAT BIOLOGY & CLASSES
# =============================================================================
UNIT_SOLDIER_ID = 1
UNIT_ARCHER_ID  = 2
UNIT_SOLDIER    = UNIT_SOLDIER_ID
UNIT_ARCHER     = UNIT_ARCHER_ID

# Higher HP yields longer Time-To-Kill (TTK). This is crucial for RL because it 
# allows the agent to witness the consequences of bad positioning and escape, 
# rather than instantly dying and learning nothing.
MAX_HP     = _env_float("FWS_MAX_HP", 1.0)
SOLDIER_HP = _env_float("FWS_SOLDIER_HP", 1.0)
ARCHER_HP  = _env_float("FWS_ARCHER_HP", 0.65) # Glass cannons

# Attack Values. 
BASE_ATK    = _env_float("FWS_BASE_ATK", 0.35)
SOLDIER_ATK = _env_float("FWS_SOLDIER_ATK", 0.25)
ARCHER_ATK  = _env_float("FWS_ARCHER_ATK", 0.15) 
MAX_ATK     = max(SOLDIER_ATK, ARCHER_ATK, BASE_ATK, 1e-6)

# Engagement Distances
BASE_RANGE   = _env_int("FWS_BASE_RANGE", 6)
ARCHER_RANGE = _env_int("FWS_ARCHER_RANGE", 5) # Gives archers extreme tactical advantage
ARCHER_LOS_BLOCKS_WALLS = _env_bool("FWS_ARCHER_BLOCK_LOS", True)

# =============================================================================
# ðŸ”‹ METABOLISM (Anti-Stalemate Mechanic)
# =============================================================================
# Agents bleed HP passively. If they hide in a corner doing nothing, they die.
# This forces exploration and confrontation over Heal Zones.
METABOLISM_ENABLED       = _env_bool("FWS_META_ON", True)
META_SOLDIER_HP_PER_TICK = _env_float("FWS_META_SOLDIER", 0.0005)
META_ARCHER_HP_PER_TICK  = _env_float("FWS_META_ARCHER",  0.0002)

# =============================================================================
# ðŸ‘ï¸ SENSORS & INSTINCT (THE AI'S "EYES")
# =============================================================================
# Raycasting is the most CPU-heavy part of the simulation.
VISION_RANGE_SOLDIER = _env_int("FWS_VISION_SOLDIER", 12)
VISION_RANGE_ARCHER  = _env_int("FWS_VISION_ARCHER", 16) # Snipers need good eyes

VISION_RANGE_BY_UNIT = {
    UNIT_SOLDIER_ID: VISION_RANGE_SOLDIER,
    UNIT_ARCHER_ID:  VISION_RANGE_ARCHER,
}
RAYCAST_MAX_STEPS = max(max(VISION_RANGE_BY_UNIT.values()), 1)
RAY_MAX_STEPS     = RAYCAST_MAX_STEPS

# Instinct detects "Flanking" mathematically via local unit density.
INSTINCT_RADIUS = _env_int("FWS_INSTINCT_RADIUS", 18)

# =============================================================================
# ðŸ§© TENSOR OBSERVATION LAYOUT (STRICT CONTRACT)
# =============================================================================
# WARNING: Changing these changes the input layer of the Transformer.
# 32 rays is the sweet spot. 64 rays drops FPS heavily at 1000 agents.
RAY_TOKEN_COUNT = _env_int("FWS_RAY_TOKENS", 32)
RAY_FEAT_DIM    = 8
RAYS_FLAT_DIM   = RAY_TOKEN_COUNT * RAY_FEAT_DIM

RICH_BASE_DIM   = 23 
INSTINCT_DIM    = 4  
RICH_TOTAL_DIM  = RICH_BASE_DIM + INSTINCT_DIM

OBS_DIM = RAYS_FLAT_DIM + RICH_TOTAL_DIM

SEMANTIC_RICH_BASE_INDICES = {
    "own_context":    (0, 1, 2, 5, 6, 7, 8),
    "world_context":  (11, 20, 21, 22),
    "zone_context":   (9, 10),
    "team_context":   (3, 4, 12, 13, 14, 15),
    "combat_context": (16, 17, 18, 19),
}

SEMANTIC_TOKEN_ORDER = (
    "own_context", "world_context", "zone_context", "team_context", "combat_context", "instinct_context"
)

NUM_ACTIONS = _env_int("FWS_NUM_ACTIONS", 41)

# =============================================================================
# ðŸ”„ POPULATION CONTROL (REINFORCEMENTS)
# =============================================================================
RESPAWN_ENABLED = _env_bool("FWS_RESPAWN", True)

# New Wave-Based Respawn logic.
RESP_FLOOR_PER_TEAM      = _env_int("FWS_RESP_FLOOR_PER_TEAM", 110) # Never let a team drop below 300
RESP_MAX_PER_TICK        = _env_int("FWS_RESP_MAX_PER_TICK", 15)    # Smooth out spawn-lag
RESP_PERIOD_TICKS        = _env_int("FWS_RESP_PERIOD_TICKS", 600)   # Reinforcement chopper arrives every 600 ticks
RESP_PERIOD_BUDGET       = _env_int("FWS_RESP_PERIOD_BUDGET", 40)   # Drops 40 agents 
RESP_HYST_COOLDOWN_TICKS = _env_int("FWS_RESP_HYST_COOLDOWN_TICKS", 45) 
RESP_WALL_MARGIN         = _env_int("FWS_RESP_WALL_MARGIN", 2)

SPAWN_ARCHER_RATIO       = _env_float("FWS_SPAWN_ARCHER_RATIO", 0.35) 

# Legacy fallbacks
RESPAWN_PROB_PER_DEAD        = _env_float("FWS_RESPAWN_PROB", 0.05)
RESPAWN_SPAWN_TRIES          = _env_int("FWS_RESPAWN_TRIES", 200)
RESPAWN_MUTATION_STD         = _env_float("FWS_MUT_STD", 0.05) # Increased mutation volatility for better evolution
RESPAWN_CLONE_PROB           = _env_float("FWS_CLONE_PROB", 0.70) # Highly favor cloning successful survivors
RESPAWN_USE_TEAM_ELITE       = _env_bool("FWS_TEAM_ELITE", True)
RESPAWN_RESET_OPT_ON_RESPAWN = _env_bool("FWS_RESET_OPT", True)
RESPAWN_JITTER_RADIUS        = _env_int("FWS_RESP_JITTER", 3)
RESPAWN_COOLDOWN_TICKS       = _env_int("FWS_RESPAWN_CD", 500)
RESPAWN_BATCH_PER_TEAM       = _env_int("FWS_RESPAWN_BATCH", 1)
RESPAWN_ARCHER_SHARE         = _env_float("FWS_RESPAWN_ARCHER_SHARE", 0.50)
RESPAWN_INTERIOR_BIAS        = _env_float("FWS_RESPAWN_INTERIOR_BIAS", 0.95)

# =============================================================================
# ðŸ† REWARD SHAPING (RL Feedback Loop)
# =============================================================================
# This defines the "Win Condition" for the agents.

TEAM_KILL_REWARD       = _env_float("FWS_REW_KILL",       1.0)
TEAM_DMG_DEALT_REWARD  = _env_float("FWS_REW_DMG_DEALT",  0.00)
TEAM_DEATH_PENALTY     = _env_float("FWS_REW_DEATH",     -1.0) # Increased penalty for dying
TEAM_DMG_TAKEN_PENALTY = _env_float("FWS_REW_DMG_TAKEN",  0.00)

PPO_REWARD_HP_TICK         = _env_float("FWS_PPO_REW_HP_TICK", 0.3)     # Strong incentive to find healers
PPO_REWARD_KILL_INDIVIDUAL = _env_float("FWS_PPO_REW_KILL_AGENT", 5.0)  # Massive dopamine hit for executing enemies
PPO_REWARD_DEATH           = _env_float("FWS_PPO_REW_DEATH", -3.0)      # Massive punishment
PPO_REWARD_CONTESTED_CP    = _env_float("FWS_PPO_REW_CONTEST", 0.1)     # Objective play incentive

# =============================================================================
# ðŸ§  REINFORCEMENT LEARNING (PROXIMAL POLICY OPTIMIZATION)
# =============================================================================
PPO_ENABLED       = _env_bool("FWS_PPO_ENABLED", True)
PPO_RESET_LOG     = _env_bool("FWS_PPO_RESET_LOG", True)
PPO_WINDOW_TICKS  = _env_int("FWS_PPO_TICKS", 512) 

PPO_LR            = _env_float("FWS_PPO_LR", 3e-4) 
PPO_LR_T_MAX      = _env_int("FWS_PPO_T_MAX", 10_000_000) # Extended decay for long runs
PPO_LR_ETA_MIN    = _env_float("FWS_PPO_ETA_MIN", 1e-6) 

PPO_CLIP          = _env_float("FWS_PPO_CLIP", 0.2)
PPO_CLIP_EPS      = PPO_CLIP 
PPO_ENTROPY_COEF  = _env_float("FWS_PPO_ENTROPY", 0.02) # Boosted slightly to force 1000 agents to try diverse tactics
PPO_VALUE_COEF    = _env_float("FWS_PPO_VCOEF", 0.5)    
PPO_EPOCHS        = _env_int("FWS_PPO_EPOCHS", 4)       # Increased epochs for better sample efficiency
PPO_MINIBATCHES   = _env_int("FWS_PPO_MINIB", 8)
PPO_MAX_GRAD_NORM = _env_float("FWS_PPO_MAXGN", 0.5)    # Tighter gradient clipping for stability in massive crowds
PPO_GAMMA         = _env_float("FWS_PPO_GAMMA", 0.995)  # Make agents care slightly more about the future
PPO_LAMBDA        = _env_float("FWS_PPO_LAMBDA", 0.95)  
PPO_UPDATE_TICKS  = _env_int("FWS_PPO_UPDATE_TICKS", 5)

PPO_ENTROPY_BONUS = _env_float("FWS_PPO_ENTROPY_BONUS", PPO_ENTROPY_COEF)
if _env_is_set("FWS_PPO_ENTROPY_BONUS") and not _env_is_set("FWS_PPO_ENTROPY"):
    PPO_ENTROPY_COEF = float(PPO_ENTROPY_BONUS)

PER_AGENT_BRAINS        = _env_bool("FWS_PER_AGENT_BRAINS", True) 
MUTATION_PERIOD_TICKS   = _env_int("FWS_MUTATE_EVERY", 1500)
MUTATION_FRACTION_ALIVE = _env_float("FWS_MUTATE_FRAC", 0.02)

# =============================================================================
# ðŸ¤– BRAIN ARCHITECTURE (THE NEURAL ENGINES)
# =============================================================================
# The experiment: Red (Tron/Reactive) vs Blue (Mirror/Reflective)
BRAIN_KIND: str = os.getenv("FWS_BRAIN", "tron").strip().lower()
TEAM_BRAIN_ASSIGNMENT: bool = _env_bool("FWS_TEAM_BRAIN_ASSIGNMENT", True)
# ------------------------------------------------------------------
# ðŸ§  Team brain assignment mode (keeps old behavior by default)
# ------------------------------------------------------------------
# TEAM_BRAIN_ASSIGNMENT=True controls whether we do *any* team-aware logic.
#
# TEAM_BRAIN_ASSIGNMENT_MODE:
#   - "exclusive" (default): old behavior (Red=Tron, Blue=Mirror)
#   - "mix": each team can spawn BOTH architectures (use strategy below)
TEAM_BRAIN_ASSIGNMENT_MODE: str = _env_str("FWS_TEAM_BRAIN_MODE", "mix").strip().lower()

# TEAM_BRAIN_MIX_STRATEGY:
#   - "alternate" (default): deterministic 50/50 per team (tron, mirror, tron, mirror...)
#   - "random": probabilistic mix using TEAM_BRAIN_MIX_P_TRON
TEAM_BRAIN_MIX_STRATEGY: str = _env_str("FWS_TEAM_BRAIN_MIX_STRATEGY", "alternate").strip().lower()

# Only used when TEAM_BRAIN_MIX_STRATEGY == "random"
# Probability that a newly created brain is Tron (else Mirror).
TEAM_BRAIN_MIX_P_TRON: float = _env_float("FWS_TEAM_BRAIN_MIX_P_TRON", 0.5)

# Seed for ONLY the brain-mix RNG (so it doesn't affect map spawning RNG).
# If you already have RNG_SEED in config, we reuse it; otherwise default 0.
TEAM_BRAIN_MIX_SEED: int = _env_int("FWS_TEAM_BRAIN_MIX_SEED", int(globals().get("RNG_SEED", 0)))

# --- Transformer Hyperparameters ---
# Pushed up for high "IQ". d_model=128 + 8 heads allows complex multi-modal 
# processing of spatial rays and tactical tokens.
TRON_D_MODEL       = _env_int("FWS_TRON_DMODEL", 64)  # High capacity memory
TRON_HEADS         = _env_int("FWS_TRON_HEADS", 4)     # Parallel reasoning paths
TRON_DROPOUT       = _env_float("FWS_TRON_DROPOUT", 0.05)
TRON_RAY_LAYERS    = _env_int("FWS_TRON_RAY_LAYERS", 3) 
TRON_SEM_LAYERS    = _env_int("FWS_TRON_SEM_LAYERS", 2) 
TRON_FUSION_LAYERS = _env_int("FWS_TRON_FUSION_LAYERS", 2) 
TRON_MLP_HIDDEN    = _env_int("FWS_TRON_MLP_HID", 384) # Wide feed-forward
TRON_USE_ROPE      = _env_bool("FWS_TRON_ROPE", True)  
TRON_USE_PRENORM   = _env_bool("FWS_TRON_PRENORM", True)

# =============================================================================
# ðŸªž MIRROR TRANSFORMER HYPERPARAMETERS
# =============================================================================
# MirrorBrain currently exists and is used in team brain assignment.
# These knobs allow MirrorBrain to have an *independent* architecture from TronBrain.
#
# SAFETY RULE:
# - Every default mirrors the TRON_* value so nothing changes unless you override FWS_MIRROR_*.
# - Keep MIRROR_D_MODEL divisible by MIRROR_HEADS.

# Internal hidden size for MirrorBrain. Larger -> more capacity, more VRAM.
MIRROR_D_MODEL       = _env_int("FWS_MIRROR_DMODEL", int(TRON_D_MODEL))

# Number of attention heads. Higher -> more parallel attention, but d_model must be divisible by heads.
MIRROR_HEADS         = _env_int("FWS_MIRROR_HEADS", int(TRON_HEADS))

# Dropout for MirrorBrain (if/when used). Higher -> more regularization, less overfitting, slightly noisier training.
MIRROR_DROPOUT       = _env_float("FWS_MIRROR_DROPOUT", float(TRON_DROPOUT))

# Self-attention depth over ray tokens (spatial perception).
MIRROR_RAY_LAYERS    = _env_int("FWS_MIRROR_RAY_LAYERS", int(TRON_RAY_LAYERS))

# Self-attention depth over plan/semantic tokens (tactical reasoning).
MIRROR_SEM_LAYERS    = _env_int("FWS_MIRROR_SEM_LAYERS", int(TRON_SEM_LAYERS))

# Cross-attention depth fusing plan tokens with ray tokens (spatial+tactical fusion).
MIRROR_FUSION_LAYERS = _env_int("FWS_MIRROR_FUSION_LAYERS", int(TRON_FUSION_LAYERS))

# MLP size used in policy/value heads (capacity of final decision head).
MIRROR_MLP_HIDDEN    = _env_int("FWS_MIRROR_MLP_HID", int(TRON_MLP_HIDDEN))

# MirrorBrain uses the same positional/normalization strategy toggles as Tron by default.
MIRROR_USE_ROPE      = _env_bool("FWS_MIRROR_ROPE", bool(TRON_USE_ROPE))
MIRROR_USE_PRENORM   = _env_bool("FWS_MIRROR_PRENORM", bool(TRON_USE_PRENORM))


SPAWN_MODE = "uniform"
# =============================================================================
# ðŸ–¥ï¸ UI, VIEWER & SCREEN RECORDING (SMOOTH 60FPS)
# =============================================================================
ENABLE_UI  = _env_bool("FWS_UI", True)

# --- The De-coupling Magic ---
# Extracting 1000 agents' data from the GPU to the CPU every single tick will 
# freeze PyGame. By setting REFRESH_EVERY to 3, the backend RL environment 
# races ahead at max speed, while the UI only fetches visual data every 3rd frame. 
# This guarantees smooth 60fps screen recordings while the PC is at 100% load.
VIEWER_STATE_REFRESH_EVERY = _env_int("FWS_VIEWER_STATE_REFRESH_EVERY", 3) 
VIEWER_PICK_REFRESH_EVERY  = _env_int("FWS_VIEWER_PICK_REFRESH_EVERY", 3)

UI_FONT_NAME: str = _env_str("FWS_UI_FONT", "consolas")
VIEWER_CENTER_WINDOW: bool = _env_bool("FWS_VIEWER_CENTER_WINDOW", True)

# Adjusted cell size so a 160x160 map fits beautifully on a 1080p/1440p monitor (800x800 map pixels).
CELL_SIZE  = _env_int("FWS_CELL_SIZE", 5) 
HUD_WIDTH  = _env_int("FWS_HUD_W", 340) # Slightly wider HUD for better text clarity
TARGET_FPS = _env_int("FWS_TARGET_FPS", 60) # Buttery smooth for screen capping

# Optional direct-to-disk recording
RECORD_VIDEO: bool     = _env_bool("FWS_RECORD_VIDEO", False)
VIDEO_FPS: int         = _env_int("FWS_VIDEO_FPS", 60)
VIDEO_SCALE: int       = _env_int("FWS_VIDEO_SCALE", 4)
VIDEO_EVERY_TICKS: int = _env_int("FWS_VIDEO_EVERY_TICKS", 1)

# Visual Aesthetics
UI_COLORS = {
    "bg": (15, 17, 22), "hud_bg": (10, 12, 16), "side_bg": (14, 16, 20), # Deepened blacks for contrast
    "grid": (35, 37, 42), "border": (80, 85, 95), "wall": (100, 105, 115),
    "empty": (20, 22, 28),

    # Vivid team colors for recording clarity
    "red_soldier": (240, 50, 50), "red_archer":  (255, 120, 0), "red": (240, 50, 50),
    "blue_soldier": (30, 160, 255), "blue_archer":  (0, 220, 180), "blue": (30, 160, 255),

    "archer_glyph": (255, 245, 120), "marker": (255, 255, 255),
    "text": (240, 240, 245), "text_dim": (160, 165, 175),
    "green": (50, 220, 120), "warn": (255, 170, 0),
    "bar_bg": (30, 35, 40), "bar_fg": (50, 220, 120),
    "graph_red": (240, 50, 50, 180), "graph_blue": (30, 160, 255, 180),
    "graph_grid": (50, 50, 60), "pause_text": (255, 200, 50)
}
# =============================================================================
# ðŸ’¾ CHECKPOINT / RESUME
# =============================================================================
CHECKPOINT_ENABLED = _env_bool("FWS_CHECKPOINT", True)

# Save a checkpoint every N ticks (0 disables periodic saves; still can save on exit if enabled)
CHECKPOINT_EVERY_TICKS = _env_int("FWS_CHECKPOINT_EVERY_TICKS", 5000)

# Keep last N rotated snapshots (old ckpt_latest becomes ckpt_tick_XXXXXXXXXXXX.pt).
# 0 = only keep ckpt_latest.pt
CHECKPOINT_KEEP_LAST = _env_int("FWS_CHECKPOINT_KEEP_LAST", 2)

# Also save one last checkpoint when the program exits (Ctrl+C / window close / crash path).
CHECKPOINT_ON_EXIT = _env_bool("FWS_CHECKPOINT_ON_EXIT", True)

# Save brains for dead slots too (exact continuation, but larger files).
# If False, dead-slot brains are omitted (smaller ckpt, but NOT bit-exact continuation).
CHECKPOINT_SAVE_DEAD_BRAINS = _env_bool("FWS_CHECKPOINT_SAVE_DEAD_BRAINS", True)

# Directory name inside run_dir where checkpoints are written.
CHECKPOINT_DIRNAME = _env_str("FWS_CHECKPOINT_DIRNAME", "checkpoints")

# Resume support
RESUME_PATH = _env_str("FWS_RESUME_PATH", "")
RESUME_STRICT = _env_bool("FWS_RESUME_STRICT", True)

# =============================================================================
# ðŸ› ï¸ PROFILE OVERRIDE INJECTION
# =============================================================================
def _apply_profile_overrides() -> None:
    if PROFILE == "default":
        return

    presets = {
        "debug": [
            ("FWS_GRID_W", "GRID_WIDTH", 80),
            ("FWS_GRID_H", "GRID_HEIGHT", 80),
            ("FWS_START_PER_TEAM", "START_AGENTS_PER_TEAM", 30),
            ("FWS_MAX_AGENTS", "MAX_AGENTS", 160),
            ("FWS_RAND_WALLS", "RANDOM_WALLS", 6),
            ("FWS_UI", "ENABLE_UI", True),
            ("FWS_TARGET_FPS", "TARGET_FPS", 30),
            ("FWS_RECORD_VIDEO", "RECORD_VIDEO", False),
            ("FWS_USE_VMAP", "USE_VMAP", False),
        ],
        "train_fast": [
            ("FWS_UI", "ENABLE_UI", False),
            ("FWS_USE_VMAP", "USE_VMAP", True),
            ("FWS_TRON_DMODEL", "TRON_D_MODEL", 96),
            ("FWS_TRON_HEADS", "TRON_HEADS", 4),
            ("FWS_TRON_RAY_LAYERS", "TRON_RAY_LAYERS", 2),
            ("FWS_TRON_SEM_LAYERS", "TRON_SEM_LAYERS", 2),
            ("FWS_TRON_FUSION_LAYERS", "TRON_FUSION_LAYERS", 1),
            ("FWS_TRON_MLP_HID", "TRON_MLP_HIDDEN", 192),
        ],
        "train_quality": [
            ("FWS_UI", "ENABLE_UI", False),
            ("FWS_USE_VMAP", "USE_VMAP", True),
            ("FWS_TRON_DMODEL", "TRON_D_MODEL", 192),
            ("FWS_TRON_HEADS", "TRON_HEADS", 6),
            ("FWS_TRON_RAY_LAYERS", "TRON_RAY_LAYERS", 4),
            ("FWS_TRON_SEM_LAYERS", "TRON_SEM_LAYERS", 4),
            ("FWS_TRON_FUSION_LAYERS", "TRON_FUSION_LAYERS", 2),
            ("FWS_TRON_MLP_HID", "TRON_MLP_HIDDEN", 384),
        ],
    }

    rows = presets.get(PROFILE)
    if not rows: return

    g = globals()
    for env_key, var_name, value in rows:
        if not _env_is_set(env_key):
            g[var_name] = value

    if int(g.get("TRON_D_MODEL", 0)) % max(int(g.get("TRON_HEADS", 1)), 1) != 0:
        raise ValueError(f"TRON_D_MODEL must be divisible by TRON_HEADS (got {g.get('TRON_D_MODEL')} / {g.get('TRON_HEADS')})")

_apply_profile_overrides()

def summary_str() -> str:
    return (
        f"[final_war_sim: GOD LEVEL] "
        f"dev={DEVICE.type} "
        f"grid={GRID_WIDTH}x{GRID_HEIGHT} "
        f"start={START_AGENTS_PER_TEAM}/team "
        f"obs={OBS_DIM} acts={NUM_ACTIONS} "
        f"AMP={'on' if AMP_ENABLED else 'off'}"
    )
====[ END war_simulation\config.py ]============================================================


====[ 9/35 | war_simulation\dump_py_to_text.py ]=============================================
from pathlib import Path
from datetime import datetime

# --- Config ---
BASE_DIR = Path(r"C:\Kishan\RL_Project\rich_feature")
OUT_FILE = BASE_DIR / "codes" / "rich_feature.txt"

# Set to True if you want to APPEND on every run (instead of recreating fresh)
APPEND = False

# Folders to ignore anywhere in the path
IGNORE_DIRS = {
    "__pycache__", ".git", ".hg", ".svn",
    "venv", ".venv", "env", ".env",
    ".mypy_cache", ".pytest_cache", "build", "dist"
}

def should_ignore(p: Path) -> bool:
    """Return True if any part of the path is in IGNORE_DIRS."""
    return any(part in IGNORE_DIRS for part in p.parts)

def main():
    # Ensure output directory exists
    OUT_FILE.parent.mkdir(parents=True, exist_ok=True)

    # Gather all .py files (recursively), excluding ignored dirs
    py_files = []
    for p in BASE_DIR.rglob("*.py"):
        if p.is_file() and not should_ignore(p.relative_to(BASE_DIR)):
            py_files.append(p)

    # Sort deterministically by relative path
    py_files.sort(key=lambda p: str(p.relative_to(BASE_DIR)).lower())

    mode = "a" if APPEND else "w"
    with OUT_FILE.open(mode, encoding="utf-8", errors="replace") as out:
        if not APPEND:
            out.write(
                f"# Aggregated Python sources\n"
                f"# Base: {BASE_DIR}\n"
                f"# Generated: {datetime.now():%Y-%m-%d %H:%M:%S}\n"
                f"# Total files: {len(py_files)}\n"
                f"{'-'*80}\n"
            )

        for idx, f in enumerate(py_files, 1):
            rel = f.relative_to(BASE_DIR)
            header = (
                f"\n\n====[ {idx}/{len(py_files)} | {rel} ]"
                f"{'=' * max(1, 78 - len(str(rel)))}\n"
            )
            out.write(header)
            try:
                code = f.read_text(encoding="utf-8", errors="replace")
            except Exception as e:
                code = f"# [ERROR READING FILE: {e}]\n"
            out.write(code)
            out.write(f"\n====[ END {rel} ]{'=' * 60}\n")

    print(f"Done. Wrote {len(py_files)} .py files into:\n{OUT_FILE}")

if __name__ == "__main__":
    main()
====[ END war_simulation\dump_py_to_text.py ]============================================================


====[ 10/35 | war_simulation\engine\__init__.py ]=============================================
# war_simulation/engine/__init__.py
from .grid import make_grid, assert_on_same_device
from .agent_registry import AgentsRegistry

__all__ = [
    "make_grid",
    "assert_on_same_device",
    "AgentsRegistry",
]

====[ END war_simulation\engine\__init__.py ]============================================================


====[ 11/35 | war_simulation\engine\agent_registry.py ]=======================================
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict, Optional
import torch
import torch.nn as nn
import config

# ================================================================
# Column layout (Struct-of-Arrays for GPU efficiency)
# ================================================================
COL_ALIVE = 0       # float: 1.0 alive, 0.0 dead
COL_TEAM  = 1       # float: 2.0 red, 3.0 blue
COL_X     = 2       # float: x coordinate
COL_Y     = 3       # float: y coordinate
COL_HP    = 4       # float: current health points
COL_UNIT  = 5       # float: 1.0 = Soldier, 2.0 = Archer
# --- Per-Agent Attribute Columns ---
COL_HP_MAX = 6      # float: maximum health points for this agent
COL_VISION = 7      # float: vision range in cells for this agent
COL_ATK    = 8      # float: attack power for this agent
# --- NEW: UNIQUE AGENT ID ---
COL_AGENT_ID = 9    # float: permanent unique ID for this agent

# Update the total number of features
NUM_COLS = 10

# Ensure config matches this new layout if it's used elsewhere
if hasattr(config, 'AGENT_FEATURES'):
    config.AGENT_FEATURES = NUM_COLS

TEAM_RED_ID  = 2.0
TEAM_BLUE_ID = 3.0

UNIT_SOLDIER = float(getattr(config, "UNIT_SOLDIER", 1.0))
UNIT_ARCHER  = float(getattr(config, "UNIT_ARCHER", 2.0))

# ================================================================
# Buckets: allow grouping agents with same NN architecture
# ================================================================
@dataclass
class Bucket:
    signature: str
    indices: torch.Tensor          # LongTensor [K] of agent indices
    models: List[nn.Module]        # length K (same order as indices)

class AgentsRegistry:
    """
    Stores all agents in the simulation as a big tensor (SoA layout).
    Mirrors: grid occupancy channel -> registry rows.
    """

    def __init__(self, grid: torch.Tensor) -> None:
        self.grid = grid
        self.device = grid.device
        self.capacity = int(config.MAX_AGENTS)

        # --- NEW: Unique ID counter ---
        self._next_agent_id: int = 0

        # Main agent tensor (SoA layout)
        self.agent_data = torch.zeros(
            (self.capacity, NUM_COLS),
            dtype=config.TORCH_DTYPE,
            device=config.TORCH_DEVICE
        )
        self.agent_data[:, COL_ALIVE] = 0.0

        # Brains are stored separately
        self.brains: List[Optional[nn.Module]] = [None] * self.capacity
        self.generations: List[int] = [0] * self.capacity

        # Add column constants as instance attributes for easy access
        self.COL_ALIVE, self.COL_TEAM, self.COL_X, self.COL_Y, self.COL_HP, self.COL_UNIT = 0, 1, 2, 3, 4, 5
        self.COL_HP_MAX, self.COL_VISION, self.COL_ATK, self.COL_AGENT_ID = 6, 7, 8, 9


    def clear(self) -> None:
        """Reset all agents (keeps capacity)."""
        self.agent_data.zero_()
        self.agent_data[:, COL_ALIVE] = 0.0
        self.brains = [None] * self.capacity
        self.generations = [0] * self.capacity
        # --- NEW: Reset ID counter on clear ---
        self._next_agent_id = 0

    # --- NEW: Method to get the next available unique ID ---
    def get_next_id(self) -> int:
        """Returns the next available unique agent ID and increments the counter."""
        agent_id = self._next_agent_id
        self._next_agent_id += 1
        return agent_id

    def register(
        self,
        slot: int,
        *,
        agent_id: int, # --- NEW: Required parameter
        team_is_red: bool,
        x: int,
        y: int,
        hp: float,
        atk: float,
        brain: nn.Module,
        unit: float | int,
        hp_max: float,
        vision_range: int,
        generation: int = 0,
    ) -> None:
        """
        Put an agent into a fixed slot with all its attributes.
        """
        assert 0 <= slot < self.capacity
        d = self.agent_data
        d[slot, COL_ALIVE] = 1.0
        d[slot, COL_TEAM]  = TEAM_RED_ID if team_is_red else TEAM_BLUE_ID
        d[slot, COL_X]     = float(x)
        d[slot, COL_Y]     = float(y)
        d[slot, COL_HP]    = float(hp)
        d[slot, COL_ATK]   = float(atk)
        d[slot, COL_UNIT]  = float(unit)
        d[slot, COL_HP_MAX] = float(hp_max)
        d[slot, COL_VISION]= float(vision_range)
        # --- NEW: Store the permanent unique ID ---
        d[slot, COL_AGENT_ID] = float(agent_id)

        self.brains[slot]  = brain.to(self.device)
        self.generations[slot] = int(generation)

    def kill(self, slots: torch.Tensor) -> None:
        """Mark agents dead (grid clearing happens in TickEngine)."""
        if slots is None or slots.numel() == 0:
            return
        self.agent_data[slots, COL_ALIVE] = 0.0

    def positions_xy(self, indices: torch.Tensor) -> torch.Tensor:
        """Return LongTensor [(N,2)] of XY positions for given indices."""
        x = self.agent_data[indices, COL_X].to(torch.long)
        y = self.agent_data[indices, COL_Y].to(torch.long)
        return torch.stack((x, y), dim=1)

    @staticmethod
    def _signature(model: nn.Module) -> str:
        """Create a cheap architecture fingerprint (MLP-friendly)."""
        sig_parts: List[str] = [model.__class__.__name__]
        try:
            for _, m in model.named_modules():
                if isinstance(m, nn.Linear):
                    sig_parts.append(f"L({m.in_features},{m.out_features})")
        except Exception:
            pass
        return "|".join(sig_parts)

    def build_buckets(self, alive_idx: torch.Tensor) -> List[Bucket]:
        """Group alive agents by model signature for batched inference."""
        buckets_dict: Dict[str, List[int]] = {}
        for i in alive_idx.tolist():
            brain = self.brains[i]
            if brain is None:
                self.agent_data[i, COL_ALIVE] = 0.0
                continue
            key = self._signature(brain)
            buckets_dict.setdefault(key, []).append(i)

        out: List[Bucket] = []
        for key, lst in buckets_dict.items():
            idx = torch.tensor(lst, dtype=torch.long, device=self.device)
            models = [self.brains[j] for j in lst if j < len(self.brains) and self.brains[j] is not None]
            if models:
                out.append(Bucket(signature=key, indices=idx, models=models))
        return out
====[ END war_simulation\engine\agent_registry.py ]============================================================


====[ 12/35 | war_simulation\engine\game\move_mask.py ]=======================================
from __future__ import annotations
import torch
import config

# 8 directions (dx, dy): N, NE, E, SE, S, SW, W, NW
DIRS8 = torch.tensor([
    [ 0, -1],
    [ 1, -1],
    [ 1,  0],
    [ 1,  1],
    [ 0,  1],
    [-1,  1],
    [-1,  0],
    [-1, -1],
], dtype=torch.long)

@torch.no_grad()
def build_mask(
    pos_xy: torch.Tensor,            # (N,2) long/float (x,y)
    teams: torch.Tensor,             # (N,)  float: 2.0=red, 3.0=blue
    grid: torch.Tensor,              # (3,H,W) float; ch0=occ(0,1,2,3)
    unit: torch.Tensor | None = None # (N,) long/float: 1=soldier, 2=archer
) -> torch.Tensor:
    """
    Returns action mask [N, A] bool.
      A=17: idle(1) + 8 moves + 8 melee (r=1)
      A=41: idle(1) + 8 moves + 8Ã—(r=1..4) ranged
        - soldier: r=1 only
        - archer:  r=1..config.ARCHER_RANGE (clipped to 4)
    """
    device = grid.device
    N = int(pos_xy.size(0))
    H, W = int(grid.size(1)), int(grid.size(2))
    A = int(getattr(config, "NUM_ACTIONS", 17))
    mask = torch.zeros((N, A), dtype=torch.bool, device=device)

    # idle
    if A >= 1:
        mask[:, 0] = True

    if N == 0 or A <= 1:
        return mask

    # positions
    x0 = pos_xy[:, 0].to(torch.long, non_blocking=True)
    y0 = pos_xy[:, 1].to(torch.long, non_blocking=True)
    dirs = DIRS8.to(device)  # (8,2)

    # -------------------- MOVE (cols 1..8) --------------------
    nx = x0.view(N, 1) + dirs[:, 0].view(1, 8)
    ny = y0.view(N, 1) + dirs[:, 1].view(1, 8)
    inb = (nx >= 0) & (nx < W) & (ny >= 0) & (ny < H)
    nx_cl = nx.clamp(0, W - 1)
    ny_cl = ny.clamp(0, H - 1)
    occ = grid[0][ny_cl, nx_cl]          # (N,8)
    free = (occ == 0.0) & inb
    move_cols = min(8, max(0, A - 1))
    if move_cols > 0:
        mask[:, 1:1 + move_cols] = free[:, :move_cols]

    # -------------------- ATTACK --------------------
    if A <= 9:
        return mask  # no attack columns at all

    teamv = teams.to(torch.long, non_blocking=True)  # (N,)

    # Legacy 17-action: only r=1 melee per dir
    if A <= 17:
        tgt_team = occ  # r=1 neighbor occupancy
        enemy = (tgt_team != 0.0) & (tgt_team != 1.0) & (tgt_team != teamv.view(N, 1))
        k = min(8, max(0, A - 9))
        if k > 0:
            mask[:, 9:9 + k] = enemy[:, :k]
        return mask

    # 41-action layout: 8 dirs Ã— 4 ranges
    RMAX = 4
    dx = dirs[:, 0].view(1, 8, 1)  # (1,8,1)
    dy = dirs[:, 1].view(1, 8, 1)
    rvec = torch.arange(1, RMAX + 1, device=device, dtype=torch.long).view(1, 1, RMAX)

    tx = x0.view(N, 1, 1) + dx * rvec  # (N,8,4)
    ty = y0.view(N, 1, 1) + dy * rvec
    inb_r = (tx >= 0) & (tx < W) & (ty >= 0) & (ty < H)
    txc = tx.clamp(0, W - 1)
    tyc = ty.clamp(0, H - 1)

    tgt_occ = grid[0][tyc, txc]  # (N,8,4)
    enemy_r = (tgt_occ != 0.0) & (tgt_occ != 1.0) & (tgt_occ.to(torch.long) != teamv.view(N, 1, 1))
    enemy_r &= inb_r  # enforce bounds

    # Unit gating: soldiers r=1; archers r<=ARCHER_RANGE
    if unit is None:
        units = torch.full((N,), 2, device=device, dtype=torch.long)  # default permissive: archer
    else:
        units = unit.to(torch.long, non_blocking=True)

    ar_range = int(getattr(config, "ARCHER_RANGE", 4))
    ar_range = max(1, min(RMAX, ar_range))

    allow_r = torch.zeros((N, RMAX), dtype=torch.bool, device=device)  # (N,4)
    # soldiers
    allow_r[units == 1, 0] = True
    # archers
    if (units == 2).any():
        allow_r[units == 2, :ar_range] = True

    atk_ok = enemy_r & allow_r.view(N, 1, RMAX)  # (N,8,4)

    # Write contiguous 4-column blocks per direction
    base = 9
    for d in range(8):
        c0 = base + d * RMAX
        c1 = c0 + RMAX
        if c0 >= A:
            break
        cols = slice(c0, min(c1, A))
        rlim = cols.stop - cols.start  # number of range columns weâ€™ll write (<=4)
        if rlim > 0:
            mask[:, cols] = atk_ok[:, d, :rlim]

    return mask

====[ END war_simulation\engine\game\move_mask.py ]============================================================


====[ 13/35 | war_simulation\engine\grid.py ]=================================================
# final_war_sim/engine/grid.py
from __future__ import annotations
import torch
import config

import torch  # already there
from torch import Tensor  # add this
def make_grid(device: torch.device) -> torch.Tensor:
    """
    Channels:
      0: occupancy (0 empty, 1 wall, 2 red, 3 blue)
      1: hp        (0..MAX_HP)
      2: agent_id  (-1 for empty)
    """
    H, W = config.GRID_HEIGHT, config.GRID_WIDTH
    g = torch.zeros((3, H, W), dtype=config.TORCH_DTYPE, device=device)
    # walls
    g[0, 0, :] = 1.0; g[0, H-1, :] = 1.0; g[0, :, 0] = 1.0; g[0, :, W-1] = 1.0
    g[2].fill_(-1.0)
    return g

def assert_on_same_device(*tensors: torch.Tensor) -> None:
    """
    Hard guard: all tensors must be on same device & dtype matches config.torch_dtype for floats.
    """
    if not tensors:
        return
    dev = tensors[0].device
    for t in tensors:
        if t.device != dev:
            raise RuntimeError(f"Device mismatch: {dev} vs {t.device}")
        if t.is_floating_point() and t.dtype != config.TORCH_DTYPE:
            raise RuntimeError(f"Dtype mismatch: expected {config.TORCH_DTYPE}, got {t.dtype}")

====[ END war_simulation\engine\grid.py ]============================================================


====[ 14/35 | war_simulation\engine\mapgen.py ]===============================================
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Tuple
import random
import torch

import config

# Grid channels:
#   0: occupancy (0 empty, 1 wall, 2 red, 3 blue)
#   1: hp        (0..MAX_HP)
#   2: agent_id  (-1 if empty)

@dataclass
class Zones:
    """
    Immutable masks for special tiles (kept off-grid to avoid renderer/engine churn).
    Shapes: [H, W] torch.bool on config.TORCH_DEVICE.
    """
    heal_mask: torch.Tensor                  # True where tiles heal HP
    cp_masks: List[torch.Tensor]             # list of boolean masks for capture patches

    @property
    def cp_count(self) -> int:
        return len(self.cp_masks)


# --------------------------------------------------------------
# Random thin gray walls (1-cell thick, meandering segments)
# --------------------------------------------------------------
@torch.no_grad()
def add_random_walls(
    grid: torch.Tensor,
    n_segments: int = config.RANDOM_WALLS,
    seg_min: int = config.WALL_SEG_MIN,
    seg_max: int = config.WALL_SEG_MAX,
    avoid_margin: int = config.WALL_AVOID_MARGIN,
    allow_over_agents: bool = False,
) -> None:
    """
    Carve n_segments 1-cell-thick wall traces into grid[0] by writing 1.0 to occupancy.
    Designed to be called BEFORE spawning agents. If called after spawn and
    allow_over_agents=False, we skip cells that are occupied by agents (2/3).
    """
    assert grid.ndim == 3 and grid.size(0) >= 3, "grid must be (3,H,W)"
    occ = grid[0]
    H, W = int(occ.size(0)), int(occ.size(1))

    # 8-connected step vectors (dx, dy)
    dirs8 = torch.tensor(
        [[ 0, -1],[ 1, -1],[ 1,  0],[ 1,  1],[ 0,  1],[-1,  1],[-1,  0],[-1, -1]],
        dtype=torch.long, device=occ.device
    )

    def _place_wall_cell(x: int, y: int) -> None:
        if 0 <= x < W and 0 <= y < H:
            if not allow_over_agents:
                v = float(occ[y, x].item())
                if v in (2.0, 3.0):  # skip unit cells
                    return
            occ[y, x] = 1.0
            grid[1, y, x] = 0.0
            grid[2, y, x] = -1.0

    # Bounds for starts (respect avoid_margin; outer border already walled by grid maker)
    x0_min, x0_max = max(1, avoid_margin), W - max(1, avoid_margin) - 1
    y0_min, y0_max = max(1, avoid_margin), H - max(1, avoid_margin) - 1

    if x0_min >= x0_max or y0_min >= y0_max or n_segments <= 0:
        return

    for _ in range(max(0, int(n_segments))):
        x = random.randint(x0_min, x0_max)
        y = random.randint(y0_min, y0_max)
        L = random.randint(max(1, int(seg_min)), max(1, int(seg_max)))
        # start drawing
        _place_wall_cell(x, y)
        last_dir = random.randrange(8)

        for _step in range(L):
            # small turn bias to keep lines meandering, not jittering
            if random.random() < 0.70:
                d = last_dir
            else:
                d = (last_dir + random.choice([-2, -1, 1, 2])) % 8
            last_dir = d
            dx, dy = int(dirs8[d, 0].item()), int(dirs8[d, 1].item())

            # step and clamp inside interior
            x = max(1, min(W - 2, x + dx))
            y = max(1, min(H - 2, y + dy))

            # guarantee 1-cell thickness (avoid 2x2 solid blocks)
            _place_wall_cell(x, y)
            # optionally punch a gap occasionally to reduce partition risk
            if random.random() < 0.05:
                # leave a deliberate gap (do nothing)
                pass

    # done â€” leave connectivity checks for higher-level map validators if needed


# --------------------------------------------------------------
# Heal & Capture zones (rectangular patches, scaled to grid)
# --------------------------------------------------------------
@torch.no_grad()
def make_zones(
    H: int,
    W: int,
    *,
    heal_count: int = config.HEAL_ZONE_COUNT,
    heal_ratio: float = config.HEAL_ZONE_SIZE_RATIO,
    cp_count: int = config.CP_COUNT,
    cp_ratio: float = config.CP_SIZE_RATIO,
    device: torch.device | None = None,
) -> Zones:
    """
    Returns boolean masks for heal and capture zones.
    - Sizes are computed as round(ratio * grid_dimension) per side (rectangular).
    - Masks are non-overlapping where possible; if overlap occurs, that's OK (semantics are additive).
    """
    device = device or config.TORCH_DEVICE
    heal_mask = torch.zeros((H, W), dtype=torch.bool, device=device)
    cp_masks: List[torch.Tensor] = []

    # Helper to sample a rectangle that sits within the interior
    def _sample_rect(h_side: int, w_side: int) -> Tuple[int, int, int, int]:
        # keep 1-cell border clear (outer walls)
        x0 = random.randint(1, max(1, W - w_side - 2))
        y0 = random.randint(1, max(1, H - h_side - 2))
        return y0, y0 + h_side, x0, x0 + w_side  # (y0, y1, x0, x1)

    # Heal zones
    if heal_count > 0 and heal_ratio > 0.0:
        h_side = max(1, int(round(heal_ratio * H)))
        w_side = max(1, int(round(heal_ratio * W)))
        for _ in range(int(heal_count)):
            y0, y1, x0, x1 = _sample_rect(h_side, w_side)
            heal_mask[y0:y1, x0:x1] = True

    # Capture zones
    if cp_count > 0 and cp_ratio > 0.0:
        h_side = max(1, int(round(cp_ratio * H)))
        w_side = max(1, int(round(cp_ratio * W)))
        for _ in range(int(cp_count)):
            y0, y1, x0, x1 = _sample_rect(h_side, w_side)
            m = torch.zeros((H, W), dtype=torch.bool, device=device)
            m[y0:y1, x0:x1] = True
            cp_masks.append(m)

    return Zones(heal_mask=heal_mask, cp_masks=cp_masks)

====[ END war_simulation\engine\mapgen.py ]============================================================


====[ 15/35 | war_simulation\engine\ray_engine\raycast_32.py ]================================
from __future__ import annotations
from typing import Optional

import torch
import numpy as np

import config

# Grid channels:
#   0: occupancy (0 empty, 1 wall, 2 red, 3 blue)
#   1: hp        (0..MAX_HP)
#   2: agent_id  (-1 if empty)

def _generate_32_directions() -> torch.Tensor:
    """Generates 32 unique direction vectors, evenly spaced around a circle."""
    angles = np.linspace(0, 2 * np.pi, 32, endpoint=False)
    dx = np.cos(angles)
    dy = np.sin(angles)
    final_dirs = np.stack([dx, dy], axis=1)
    return torch.tensor(final_dirs, dtype=torch.float32)

DIRS32 = _generate_32_directions()

# One-hot types per first hit:
# 0 none, 1 wall, 2 red-soldier, 3 red-archer, 4 blue-soldier, 5 blue-archer  -> 6 classes
_TYPE_CLASSES = 6

@torch.no_grad()
def raycast32_firsthit(
    pos_xy: torch.Tensor,               # (N,2) long
    grid: torch.Tensor,                  # (3,H,W)
    unit_map: torch.Tensor,              # (H,W) int32 âˆˆ{-1, 1, 2}
    max_steps_each: Optional[torch.Tensor] = None,  # (N,) long â€” per-agent vision
) -> torch.Tensor:
    """
    First-hit ray features with fixed 8 dims per ray for 32 directions.
      [onehot6(none,wall,red-sold,red-arch,blue-sold,blue-arch), dist_norm, hp_norm]
    Total = 32 rays * 8 dims = 256 per agent.
    """
    device = grid.device
    dtype = getattr(config, "TORCH_DTYPE", torch.float32)

    pos_xy = pos_xy.to(dtype=torch.long, device=device)
    N = int(pos_xy.size(0))
    H, W = int(grid.size(1)), int(grid.size(2))

    # Global cap & per-agent steps
    R_global = int(getattr(config, "RAYCAST_MAX_STEPS", 10))
    if max_steps_each is None:
        max_steps_each = torch.full((N,), R_global, device=device, dtype=torch.long)
    else:
        max_steps_each = torch.clamp(
            max_steps_each.to(device=device, dtype=torch.long), 0, R_global
        )

    # Prepare coordinates for all steps up to global cap
    dirs = DIRS32.to(device).view(1, 32, 2)                     # (1,32,2)
    base = pos_xy.view(N, 1, 1, 2).float()                      # (N,1,1,2)
    steps = torch.arange(1, R_global + 1, device=device,
                         dtype=torch.float32).view(1, 1, R_global, 1)  # (1,1,S,1)

    # Float ray-march then cast to long for indexing
    coords = (base + dirs.view(1, 32, 1, 2) * steps).long()     # (N,32,S,2)
    x = coords[..., 0].clamp_(0, W - 1)                          # (N,32,S)
    y = coords[..., 1].clamp_(0, H - 1)                          # (N,32,S)

    step_ids = torch.arange(1, R_global + 1, device=device,
                            dtype=torch.long).view(1, 1, R_global)
    active = step_ids <= max_steps_each.view(N, 1, 1)            # (N,1,S)

    occ = grid[0][y, x]                                          # (N,32,S)
    hp = grid[1][y, x]                                           # (N,32,S)

    is_wall = (occ == 1) & active
    has_agent = (grid[2][y, x] >= 0) & active

    idx_wall = torch.where(is_wall.any(dim=-1),
                           is_wall.to(torch.float32).argmax(dim=-1), -1)
    idx_agent = torch.where(has_agent.any(dim=-1),
                            has_agent.to(torch.float32).argmax(dim=-1), -1)

    first_kind = torch.full((N, 32), 0, dtype=torch.int64, device=device)
    first_idx = torch.full((N, 32), -1, dtype=torch.long, device=device)

    both_hit = (idx_wall >= 0) & (idx_agent >= 0)
    only_wall = (idx_wall >= 0) & ~both_hit
    only_agent = (idx_agent >= 0) & ~both_hit

    if both_hit.any():
        earlier_is_wall = (idx_wall <= idx_agent)
        first_idx[both_hit] = torch.where(earlier_is_wall,
                                          idx_wall, idx_agent)[both_hit]
        first_kind[both_hit & earlier_is_wall] = 1
        first_kind[both_hit & ~earlier_is_wall] = -2  # temp code for agent

    if only_wall.any():
        first_idx[only_wall] = idx_wall[only_wall]
        first_kind[only_wall] = 1

    if only_agent.any():
        first_idx[only_agent] = idx_agent[only_agent]
        first_kind[only_agent] = -2  # temp code for agent

    agent_mask = (first_kind == -2)
    if agent_mask.any():
        gather_idx = first_idx.clamp_min(0).unsqueeze(-1)
        gather_y = torch.gather(y, 2, gather_idx).squeeze(-1)
        gather_x = torch.gather(x, 2, gather_idx).squeeze(-1)
        t = grid[0][gather_y, gather_x].to(torch.int32)
        u = unit_map[gather_y, gather_x].to(torch.int32)
        code = torch.zeros_like(t, dtype=torch.int64)
        code[(t == 2) & (u == 1)] = 2
        code[(t == 2) & (u == 2)] = 3
        code[(t == 3) & (u == 1)] = 4
        code[(t == 3) & (u == 2)] = 5
        first_kind[agent_mask] = code[agent_mask]

    den = max_steps_each.clamp_min(1).to(torch.float32).view(N, 1)
    dist_idx = first_idx.to(torch.float32) + 1.0
    valid = (first_idx >= 0).to(torch.float32)
    dist_norm = (dist_idx / den) * valid

    hp_first = torch.gather(hp, 2,
                            first_idx.clamp_min(0).unsqueeze(-1)).squeeze(-1) * valid

    onehot = torch.zeros((N, 32, _TYPE_CLASSES), dtype=dtype, device=device)
    idx_valid = first_kind.clamp(min=0, max=_TYPE_CLASSES - 1)
    onehot.scatter_(2, idx_valid.unsqueeze(-1), 1.0)

    max_hp = float(getattr(config, "MAX_HP", 1.0)) or 1.0
    hp_norm = (hp_first / max_hp).to(dtype)
    dist_norm = dist_norm.to(dtype)

    feat = torch.cat([onehot, dist_norm.unsqueeze(-1), hp_norm.unsqueeze(-1)], dim=-1)
    return feat.reshape(N, 32 * 8)
====[ END war_simulation\engine\ray_engine\raycast_32.py ]============================================================


====[ 16/35 | war_simulation\engine\ray_engine\raycast_64.py ]================================
from __future__ import annotations
from typing import Optional
import torch
import numpy as np

import config

# Grid channels:
#   0: occupancy (0 empty, 1 wall, 2 red, 3 blue)
#   1: hp        (0..MAX_HP)
#   2: agent_id  (-1 if empty)

def _generate_64_directions() -> torch.Tensor:
    """Generates 64 unique direction vectors, evenly spaced around a circle."""
    angles = np.linspace(0, 2 * np.pi, 64, endpoint=False)
    dx = np.cos(angles)
    dy = np.sin(angles)
    
    # Create the direction vectors as floats, without rounding or making them unique,
    # as this preserves the 64 distinct directions.
    final_dirs = np.stack([dx, dy], axis=1)
        
    return torch.tensor(final_dirs, dtype=torch.float32)

DIRS64 = _generate_64_directions()

# One-hot types per first hit:
# 0 none, 1 wall, 2 red-soldier, 3 red-archer, 4 blue-soldier, 5 blue-archer  -> 6 classes
_TYPE_CLASSES = 6

@torch.no_grad()
def raycast64_firsthit(
    pos_xy: torch.Tensor,               # (N,2) long
    grid: torch.Tensor,                 # (3,H,W)
    unit_map: torch.Tensor,             # (H,W) int32 âˆˆ{-1,1,2}
    max_steps_each: Optional[torch.Tensor] = None,  # (N,) long â€” per-agent vision
) -> torch.Tensor:
    """
    First-hit ray features with fixed 8 dims per ray for 64 directions.
      [ onehot6(none,wall,red-sold,red-arch,blue-sold,blue-arch), dist_norm, hp_norm ]
    Total = 64 rays * 8 dims = 512 per agent.
    """
    device = grid.device
    dtype = getattr(config, "TORCH_DTYPE", torch.float32)

    pos_xy = pos_xy.to(dtype=torch.long, device=device)
    N = int(pos_xy.size(0))
    H, W = int(grid.size(1)), int(grid.size(2))

    # Global cap & per-agent steps
    R_global = int(getattr(config, "RAYCAST_MAX_STEPS", 10))
    if max_steps_each is None:
        max_steps_each = torch.full((N,), R_global, device=device, dtype=torch.long)
    else:
        max_steps_each = torch.clamp(max_steps_each.to(device=device, dtype=torch.long), 0, R_global)

    # Prepare coordinates for all steps up to global cap
    dirs = DIRS64.to(device).view(1, 64, 2)                  # (1,64,2)
    base = pos_xy.view(N, 1, 1, 2).float()                   # (N,1,1,2)
    steps = torch.arange(1, R_global + 1, device=device, dtype=torch.float32).view(1, 1, R_global, 1)  # (1,1,S,1)
    
    # Calculate ray coordinates using float arithmetic and then cast to long for indexing
    coords_float = base + dirs.view(1, 64, 1, 2) * steps
    coords = coords_float.long() # (N,64,S,2)

    x = coords[..., 0].clamp_(0, W - 1)                      # (N,64,S)
    y = coords[..., 1].clamp_(0, H - 1)                      # (N,64,S)

    step_ids = torch.arange(1, R_global + 1, device=device, dtype=torch.long).view(1, 1, R_global)
    active = step_ids <= max_steps_each.view(N, 1, 1)        # (N,1,S)

    occ = grid[0][y, x]                                      # (N,64,S)
    hp  = grid[1][y, x]                                      # (N,64,S)
    
    is_wall = (occ == 1) & active
    has_agent = (grid[2][y, x] >= 0) & active

    idx_wall = torch.where(is_wall.any(dim=-1), is_wall.to(torch.float32).argmax(dim=-1), -1)
    idx_agent = torch.where(has_agent.any(dim=-1), has_agent.to(torch.float32).argmax(dim=-1), -1)

    first_kind = torch.full((N, 64), 0, dtype=torch.int64, device=device)
    first_idx  = torch.full((N, 64), -1, dtype=torch.long, device=device)

    both_hit = (idx_wall >= 0) & (idx_agent >= 0)
    only_wall = (idx_wall >= 0) & ~both_hit
    only_agent = (idx_agent >= 0) & ~both_hit

    if both_hit.any():
        earlier_is_wall = (idx_wall <= idx_agent)
        first_idx[both_hit] = torch.where(earlier_is_wall, idx_wall, idx_agent)[both_hit]
        first_kind[both_hit & earlier_is_wall] = 1
        first_kind[both_hit & ~earlier_is_wall] = -2 # Temp code for agent

    if only_wall.any():
        first_idx[only_wall] = idx_wall[only_wall]
        first_kind[only_wall] = 1

    if only_agent.any():
        first_idx[only_agent] = idx_agent[only_agent]
        first_kind[only_agent] = -2 # Temp code for agent

    agent_mask = (first_kind == -2)
    if agent_mask.any():
        gather_idx = first_idx.clamp_min(0).unsqueeze(-1)
        gather_y = torch.gather(y, 2, gather_idx).squeeze(-1)
        gather_x = torch.gather(x, 2, gather_idx).squeeze(-1)
        t = grid[0][gather_y, gather_x].to(torch.int32)
        u = unit_map[gather_y, gather_x].to(torch.int32)
        code = torch.zeros_like(t, dtype=torch.int64)
        code[(t == 2) & (u == 1)] = 2
        code[(t == 2) & (u == 2)] = 3
        code[(t == 3) & (u == 1)] = 4
        code[(t == 3) & (u == 2)] = 5
        first_kind[agent_mask] = code[agent_mask]

    den = max_steps_each.clamp_min(1).to(torch.float32).view(N, 1)
    dist_idx = first_idx.to(torch.float32) + 1.0
    valid = (first_idx >= 0).to(torch.float32)
    dist_norm = (dist_idx / den) * valid

    hp_first = torch.gather(hp, 2, first_idx.clamp_min(0).unsqueeze(-1)).squeeze(-1) * valid
    onehot = torch.zeros((N, 64, _TYPE_CLASSES), dtype=dtype, device=device)
    idx_valid = first_kind.clamp(min=0, max=_TYPE_CLASSES - 1)
    onehot.scatter_(2, idx_valid.unsqueeze(-1), 1.0)

    max_hp = float(getattr(config, "MAX_HP", 1.0)) or 1.0
    hp_norm = (hp_first / max_hp).to(dtype)
    dist_norm = dist_norm.to(dtype)

    feat = torch.cat([onehot, dist_norm.unsqueeze(-1), hp_norm.unsqueeze(-1)], dim=-1)
    return feat.reshape(N, 64 * 8)

====[ END war_simulation\engine\ray_engine\raycast_64.py ]============================================================


====[ 17/35 | war_simulation\engine\ray_engine\raycast_firsthit.py ]==========================
from __future__ import annotations
from typing import Optional
import torch

import config

# Grid channels:
#   0: occupancy (0 empty, 1 wall, 2 red, 3 blue)
#   1: hp        (0..MAX_HP)
#   2: agent_id  (-1 if empty)

# 8 directions (dx, dy): N, NE, E, SE, S, SW, W, NW (consistent with move_mask)
DIRS8 = torch.tensor([
    [ 0, -1],
    [ 1, -1],
    [ 1,  0],
    [ 1,  1],
    [ 0,  1],
    [-1,  1],
    [-1,  0],
    [-1, -1],
], dtype=torch.long)

# One-hot types per first hit:
# 0 none, 1 wall, 2 red-soldier, 3 red-archer, 4 blue-soldier, 5 blue-archer  -> 6 classes
_TYPE_CLASSES = 6


@torch.no_grad()
def build_unit_map(agent_data: torch.Tensor, grid: torch.Tensor) -> torch.Tensor:
    """
    Build an HxW int32 map of unit types from registry tensor and grid agent ids.
    -1 where empty/no agent; else 1 (soldier) or 2 (archer).
    """
    H, W = int(grid.size(1)), int(grid.size(2))
    unit_map = torch.full((H, W), -1, dtype=torch.int32, device=grid.device)

    ids = grid[2].to(torch.long)  # (H,W) -1 if empty
    has_agent = ids >= 0
    if not has_agent.any():
        return unit_map

    # Gather unit types by agent id and scatter into map
    # agent_data shape: (N, features); COL_UNIT assumed to be float with values {1.0,2.0}
    from ..agent_registry import COL_UNIT  # local import to avoid circulars
    units_by_id = agent_data[:, COL_UNIT].to(torch.int32)  # (N,)
    picked = torch.where(
        has_agent,
        units_by_id[ids.clamp_min(0)],
        torch.tensor(-1, device=grid.device, dtype=torch.int32),
    )
    unit_map.copy_(picked)
    return unit_map


@torch.no_grad()
def raycast8_firsthit(
    pos_xy: torch.Tensor,               # (N,2) long
    grid: torch.Tensor,                 # (3,H,W)
    unit_map: torch.Tensor,             # (H,W) int32 âˆˆ{-1,1,2}
    max_steps_each: Optional[torch.Tensor] = None,  # (N,) long â€” per-agent vision; optional
) -> torch.Tensor:
    """
    First-hit ray features with fixed 8 dims per ray:
      [ onehot6(none,wall,red-sold,red-arch,blue-sold,blue-arch), dist_norm, hp_norm ]
    Total = 8 rays * 8 dims = 64 per agent.

    New: accepts per-agent max range via `max_steps_each`. If None, uses
         `config.RAYCAST_MAX_STEPS` (or 10 if missing).
    """
    device = grid.device
    dtype = getattr(config, "TORCH_DTYPE", torch.float32)

    pos_xy = pos_xy.to(dtype=torch.long, device=device)
    N = int(pos_xy.size(0))
    H, W = int(grid.size(1)), int(grid.size(2))

    # Global cap & per-agent steps
    R_global = int(getattr(config, "RAYCAST_MAX_STEPS", 10))
    if max_steps_each is None:
        max_steps_each = torch.full((N,), R_global, device=device, dtype=torch.long)
    else:
        max_steps_each = torch.clamp(max_steps_each.to(device=device, dtype=torch.long), 0, R_global)

    # Prepare coordinates for all steps up to global cap
    dirs = DIRS8.to(device).view(1, 8, 2)                    # (1,8,2)
    base = pos_xy.view(N, 1, 1, 2)                           # (N,1,1,2)
    steps = torch.arange(1, R_global + 1, device=device, dtype=torch.long).view(1, 1, R_global, 1)  # (1,1,S,1)
    coords = base + dirs.view(1, 8, 1, 2) * steps            # (N,8,S,2)

    x = coords[..., 0].clamp_(0, W - 1)                      # (N,8,S)
    y = coords[..., 1].clamp_(0, H - 1)                      # (N,8,S)

    # Active mask per agent per step (agents cannot see beyond their own max range)
    step_ids = torch.arange(1, R_global + 1, device=device, dtype=torch.long).view(1, 1, R_global)
    active = step_ids <= max_steps_each.view(N, 1, 1)        # (N,1,S)

    occ = grid[0][y, x]                                      # (N,8,S)
    hp  = grid[1][y, x]                                      # (N,8,S)
    uid = unit_map[y, x]                                     # (N,8,S) âˆˆ {-1,1,2}

    # Determine first hit per (N,8)
    # Case order of precedence: wall, agent, else none
    is_wall = (occ == 1) & active                            # (N,8,S) but active is (N,1,S) -> broadcast
    has_agent = (grid[2][y, x] >= 0) & active

    # First wall step index
    any_wall = is_wall.any(dim=-1)
    idx_wall = torch.where(
        any_wall,
        is_wall.to(torch.float32).argmax(dim=-1),
        torch.full(is_wall.shape[:-1], -1, device=device, dtype=torch.long),
    )  # (N,8)

    # First agent step index
    any_agent = has_agent.any(dim=-1)
    idx_agent = torch.where(
        any_agent,
        has_agent.to(torch.float32).argmax(dim=-1),
        torch.full(has_agent.shape[:-1], -1, device=device, dtype=torch.long),
    )  # (N,8)

    # Resolve which occurs first (prefer smaller non-negative index)
    first_kind = torch.full((N, 8), 0, dtype=torch.int64, device=device)  # 0 none
    first_idx  = torch.full((N, 8), -1, dtype=torch.long, device=device)

    both_hit = (idx_wall >= 0) & (idx_agent >= 0)
    only_wall = (idx_wall >= 0) & (~(idx_agent >= 0))
    only_agent = (~(idx_wall >= 0)) & (idx_agent >= 0)

    if both_hit.any():
        earlier_is_wall = (idx_wall <= idx_agent)
        fi = torch.where(earlier_is_wall, idx_wall, idx_agent)
        first_idx[both_hit] = fi[both_hit]
        first_kind[both_hit & earlier_is_wall] = 1  # wall
        first_kind[both_hit & (~earlier_is_wall)] = -2  # agent (temp)

    if only_wall.any():
        first_idx[only_wall] = idx_wall[only_wall]
        first_kind[only_wall] = 1  # wall

    if only_agent.any():
        first_idx[only_agent] = idx_agent[only_agent]
        first_kind[only_agent] = -2  # agent (temp)

    # Build type codes for agent hits using team+unit
    agent_mask = (first_kind == -2)
    if agent_mask.any():
        gather_y = torch.gather(y, 2, first_idx.clamp_min(0).unsqueeze(-1)).squeeze(-1)  # (N,8)
        gather_x = torch.gather(x, 2, first_idx.clamp_min(0).unsqueeze(-1)).squeeze(-1)  # (N,8)
        t = grid[0][gather_y, gather_x].to(torch.int32)  # (N,8)
        u = unit_map[gather_y, gather_x].to(torch.int32) # (N,8)
        code = torch.full_like(t, 0, dtype=torch.int64)
        # red
        code[(t == 2) & (u == 1)] = 2
        code[(t == 2) & (u == 2)] = 3
        # blue
        code[(t == 3) & (u == 1)] = 4
        code[(t == 3) & (u == 2)] = 5
        first_kind[agent_mask] = code[agent_mask]

    # Distance normalized by each agent's own vision max
    den = max_steps_each.clamp_min(1).to(torch.float32).view(N, 1).expand(N, 8)
    dist_idx = first_idx.to(torch.float32) + 1.0  # steps start at 1
    valid = (first_idx >= 0).to(torch.float32)
    dist_norm = (dist_idx / den) * valid

    # Gather hp at first hit
    hp_first = torch.gather(hp, 2, first_idx.clamp_min(0).unsqueeze(-1)).squeeze(-1)  # (N,8)
    hp_first = hp_first * valid

    # One-hot over 6 classes
    onehot = torch.zeros((N, 8, _TYPE_CLASSES), dtype=dtype, device=device)
    idx_valid = first_kind.clamp(min=0, max=_TYPE_CLASSES - 1)
    onehot.scatter_(2, idx_valid.unsqueeze(-1), 1.0)

    # Normalize hp
    max_hp = float(getattr(config, "MAX_HP", 1.0))
    if max_hp <= 0:  # avoid div by zero
        max_hp = 1.0
    hp_norm = (hp_first / max_hp).to(dtype)
    dist_norm = dist_norm.to(dtype)

    # Concatenate per ray: onehot(6) + dist_norm + hp_norm -> 8 dims
    feat = torch.cat([onehot, dist_norm.unsqueeze(-1), hp_norm.unsqueeze(-1)], dim=-1)  # (N,8,8)
    return feat.reshape(N, 8 * 8)

====[ END war_simulation\engine\ray_engine\raycast_firsthit.py ]============================================================


====[ 18/35 | war_simulation\engine\respawn.py ]==============================================
from __future__ import annotations
from dataclasses import dataclass, field
from typing import Optional, Tuple
import random
import copy
import random

import torch
import torch.jit

import config
from .agent_registry import (
    AgentsRegistry,
    COL_ALIVE,
    COL_TEAM,
    COL_X,
    COL_Y,
    COL_HP,
    COL_HP_MAX,
    COL_VISION,
    COL_ATK,
    COL_UNIT,
    TEAM_RED_ID,
    TEAM_BLUE_ID,
)
from agent.transformer_brain import TransformerBrain, scripted_transformer_brain
from agent.tron_brain import TronBrain
from agent.mirror_brain import MirrorBrain

# ----------------------------------------------------------------------
# Respawn configuration dataclass (extended with new parameters)
# ----------------------------------------------------------------------
@dataclass
class RespawnCfg:
    """Configuration for agent respawning.

    This dataclass combines the original simple probabilistic respawn
    parameters with the new advanced controller parameters. All fields
    have defaults pulled from the global config module for backward
    compatibility and ease of use.
    """
    # Master switch
    enabled: bool = True

    # ---------- Legacy simple probabilistic fields ----------
    prob_per_dead_per_tick: float = 0.05      # Not used in new controller, kept for API stability
    spawn_tries: int = 200                    # Maximum attempts to find a free cell
    mutation_std: float = 0.02                 # Noise std when mutating cloned brains
    clone_prob: float = 0.50                   # Probability to clone an existing agent instead of creating a fresh brain
    use_team_elite: bool = True                 # Whether to consider only alive agents from the same team as parents
    reset_optimizer_on_respawn: bool = True     # Whether to reset optimizers (for PPO)

    # ---------- New controller fields ----------
    floor_per_team: int = field(default_factory=lambda: int(getattr(config, "RESP_FLOOR_PER_TEAM", 50)))
    """Minimum desired number of alive agents per team; if below, respawn tries to fill up."""

    max_per_tick: int = field(default_factory=lambda: int(getattr(config, "RESP_MAX_PER_TICK", 5)))
    """Maximum number of respawns per team per tick."""

    period_ticks: int = field(default_factory=lambda: int(getattr(config, "RESP_PERIOD_TICKS", 500)))
    """Interval (in ticks) at which a periodic respawn budget is distributed."""

    period_budget: int = field(default_factory=lambda: int(getattr(config, "RESP_PERIOD_BUDGET", 20)))
    """Total number of respawns to distribute among teams every period_ticks."""

    cooldown_ticks: int = field(default_factory=lambda: int(getattr(config, "RESP_HYST_COOLDOWN_TICKS", 30)))
    """Cooldown after reaching the floor for a team before another floorâ€‘based respawn is allowed."""

    wall_margin: int = field(default_factory=lambda: int(getattr(config, "RESP_WALL_MARGIN", 2)))
    """Minimum distance from the grid border for spawn positions."""

    # Unit type configuration (pulled from config)
    unit_soldier: int = field(default_factory=lambda: int(getattr(config, "UNIT_SOLDIER", 1)))
    unit_archer: int = field(default_factory=lambda: int(getattr(config, "UNIT_ARCHER", 2)))
    spawn_archer_ratio: float = field(default_factory=lambda: float(getattr(config, "SPAWN_ARCHER_RATIO", 0.40)))
    soldier_hp: float = field(default_factory=lambda: float(getattr(config, "SOLDIER_HP", 1.0)))
    soldier_atk: float = field(default_factory=lambda: float(getattr(config, "SOLDIER_ATK", 0.05)))
    archer_hp: float = field(default_factory=lambda: float(getattr(config, "ARCHER_HP", 1.0)))
    archer_atk: float = field(default_factory=lambda: float(getattr(config, "ARCHER_ATK", 0.02)))
    vision_soldier: int = field(default_factory=lambda: int(getattr(config, "VISION_RANGE_BY_UNIT", {}).get(1, 10)))
    vision_archer: int = field(default_factory=lambda: int(getattr(config, "VISION_RANGE_BY_UNIT", {}).get(2, 15)))

# ----------------------------------------------------------------------
# Global counter for rare mutation events
# ----------------------------------------------------------------------
_respawn_counter = 0


# ----------------------------------------------------------------------
# Helper functions for team counts and distribution
# ----------------------------------------------------------------------
@dataclass
class TeamCounts:
    """Simple container for alive agent counts per team."""
    red: int
    blue: int


def _team_counts(reg: AgentsRegistry) -> TeamCounts:
    """Return the number of alive agents for each team."""
    d = reg.agent_data
    alive = (d[:, COL_ALIVE] > 0.5)
    red = int((alive & (d[:, COL_TEAM] == TEAM_RED_ID)).sum().item())
    blue = int((alive & (d[:, COL_TEAM] == TEAM_BLUE_ID)).sum().item())
    return TeamCounts(red=red, blue=blue)


def _inverse_split(a: int, b: int, budget: int) -> Tuple[int, int]:
    """
    Split a budget inversely proportional to the two numbers.
    Used to give more respawns to the team with fewer alive agents.
    """
    a = max(1, a)
    b = max(1, b)
    s = 1.0 / a + 1.0 / b
    qa = int(round(budget * (1.0 / a) / s))
    return qa, budget - qa


def _cap(n: int, cfg: RespawnCfg) -> int:
    """Clamp a desired respawn count to the perâ€‘tick maximum."""
    return max(0, min(n, cfg.max_per_tick))

# ----------------------------------------------------------------------
# Team brain selection (supports exclusive split and mixed teams)
# ----------------------------------------------------------------------

_TEAM_BRAIN_MIX_COUNTER = {TEAM_RED_ID: 0, TEAM_BLUE_ID: 0}

def _make_team_mix_rng(team_id: float):
    seed = int(getattr(config, "TEAM_BRAIN_MIX_SEED", 0))
    if seed == 0:
        return random.SystemRandom()
    salt = 101 if team_id == TEAM_RED_ID else 202
    return random.Random(seed + salt)

_TEAM_BRAIN_MIX_RNG = {
    TEAM_RED_ID: _make_team_mix_rng(TEAM_RED_ID),
    TEAM_BLUE_ID: _make_team_mix_rng(TEAM_BLUE_ID),
}

def _resolve_team_brain_kind_from_team(team_id: float) -> str:
    mode = str(getattr(config, "TEAM_BRAIN_ASSIGNMENT_MODE", "exclusive")).strip().lower()

    if mode in ("exclusive", "split", "team"):
        if team_id == TEAM_RED_ID:
            return "tron"
        if team_id == TEAM_BLUE_ID:
            return "mirror"
        return str(getattr(config, "BRAIN_KIND", "tron")).strip().lower()

    if mode in ("mix", "hybrid", "both"):
        strategy = str(getattr(config, "TEAM_BRAIN_MIX_STRATEGY", "alternate")).strip().lower()

        if strategy in ("alternate", "roundrobin", "rr"):
            i = _TEAM_BRAIN_MIX_COUNTER.get(team_id, 0)
            _TEAM_BRAIN_MIX_COUNTER[team_id] = i + 1
            return "tron" if (i % 2 == 0) else "mirror"

        if strategy in ("random", "prob", "probabilistic"):
            p_tron = float(getattr(config, "TEAM_BRAIN_MIX_P_TRON", 0.5))
            p_tron = max(0.0, min(1.0, p_tron))
            rng = _TEAM_BRAIN_MIX_RNG.get(team_id, random.SystemRandom())
            return "tron" if (rng.random() < p_tron) else "mirror"

        raise ValueError(f"Unknown TEAM_BRAIN_MIX_STRATEGY={strategy!r}")

    raise ValueError(f"Unknown TEAM_BRAIN_ASSIGNMENT_MODE={mode!r}")

def _infer_kind_from_parent(parent: torch.nn.Module) -> Optional[str]:
    # ScriptModule counts as transformer for our purposes
    if isinstance(parent, torch.jit.ScriptModule):
        return "transformer"
    if isinstance(parent, TronBrain):
        return "tron"
    if isinstance(parent, MirrorBrain):
        return "mirror"
    if isinstance(parent, TransformerBrain):
        return "transformer"
    return None

# ----------------------------------------------------------------------
# Brain creation and cloning (preserves teamâ€‘specific assignment)
# ----------------------------------------------------------------------
def _new_brain(device: torch.device, *, team_id: Optional[float] = None) -> torch.nn.Module:
    """Create a new brain module.

    - Non-PPO: scripted transformer (existing behavior).
    - PPO:
        If TEAM_BRAIN_ASSIGNMENT is enabled and team_id is known:
            - exclusive: red=tron, blue=mirror (old)
            - mix: per-team mixed assignment (alternate/random)
        Otherwise: falls back to BRAIN_KIND.
    """
    obs_dim = int(getattr(config, "OBS_DIM", 0))
    act_dim = int(getattr(config, "NUM_ACTIONS", 41))

    is_ppo = bool(getattr(config, "PPO_ENABLED", False))
    if not is_ppo:
        return scripted_transformer_brain(obs_dim, act_dim).to(device)

    team_assign = bool(getattr(config, "TEAM_BRAIN_ASSIGNMENT", True))
    if team_assign and team_id is not None:
        brain_kind = _resolve_team_brain_kind_from_team(float(team_id))
    else:
        brain_kind = str(getattr(config, "BRAIN_KIND", "tron")).strip().lower()

    if brain_kind == "transformer":
        return TransformerBrain(obs_dim, act_dim).to(device)
    if brain_kind == "mirror":
        return MirrorBrain(obs_dim, act_dim).to(device)
    return TronBrain(obs_dim, act_dim).to(device)



def _clone_brain(
    parent: Optional[torch.nn.Module],
    device: torch.device,
    *,
    team_id: Optional[float] = None,
) -> torch.nn.Module:
    """Clone a parent brain.

    - Non-PPO: try deepcopy, else fall back to fresh.
    - PPO:
        * exclusive mode: may force architecture by team (old behavior)
        * mix mode: preserve parent's architecture when possible
    """
    if parent is None:
        return _new_brain(device, team_id=team_id)

    obs_dim = int(getattr(config, "OBS_DIM", 0))
    act_dim = int(getattr(config, "NUM_ACTIONS", 41))

    is_ppo = bool(getattr(config, "PPO_ENABLED", False))
    if not is_ppo:
        try:
            return copy.deepcopy(parent)
        except Exception:
            return _new_brain(device, team_id=team_id)

    team_assign = bool(getattr(config, "TEAM_BRAIN_ASSIGNMENT", True))
    mode = str(getattr(config, "TEAM_BRAIN_ASSIGNMENT_MODE", "exclusive")).strip().lower()

    parent_kind = _infer_kind_from_parent(parent)

    if team_assign and team_id is not None:
        # exclusive: old behavior (force by team)
        if mode in ("exclusive", "split", "team"):
            brain_kind = _resolve_team_brain_kind_from_team(float(team_id))
        else:
            # mix: keep parent architecture if we can
            brain_kind = parent_kind or _resolve_team_brain_kind_from_team(float(team_id))
    else:
        brain_kind = str(getattr(config, "BRAIN_KIND", "tron")).strip().lower()

    # Instantiate + load weights only if compatible
    if brain_kind == "transformer":
        child = TransformerBrain(obs_dim, act_dim).to(device)
        if isinstance(parent, (torch.jit.ScriptModule, TransformerBrain)):
            child.load_state_dict(parent.state_dict())
        return child

    if brain_kind == "mirror":
        child = MirrorBrain(obs_dim, act_dim).to(device)
        if isinstance(parent, MirrorBrain):
            child.load_state_dict(parent.state_dict())
        return child

    # Default: tron
    child = TronBrain(obs_dim, act_dim).to(device)
    if isinstance(parent, TronBrain):
        child.load_state_dict(parent.state_dict())
    return child



@torch.no_grad()
def _perturb_brain_(brain: torch.nn.Module, std: float) -> None:
    """Add small Gaussian noise to all trainable parameters."""
    if std <= 0.0:
        return
    for p in brain.parameters():
        if p.requires_grad:
            p.add_(torch.randn_like(p) * std)


# ----------------------------------------------------------------------
# Spawn cell selection (with wall margin)
# ----------------------------------------------------------------------
def _cell_free(grid: torch.Tensor, x: int, y: int, cfg: RespawnCfg) -> bool:
    """Check if a cell is free for spawning (no wall, no agent)."""
    H, W = grid.shape[1], grid.shape[2]
    return (cfg.wall_margin <= x < W - cfg.wall_margin and
            cfg.wall_margin <= y < H - cfg.wall_margin and
            grid[0, y, x].item() == 0.0 and      # not a wall
            grid[1, y, x].item() == 0.0)          # no agent present


def _pick_uniform(grid: torch.Tensor, cfg: RespawnCfg) -> Tuple[int, int]:
    """Randomly sample a free cell with uniform distribution."""
    H, W = grid.shape[1], grid.shape[2]
    for _ in range(cfg.spawn_tries):
        x = random.randint(cfg.wall_margin, W - cfg.wall_margin - 1)
        y = random.randint(cfg.wall_margin, H - cfg.wall_margin - 1)
        if _cell_free(grid, x, y, cfg):
            return x, y
    return -1, -1  # failure


def _pick_location(grid: torch.Tensor, cfg: RespawnCfg) -> Tuple[int, int]:
    """Main entry point for spawn cell selection. Currently uniform."""
    return _pick_uniform(grid, cfg)


# ----------------------------------------------------------------------
# Unit type and stats (from config)
# ----------------------------------------------------------------------
def _choose_unit(cfg: RespawnCfg) -> int:
    """Randomly choose a unit type based on spawn_archer_ratio."""
    return cfg.unit_archer if random.random() < cfg.spawn_archer_ratio else cfg.unit_soldier


def _unit_stats(unit_id: int, cfg: RespawnCfg) -> Tuple[float, float, int]:
    """Return (hp, atk, vision_range) for the given unit type."""
    if unit_id == cfg.unit_archer:
        return cfg.archer_hp, cfg.archer_atk, cfg.vision_archer
    return cfg.soldier_hp, cfg.soldier_atk, cfg.vision_soldier


# ----------------------------------------------------------------------
# Write agent to registry (direct tensor assignment, preserves old behavior)
# ----------------------------------------------------------------------
def _write_agent_to_registry(
    reg: AgentsRegistry,
    slot: int,
    team_id: float,
    x: int,
    y: int,
    unit_id: int,
    hp: float,
    atk: float,
    vision: int,
    brain: torch.nn.Module,
) -> None:
    """Fill the registry tensors and brain list for a newly spawned agent."""
    reg.agent_data[slot, COL_ALIVE] = 1.0
    reg.agent_data[slot, COL_TEAM] = team_id
    reg.agent_data[slot, COL_X] = float(x)
    reg.agent_data[slot, COL_Y] = float(y)
    reg.agent_data[slot, COL_HP] = hp
    reg.agent_data[slot, COL_HP_MAX] = hp
    reg.agent_data[slot, COL_VISION] = float(vision)
    reg.agent_data[slot, COL_ATK] = atk
    reg.agent_data[slot, COL_UNIT] = float(unit_id)

    # Replace brain and clear optimizer (optimizer will be recreated by PPO if needed)
    reg.brains[slot] = brain
    if hasattr(reg, "optimizers"):
        reg.optimizers[slot] = None


# ----------------------------------------------------------------------
# Core respawn function (spawns a given number of agents for a team)
# ----------------------------------------------------------------------
@torch.no_grad()
def _respawn_some(
    reg: AgentsRegistry,
    grid: torch.Tensor,
    team_id: float,
    count: int,
    cfg: RespawnCfg,
) -> int:
    """
    Attempt to spawn up to `count` agents of the given team.

    Returns the number actually spawned (may be less due to lack of free cells).
    """
    global _respawn_counter

    if count <= 0:
        return 0

    # Find dead slots (alive == 0) that belong to the same team (optional, but safe)
    data = reg.agent_data
    alive = (data[:, COL_ALIVE] > 0.5)
    dead_slots = (~alive).nonzero(as_tuple=False).squeeze(1)
    if dead_slots.numel() == 0:
        return 0

    # Gather potential parents (alive agents of the same team)
    parents = (alive & (data[:, COL_TEAM] == team_id)).nonzero(as_tuple=False).squeeze(1)

    spawned = 0
    # We will iterate over dead slots sequentially; stop when we have spawned enough or run out.
    for k in range(min(count, dead_slots.numel())):
        slot = int(dead_slots[k].item())
        x, y = _pick_location(grid, cfg)
        if x < 0:  # no free cell found
            break

        # Choose unit type and base stats
        unit_id = _choose_unit(cfg)
        hp0, atk0, vision0 = _unit_stats(unit_id, cfg)

        # Rare mutation event (every 1000th respawn globally)
        _respawn_counter += 1
        if _respawn_counter % 1000 == 0:
            hp0 *= (1.0 + random.uniform(0.5, 2.0))
            atk0 *= (1.0 + random.uniform(0.5, 2.0))
            vision0 = int(vision0 * (1.0 + random.uniform(0.5, 2.0)))
            print(f"** Rare Mutation on slot {slot} (team {'red' if team_id==TEAM_RED_ID else 'blue'})! "
                  f"HP:{hp0:.2f}, ATK:{atk0:.2f}, VIS:{vision0} **")

        # Decide whether to clone an existing parent or create a fresh brain
        use_clone = (parents.numel() > 0) and (random.random() < cfg.clone_prob)
        if use_clone:
            pj = int(parents[random.randrange(parents.numel())].item())
            brain = _clone_brain(reg.brains[pj], reg.device, team_id=team_id)
            _perturb_brain_(brain, cfg.mutation_std)
        else:
            brain = _new_brain(reg.device, team_id=team_id)

        # Write to registry
        _write_agent_to_registry(reg, slot, team_id, x, y, unit_id, hp0, atk0, vision0, brain)

        # Update grid: channel 0 = wall (unchanged), channel 1 = agent presence, channel 2 = team
        grid[1, y, x] = 1.0
        grid[2, y, x] = team_id

        spawned += 1

    return spawned


# ----------------------------------------------------------------------
# RespawnController: advanced teamâ€‘aware respawn logic
# ----------------------------------------------------------------------
class RespawnController:
    """Controller that manages floorâ€‘based and periodic respawning."""

    def __init__(self, cfg: RespawnCfg):
        self.cfg = cfg
        self._cooldown_red_until = 0
        self._cooldown_blue_until = 0
        self._last_period_tick = 0

    def step(self, tick: int, reg: AgentsRegistry, grid: torch.Tensor) -> Tuple[int, int]:
        """
        Execute one step of the respawn logic.

        Returns:
            Tuple (spawned_red, spawned_blue) with the numbers of agents spawned this tick.
        """
        if not self.cfg.enabled:
            return 0, 0

        counts = _team_counts(reg)
        spawned_r = spawned_b = 0

        # Floorâ€‘based respawn (with cooldown)
        if counts.red < self.cfg.floor_per_team and tick >= self._cooldown_red_until:
            need = self.cfg.floor_per_team - counts.red
            spawned = _respawn_some(reg, grid, TEAM_RED_ID, _cap(need, self.cfg), self.cfg)
            spawned_r += spawned
            if counts.red + spawned >= self.cfg.floor_per_team:
                self._cooldown_red_until = tick + self.cfg.cooldown_ticks

        if counts.blue < self.cfg.floor_per_team and tick >= self._cooldown_blue_until:
            need = self.cfg.floor_per_team - counts.blue
            spawned = _respawn_some(reg, grid, TEAM_BLUE_ID, _cap(need, self.cfg), self.cfg)
            spawned_b += spawned
            if counts.blue + spawned >= self.cfg.floor_per_team:
                self._cooldown_blue_until = tick + self.cfg.cooldown_ticks

        # Periodic respawn (inverse proportional to current team sizes)
        if tick - self._last_period_tick >= self.cfg.period_ticks:
            self._last_period_tick = tick
            total_alive = counts.red + counts.blue
            if total_alive > 0:
                q_r, q_b = _inverse_split(counts.red, counts.blue, self.cfg.period_budget)
                spawned_r += _respawn_some(reg, grid, TEAM_RED_ID, _cap(q_r, self.cfg), self.cfg)
                spawned_b += _respawn_some(reg, grid, TEAM_BLUE_ID, _cap(q_b, self.cfg), self.cfg)

        return spawned_r, spawned_b


# ----------------------------------------------------------------------
# Public API: respawn_tick (backward compatible with old code)
# ----------------------------------------------------------------------
def respawn_tick(reg: AgentsRegistry, grid: torch.Tensor, cfg: RespawnCfg) -> None:
    """
    Perform one tick of respawning.

    This function maintains the original signature. It creates a temporary
    RespawnController and runs its step. The old probabilistic fields
    (prob_per_dead_per_tick) are ignored; the new controller uses the
    floor/period logic defined in the extended RespawnCfg.
    """
    controller = RespawnController(cfg)
    controller.step(0, reg, grid)  # tick number is not critical for oneâ€‘off calls
    # Note: step returns counts, but we ignore them to match old API.
====[ END war_simulation\engine\respawn.py ]============================================================


====[ 19/35 | war_simulation\engine\spawn.py ]================================================
from __future__ import annotations

import math
import random
from typing import Optional, Tuple

import torch
import config
from .agent_registry import AgentsRegistry

# Brains
from agent.transformer_brain import TransformerBrain, scripted_transformer_brain
from agent.tron_brain import TronBrain
from agent.mirror_brain import MirrorBrain


def _rect_dims(n: int, max_cols: int, max_rows: int) -> Tuple[int, int, int]:
    """Calculates dimensions for a compact rectangle to place n agents."""
    if n <= 0:
        return 0, 0, 0
    cols = min(max_cols, max(1, int(math.sqrt(n))))
    rows = min(max_rows, int(math.ceil(n / cols)))
    n_eff = min(n, cols * rows)
    return cols, rows, n_eff

# ----------------------------------------------------------------------
# Team brain selection (supports exclusive split and mixed teams)
# ----------------------------------------------------------------------

# Deterministic per-team alternating counter (only used when mix+alternate).
_TEAM_BRAIN_MIX_COUNTER = {True: 0, False: 0}  # True=red, False=blue

def _make_team_mix_rng(team_is_red: bool):
    """
    Dedicated RNG so brain selection does NOT perturb world spawn RNG.
    If TEAM_BRAIN_MIX_SEED == 0 -> non-deterministic (SystemRandom).
    """
    seed = int(getattr(config, "TEAM_BRAIN_MIX_SEED", 0))
    if seed == 0:
        return random.SystemRandom()
    # Salt per team so red/blue don't mirror the same sequence
    salt = 101 if team_is_red else 202
    return random.Random(seed + salt)

_TEAM_BRAIN_MIX_RNG = {True: _make_team_mix_rng(True), False: _make_team_mix_rng(False)}

def _resolve_team_brain_kind(team_is_red: bool) -> str:
    """
    Returns: "tron" | "mirror" | "transformer"
    Default behavior remains: exclusive split (red=tron, blue=mirror).
    """
    mode = str(getattr(config, "TEAM_BRAIN_ASSIGNMENT_MODE", "exclusive")).strip().lower()

    # Old behavior
    if mode in ("exclusive", "split", "team"):
        return "tron" if team_is_red else "mirror"

    if mode in ("mix", "hybrid", "both"):
        strategy = str(getattr(config, "TEAM_BRAIN_MIX_STRATEGY", "alternate")).strip().lower()

        # Deterministic 50/50: tron, mirror, tron, mirror...
        if strategy in ("alternate", "roundrobin", "rr"):
            i = _TEAM_BRAIN_MIX_COUNTER[team_is_red]
            _TEAM_BRAIN_MIX_COUNTER[team_is_red] = i + 1
            return "tron" if (i % 2 == 0) else "mirror"

        # Probabilistic: P(tron)=TEAM_BRAIN_MIX_P_TRON
        if strategy in ("random", "prob", "probabilistic"):
            p_tron = float(getattr(config, "TEAM_BRAIN_MIX_P_TRON", 0.5))
            p_tron = max(0.0, min(1.0, p_tron))
            r = _TEAM_BRAIN_MIX_RNG[team_is_red].random()
            return "tron" if (r < p_tron) else "mirror"

        raise ValueError(f"Unknown TEAM_BRAIN_MIX_STRATEGY={strategy!r}")

    raise ValueError(f"Unknown TEAM_BRAIN_ASSIGNMENT_MODE={mode!r}")

def _mk_brain(device: torch.device, *, team_is_red: Optional[bool] = None) -> torch.nn.Module:
    """Creates a new brain.

    Non-PPO: scripted transformer (existing behavior).
    PPO:
      - If TEAM_BRAIN_ASSIGNMENT is enabled and team_is_red is known:
          uses TEAM_BRAIN_ASSIGNMENT_MODE:
            * exclusive: red=tron, blue=mirror (old behavior)
            * mix: each team can spawn both (alternate/random)
      - Otherwise: falls back to config.BRAIN_KIND.
    """
    obs_dim = int(getattr(config, "OBS_DIM", 0))
    act_dim = int(getattr(config, "NUM_ACTIONS", 41))

    is_ppo = bool(getattr(config, "PPO_ENABLED", False))
    if not is_ppo:
        return scripted_transformer_brain(obs_dim, act_dim).to(device)

    team_assign = bool(getattr(config, "TEAM_BRAIN_ASSIGNMENT", True))
    if team_assign and team_is_red is not None:
        brain_kind = _resolve_team_brain_kind(bool(team_is_red))
    else:
        brain_kind = str(getattr(config, "BRAIN_KIND", "tron")).strip().lower()

    if brain_kind == "transformer":
        return TransformerBrain(obs_dim, act_dim).to(device)
    if brain_kind == "mirror":
        return MirrorBrain(obs_dim, act_dim).to(device)
    return TronBrain(obs_dim, act_dim).to(device)


def _choose_unit(is_archer_prob: float) -> float:
    return float(config.UNIT_ARCHER if random.random() < is_archer_prob else config.UNIT_SOLDIER)


def _unit_stats(unit_val: float) -> Tuple[float, float, int]:
    """Returns (hp, atk, vision_range) for a given unit id."""
    vision_map = getattr(config, "VISION_RANGE_BY_UNIT", {})
    if int(unit_val) == int(config.UNIT_ARCHER):
        hp = float(config.ARCHER_HP)
        atk = float(config.ARCHER_ATK)
        vision = int(vision_map.get(config.UNIT_ARCHER, 15))
    else:
        hp = float(config.SOLDIER_HP)
        atk = float(config.SOLDIER_ATK)
        vision = int(vision_map.get(config.UNIT_SOLDIER, 10))
    return hp, atk, vision


def _place_if_free(
    reg: AgentsRegistry,
    grid: torch.Tensor,
    slot: int,
    *,
    team_is_red: bool,
    x: int,
    y: int,
    unit_val: float,
) -> bool:
    """Places an agent if the cell is free and registers it.

    IMPORTANT:
      - Grid layout in this project: channel0=team_id/empty, channel1=hp, channel2=slot
      - AgentsRegistry.register requires slot + agent_id + vision_range keyword.
    """
    # occupied if channel0 != 0 (either team id or wall encoding)
    if grid[0, y, x] != 0.0:
        return False

    hp, atk, vision = _unit_stats(unit_val)

    agent_id = reg.get_next_id()

    reg.register(
        slot,
        agent_id=agent_id,
        team_is_red=team_is_red,
        x=x,
        y=y,
        hp=hp,
        atk=atk,
        brain=_mk_brain(reg.device, team_is_red=team_is_red),
        unit=unit_val,
        hp_max=hp,
        vision_range=vision,
        generation=1,
    )

    # Update grid
    grid[0, y, x] = 2.0 if team_is_red else 3.0
    grid[1, y, x] = hp
    grid[2, y, x] = float(slot)
    return True


def spawn_symmetric(reg: AgentsRegistry, grid: torch.Tensor, per_team: int) -> None:
    """Spawns agents in symmetric rectangular formations on opposite sides."""
    H, W = grid.size(1), grid.size(2)
    margin = 2
    half_w = W // 2
    placeable_w = half_w - margin
    placeable_h = H - 2 * margin

    per_team_eff = min(per_team, reg.capacity // 2, placeable_w * placeable_h)
    if per_team_eff <= 0:
        return

    ar_ratio = float(getattr(config, "SPAWN_ARCHER_RATIO", 0.4))

    # Red team (left)
    r_cols, r_rows, r_n = _rect_dims(per_team_eff, placeable_w, placeable_h)
    red_x0, red_y0 = margin, (H - r_rows) // 2

    # Blue team (right)
    b_cols, b_rows, b_n = _rect_dims(per_team_eff, placeable_w, placeable_h)
    blue_x0, blue_y0 = W - margin - b_cols, (H - b_rows) // 2

    slot = 0

    # Place Red
    for iy in range(r_rows):
        for ix in range(r_cols):
            if slot >= r_n or slot >= reg.capacity:
                break
            x, y = red_x0 + ix, red_y0 + iy
            unit = _choose_unit(ar_ratio)
            if _place_if_free(reg, grid, slot, team_is_red=True, x=x, y=y, unit_val=unit):
                slot += 1
        if slot >= r_n or slot >= reg.capacity:
            break

    # Place Blue
    blue_start_slot = slot
    for iy in range(b_rows):
        for ix in range(b_cols):
            if slot >= blue_start_slot + b_n or slot >= reg.capacity:
                break
            x, y = blue_x0 + ix, blue_y0 + iy
            unit = _choose_unit(ar_ratio)
            if _place_if_free(reg, grid, slot, team_is_red=False, x=x, y=y, unit_val=unit):
                slot += 1
        if slot >= blue_start_slot + b_n or slot >= reg.capacity:
            break


def spawn_uniform_random(reg: AgentsRegistry, grid: torch.Tensor, per_team: int) -> None:
    """Spawns agents for both teams randomly across the entire map."""
    H, W = grid.size(1), grid.size(2)
    margin = 2
    ar_ratio = float(getattr(config, "SPAWN_ARCHER_RATIO", 0.4))

    total_to_spawn = min(per_team * 2, reg.capacity)
    red_to_spawn = min(per_team, total_to_spawn)
    blue_to_spawn = total_to_spawn - red_to_spawn

    attempts = 0
    max_attempts = total_to_spawn * 50
    slot = 0

    while (red_to_spawn > 0 or blue_to_spawn > 0) and attempts < max_attempts and slot < total_to_spawn:
        x = random.randint(margin, W - margin - 1)
        y = random.randint(margin, H - margin - 1)

        if grid[0, y, x] == 0.0:
            team_placed = False

            # Decide which team to try first (bias toward the team that still needs more)
            if red_to_spawn > 0 and blue_to_spawn > 0:
                spawn_red = (random.random() < (red_to_spawn / (red_to_spawn + blue_to_spawn)))
            else:
                spawn_red = (red_to_spawn > 0)

            if spawn_red and red_to_spawn > 0:
                unit = _choose_unit(ar_ratio)
                if _place_if_free(reg, grid, slot, team_is_red=True, x=x, y=y, unit_val=unit):
                    slot += 1
                    red_to_spawn -= 1
                    team_placed = True
            elif (not spawn_red) and blue_to_spawn > 0:
                unit = _choose_unit(ar_ratio)
                if _place_if_free(reg, grid, slot, team_is_red=False, x=x, y=y, unit_val=unit):
                    slot += 1
                    blue_to_spawn -= 1
                    team_placed = True

            # If first attempt failed (rare), try the other team once
            if not team_placed:
                if spawn_red and blue_to_spawn > 0:
                    unit = _choose_unit(ar_ratio)
                    if _place_if_free(reg, grid, slot, team_is_red=False, x=x, y=y, unit_val=unit):
                        slot += 1
                        blue_to_spawn -= 1
                elif (not spawn_red) and red_to_spawn > 0:
                    unit = _choose_unit(ar_ratio)
                    if _place_if_free(reg, grid, slot, team_is_red=True, x=x, y=y, unit_val=unit):
                        slot += 1
                        red_to_spawn -= 1

        attempts += 1

    if slot < total_to_spawn:
        print(f"[spawn] Warning: Could only spawn {slot}/{total_to_spawn} agents. The map might be too full.")

====[ END war_simulation\engine\spawn.py ]============================================================


====[ 20/35 | war_simulation\engine\tick.py ]=================================================
from __future__ import annotations
from dataclasses import dataclass
import collections
from typing import Dict, Optional, List, Tuple, TYPE_CHECKING

import torch

import config
from simulation.stats import SimulationStats
from engine.agent_registry import (
    AgentsRegistry,
    COL_ALIVE, COL_TEAM, COL_X, COL_Y, COL_HP, COL_ATK, COL_UNIT, COL_VISION, COL_HP_MAX, COL_AGENT_ID
)
from engine.ray_engine.raycast_32 import raycast32_firsthit
from engine.game.move_mask import build_mask, DIRS8
from engine.respawn import RespawnController, RespawnCfg
from engine.mapgen import Zones

from agent.ensemble import ensemble_forward
from agent.transformer_brain import TransformerBrain

if TYPE_CHECKING:
    from rl.ppo_runtime import PerAgentPPORuntime

try:
    from rl.ppo_runtime import PerAgentPPORuntime as _PerAgentPPORuntimeRT
except Exception:
    _PerAgentPPORuntimeRT = None


@dataclass
class TickMetrics:
    alive: int = 0
    moved: int = 0
    attacks: int = 0
    deaths: int = 0
    tick: int = 0
    cp_red_tick: float = 0.0
    cp_blue_tick: float = 0.0


class TickEngine:
    def __init__(self, registry: AgentsRegistry, grid: torch.Tensor,
                 stats: SimulationStats, zones: Optional[Zones] = None) -> None:
        self.registry = registry
        self.grid = grid
        self.stats = stats
        self.device = grid.device
        self.H, self.W = int(grid.size(1)), int(grid.size(2))
        self.respawner = RespawnController(RespawnCfg())
        self.agent_scores: Dict[int, float] = collections.defaultdict(float)
        self.zones: Optional[Zones] = zones
        self._z_heal: Optional[torch.Tensor] = None
        self._z_cp_masks: List[torch.Tensor] = []
        self._ensure_zone_tensors()
        self.DIRS8_dev = DIRS8.to(self.device)
        self._ACTIONS = int(getattr(config, "NUM_ACTIONS", 41))
        self._OBS_DIM = config.OBS_DIM
        self._grid_dt = self.grid.dtype
        self._data_dt = self.registry.agent_data.dtype

        # ================================================================
        # Instinct cache (computed under no_grad) â€” NEW
        # ================================================================
        self._instinct_cached_r: int = -999999
        self._instinct_offsets: Optional[torch.Tensor] = None  # (M,2) long dx,dy within radius
        self._instinct_area: float = 1.0

        self._g0 = torch.tensor(0.0, device=self.device, dtype=self._grid_dt)
        self._gneg = torch.tensor(-1.0, device=self.device, dtype=self._grid_dt)
        self._d0 = torch.tensor(0.0, device=self.device, dtype=self._data_dt)
        self._ppo_enabled = bool(getattr(config, "PPO_ENABLED", False))
        self._ppo: Optional["PerAgentPPORuntime"] = None
        if self._ppo_enabled and _PerAgentPPORuntimeRT is not None:
            self._ppo = _PerAgentPPORuntimeRT(
                registry=self.registry, device=self.device,
                obs_dim=self._OBS_DIM, act_dim=self._ACTIONS,
            )

    def _ppo_reset_on_respawn(self, was_dead: torch.Tensor) -> None:
        """Reset per-slot PPO state for any slot that was dead before respawn and is alive after."""
        if self._ppo is None:
            return
        data = self.registry.agent_data
        now_alive = (data[:, COL_ALIVE] > 0.5)
        spawned_slots = (was_dead & now_alive).nonzero(as_tuple=False).squeeze(1)
        if spawned_slots.numel() == 0:
            return
        self._ppo.reset_agents(spawned_slots)
        if bool(getattr(config, "PPO_RESET_LOG", False)):
            # Keep logs short to avoid spam; show up to 16 slots.
            sl = spawned_slots[:16].tolist()
            suffix = "" if spawned_slots.numel() <= 16 else "..."
            print(f"[ppo] reset state for {int(spawned_slots.numel())} respawned slots: {sl}{suffix}")

    def _ensure_zone_tensors(self) -> None:
        self._z_heal, self._z_cp_masks = None, []
        if self.zones is None: return
        try:
            if getattr(self.zones, "heal_mask", None) is not None:
                self._z_heal = self.zones.heal_mask.to(self.device, non_blocking=True).bool()
            self._z_cp_masks = [m.to(self.device, non_blocking=True).bool() for m in getattr(self.zones, "cp_masks", [])]
        except Exception as e:
            print(f"[tick] WARN: zone tensor setup failed ({e}); zones disabled.")

    @staticmethod
    def _as_long(x: torch.Tensor) -> torch.Tensor: return x.to(torch.long)

    def _recompute_alive_idx(self) -> torch.Tensor:
        return (self.registry.agent_data[:, COL_ALIVE] > 0.5).nonzero(as_tuple=False).squeeze(1)

    @torch.no_grad()
    def _get_instinct_offsets(self) -> Tuple[torch.Tensor, float]:
        """
        Returns cached integer (dx,dy) offsets inside a discrete circle of radius R (cells),
        plus the offset-count area used for density normalization.
        """
        R = int(getattr(config, "INSTINCT_RADIUS", 6))
        if R < 0:
            R = 0

        if self._instinct_offsets is None or self._instinct_cached_r != R:
            # Build once per radius change. Keep on engine device.
            if R == 0:
                offsets = torch.zeros((1, 2), device=self.device, dtype=torch.long)
            else:
                r = torch.arange(-R, R + 1, device=self.device, dtype=torch.long)
                dx, dy = torch.meshgrid(r, r, indexing="xy")  # dx: (S,S), dy: (S,S)
                mask = (dx * dx + dy * dy) <= (R * R)
                offsets = torch.stack([dx[mask], dy[mask]], dim=1).contiguous()  # (M,2)
                if offsets.numel() == 0:
                    offsets = torch.zeros((1, 2), device=self.device, dtype=torch.long)

            self._instinct_offsets = offsets
            self._instinct_area = float(int(offsets.size(0)))
            self._instinct_cached_r = R

        return self._instinct_offsets, self._instinct_area

    @torch.no_grad()
    def _compute_instinct_context(
        self,
        alive_idx: torch.Tensor,
        pos_xy: torch.Tensor,
        unit_map: torch.Tensor,
    ) -> torch.Tensor:
        """
        Instinct token (4 floats) per alive agent:
          1) ally_archer_density
          2) ally_soldier_density
          3) noisy_enemy_density
          4) threat_ratio = enemy_density / (ally_total_density + eps)
        Densities are counts / area, where area = number of discrete cells in the radius mask.
        """
        N = int(alive_idx.numel())
        if N == 0:
            return torch.empty((0, 4), device=self.device, dtype=self._data_dt)

        data = self.registry.agent_data
        offsets, area = self._get_instinct_offsets()
        M = int(offsets.size(0))
        if M <= 0 or area <= 0.0:
            return torch.zeros((N, 4), device=self.device, dtype=self._data_dt)

        # Broadcasted neighborhood coords (N,M)
        x0 = pos_xy[:, 0].to(torch.long).view(N, 1)
        y0 = pos_xy[:, 1].to(torch.long).view(N, 1)
        ox = offsets[:, 0].view(1, M)
        oy = offsets[:, 1].view(1, M)
        xx = (x0 + ox).clamp(0, self.W - 1)
        yy = (y0 + oy).clamp(0, self.H - 1)

        occ = self.grid[0][yy, xx]      # (N,M) float, 0 empty, 1 wall, 2 red, 3 blue
        uid = unit_map[yy, xx]          # (N,M) long/int, -1 none, 1 soldier, 2 archer

        teams = data[alive_idx, COL_TEAM]  # (N,) float: 2.0 red, 3.0 blue
        team_is_red = (teams == 2.0)
        ally_occ = torch.where(team_is_red, occ.new_full((N,), 2.0), occ.new_full((N,), 3.0)).view(N, 1)
        enemy_occ = torch.where(team_is_red, occ.new_full((N,), 3.0), occ.new_full((N,), 2.0)).view(N, 1)

        ally_mask = (occ == ally_occ)
        enemy_mask = (occ == enemy_occ)

        ally_arch = ally_mask & (uid == 2)
        ally_sold = ally_mask & (uid == 1)

        ally_arch_c = ally_arch.sum(dim=1).to(torch.float32)
        ally_sold_c = ally_sold.sum(dim=1).to(torch.float32)
        enemy_c = enemy_mask.sum(dim=1).to(torch.float32)

        # Exclude self cell (offset (0,0) is included by construction).
        self_unit = data[alive_idx, COL_UNIT]  # (N,) float: 1 soldier, 2 archer
        ally_arch_c = (ally_arch_c - (self_unit == 2.0).to(torch.float32)).clamp_min(0.0)
        ally_sold_c = (ally_sold_c - (self_unit == 1.0).to(torch.float32)).clamp_min(0.0)

        # Add small noise to enemy count (requirement).
        noise = torch.randn((N,), device=self.device, dtype=torch.float32) * 0.25
        enemy_c_noisy = (enemy_c + noise).clamp_min(0.0)

        inv_area = 1.0 / float(area)
        ally_arch_d = ally_arch_c * inv_area
        ally_sold_d = ally_sold_c * inv_area
        enemy_d = enemy_c_noisy * inv_area

        eps = 1e-4 if self._data_dt == torch.float16 else 1e-6
        ally_total_d = ally_arch_d + ally_sold_d
        threat = enemy_d / (ally_total_d + eps)

        out = torch.stack([ally_arch_d, ally_sold_d, enemy_d, threat], dim=1)
        return out.to(dtype=self._data_dt)

    def _apply_deaths(self, sel: torch.Tensor, metrics: TickMetrics, credit_kills: bool = True) -> Tuple[int, int]:
        data = self.registry.agent_data
        dead_idx = sel.nonzero(as_tuple=False).squeeze(1) if sel.dtype == torch.bool else sel.view(-1)
        if dead_idx.numel() == 0:
            return 0, 0

        dead_team = data[dead_idx, COL_TEAM]
        red_deaths = int((dead_team == 2.0).sum().item())
        blue_deaths = int((dead_team == 3.0).sum().item())

        if red_deaths:
            self.stats.add_death("red", red_deaths)
            if credit_kills:
                self.stats.add_kill("blue", red_deaths)

        if blue_deaths:
            self.stats.add_death("blue", blue_deaths)
            if credit_kills:
                self.stats.add_kill("red", blue_deaths)

        gx, gy = self._as_long(data[dead_idx, COL_X]), self._as_long(data[dead_idx, COL_Y])
        self.grid[0][gy, gx], self.grid[1][gy, gx], self.grid[2][gy, gx] = self._g0, self._g0, self._gneg
        data[dead_idx, COL_ALIVE] = self._d0
        metrics.deaths += int(dead_idx.numel())
        return red_deaths, blue_deaths

    @torch.no_grad()
    def _build_transformer_obs(self, alive_idx: torch.Tensor, pos_xy: torch.Tensor) -> torch.Tensor:
        from engine.ray_engine.raycast_firsthit import build_unit_map
        data = self.registry.agent_data
        N = alive_idx.numel()

        # --- Zone flags for rich features ---
        if self._z_heal is not None:
            on_heal = self._z_heal[pos_xy[:, 1], pos_xy[:, 0]]
        else:
            on_heal = torch.zeros(N, device=self.device, dtype=torch.bool)

        on_cp = torch.zeros(N, device=self.device, dtype=torch.bool)
        if self._z_cp_masks:
            for cp_mask in self._z_cp_masks:
                on_cp |= cp_mask[pos_xy[:, 1], pos_xy[:, 0]]

        def _norm_const(v: float, scale: float) -> torch.Tensor:
            s = scale if scale > 0 else 1.0
            return torch.full((N,), v / s, dtype=self._data_dt, device=self.device)

        expected_ray_dim = 32 * 8
        unit_map = build_unit_map(data, self.grid)
        rays = raycast32_firsthit(
            pos_xy, self.grid, unit_map,
            max_steps_each=data[alive_idx, COL_VISION].long()
        )
        if rays.shape != (N, expected_ray_dim):
            raise RuntimeError(
                f"[obs] ray tensor shape mismatch: got {tuple(rays.shape)}, "
                f"expected ({N}, {expected_ray_dim})."
            )

        hp_max = data[alive_idx, COL_HP_MAX].clamp_min(1.0)

        rich_base = torch.stack([
            data[alive_idx, COL_HP] / hp_max,
            data[alive_idx, COL_X] / (self.W - 1),
            data[alive_idx, COL_Y] / (self.H - 1),
            (data[alive_idx, COL_TEAM] == 2.0),
            (data[alive_idx, COL_TEAM] == 3.0),
            (data[alive_idx, COL_UNIT] == 1.0),
            (data[alive_idx, COL_UNIT] == 2.0),
            data[alive_idx, COL_ATK] / (config.MAX_ATK or 1.0),
            data[alive_idx, COL_VISION] / (config.RAYCAST_MAX_STEPS or 15.0),
            on_heal.to(self._data_dt),
            on_cp.to(self._data_dt),
            _norm_const(float(self.stats.tick), 50000.0),
            _norm_const(self.stats.red.score, 1000.0), _norm_const(self.stats.blue.score, 1000.0),
            _norm_const(self.stats.red.cp_points, 500.0), _norm_const(self.stats.blue.cp_points, 500.0),
            _norm_const(self.stats.red.kills, 500.0), _norm_const(self.stats.blue.kills, 500.0),
            _norm_const(self.stats.red.deaths, 500.0), _norm_const(self.stats.blue.deaths, 500.0),
            # Padding to preserve RICH_BASE_DIM=23 layout (reserved slots).
            torch.zeros(N, device=self.device, dtype=self._data_dt),
            torch.zeros(N, device=self.device, dtype=self._data_dt),
            torch.zeros(N, device=self.device, dtype=self._data_dt),
        ], dim=1).to(dtype=self._data_dt)

        instinct = self._compute_instinct_context(alive_idx=alive_idx, pos_xy=pos_xy, unit_map=unit_map)
        # Hard invariant
        if instinct.shape != (N, 4):
            raise RuntimeError(f"instinct shape {tuple(instinct.shape)} != (N,4)")

        rich = torch.cat([rich_base, instinct], dim=1)

        expected_rich_dim = int(self._OBS_DIM) - expected_ray_dim
        if rich.shape != (N, expected_rich_dim):
            raise RuntimeError(
                f"[obs] rich tensor shape mismatch: got {tuple(rich.shape)}, "
                f"expected ({N}, {expected_rich_dim})."
            )

        obs = torch.cat([rays, rich.to(rays.dtype)], dim=1)
        if obs.shape != (N, int(self._OBS_DIM)):
            raise RuntimeError(
                f"[obs] final obs shape mismatch: got {tuple(obs.shape)}, "
                f"expected ({N}, {int(self._OBS_DIM)})."
            )
        return obs

    @torch.no_grad()
    def run_tick(self) -> Dict[str, float]:
        data = self.registry.agent_data
        metrics = TickMetrics()
        alive_idx = self._recompute_alive_idx()
        if alive_idx.numel() == 0:
            self.stats.on_tick_advanced(1)
            metrics.tick = int(self.stats.tick)
            was_dead = (data[:, COL_ALIVE] <= 0.5) if self._ppo is not None else None
            self.respawner.step(self.stats.tick, self.registry, self.grid)
            if was_dead is not None:
                self._ppo_reset_on_respawn(was_dead)
            return vars(metrics)

        pos_xy = self.registry.positions_xy(alive_idx)
        obs = self._build_transformer_obs(alive_idx, pos_xy)
        # ABSOLUTE invariant: obs width must match config.OBS_DIM
        if obs.dim() != 2 or int(obs.shape[1]) != int(config.OBS_DIM):
            raise RuntimeError(
                f"[obs] shape mismatch: got {tuple(obs.shape)}, expected (N,{int(config.OBS_DIM)})"
            )
        mask = build_mask(pos_xy, data[alive_idx, COL_TEAM], self.grid, unit=self._as_long(data[alive_idx, COL_UNIT]))
        actions = torch.zeros_like(alive_idx, dtype=torch.long)
        rec_agent_ids, rec_obs, rec_logits, rec_values, rec_actions, rec_teams = [], [], [], [], [], []

        for bucket in self.registry.build_buckets(alive_idx):
            loc = torch.searchsorted(alive_idx, bucket.indices)
            dist, vals = ensemble_forward(bucket.models, obs[loc])
            logits32 = torch.where(mask[loc], dist.logits, torch.finfo(torch.float32).min).to(torch.float32)
            a = torch.distributions.Categorical(logits=logits32).sample()
            if self._ppo:
                rec_agent_ids.append(bucket.indices)
                rec_obs.append(obs[loc])
                rec_logits.append(logits32)
                rec_values.append(vals)
                rec_actions.append(a)
                rec_teams.append(data[bucket.indices, COL_TEAM])
            actions[loc] = a

        metrics.alive = int(alive_idx.numel())

        is_move = (actions >= 1) & (actions <= 8)
        if is_move.any():
            move_idx, dir_idx = alive_idx[is_move], actions[is_move] - 1
            x0, y0 = pos_xy[is_move].T
            nx, ny = (x0 + self.DIRS8_dev[dir_idx, 0]).clamp(0, self.W - 1), (y0 + self.DIRS8_dev[dir_idx, 1]).clamp(0, self.H - 1)
            can_move = (self.grid[0][ny, nx] == self._g0)
            if can_move.any():
                move_idx, x0, y0, nx, ny = move_idx[can_move], x0[can_move], y0[can_move], nx[can_move], ny[can_move]
                self.grid[0, y0, x0], self.grid[1, y0, x0], self.grid[2, y0, x0] = self._g0, self._g0, self._gneg
                data[move_idx, COL_X], data[move_idx, COL_Y] = nx.to(self._data_dt), ny.to(self._data_dt)
                self.grid[0, ny, nx] = data[move_idx, COL_TEAM].to(self._grid_dt)
                self.grid[1, ny, nx] = data[move_idx, COL_HP].to(self._grid_dt)
                self.grid[2, ny, nx] = move_idx.to(self._grid_dt)
                metrics.moved = int(can_move.sum().item())

        combat_rd, combat_bd = 0, 0
        meta_rd, meta_bd = 0, 0

        individual_rewards = torch.zeros(self.registry.capacity, device=self.device, dtype=self._data_dt)

        if (is_attack := actions >= 9).any():
            atk_idx, atk_act = alive_idx[is_attack], actions[is_attack]
            r, dir_idx = ((atk_act - 9) % 4) + 1, (atk_act - 9) // 4
            dxy = self.DIRS8_dev[dir_idx] * r.unsqueeze(1)
            ax, ay = pos_xy[is_attack].T
            tx, ty = (ax + dxy[:, 0]).clamp(0, self.W - 1), (ay + dxy[:, 1]).clamp(0, self.H - 1)
            victims = self._as_long(self.grid[2][ty, tx])
            if (valid_hit := victims >= 0).any():
                atk_idx, victims = atk_idx[valid_hit], victims[valid_hit]
                if (is_enemy := data[atk_idx, COL_TEAM] != data[victims, COL_TEAM]).any():
                    atk_idx, victims = atk_idx[is_enemy], victims[is_enemy]
                    was_alive = data[victims, COL_HP] > 0
                    data[victims, COL_HP] -= data[atk_idx, COL_ATK]
                    now_dead = data[victims, COL_HP] <= 0
                    if (killers := atk_idx[was_alive & now_dead]).numel() > 0:
                        reward_val = float(config.PPO_REWARD_KILL_INDIVIDUAL)
                        individual_rewards.index_add_(0, killers, torch.full_like(killers, reward_val, dtype=self._data_dt))
                        for killer_slot in killers:
                            uid = int(data[killer_slot.item(), COL_AGENT_ID].item())
                            self.agent_scores[uid] += reward_val
                    vy, vx = self._as_long(data[victims, COL_Y]), self._as_long(data[victims, COL_X])
                    self.grid[1, vy, vx] = data[victims, COL_HP].to(self._grid_dt)
                    metrics.attacks += int(is_enemy.sum().item())

        rD, bD = self._apply_deaths((data[:, COL_ALIVE] > 0.5) & (data[:, COL_HP] <= 0.0), metrics)
        combat_rd += rD
        combat_bd += bD

        if (alive_idx := self._recompute_alive_idx()).numel() > 0:
            pos_xy = self.registry.positions_xy(alive_idx)
            if self._z_heal is not None and (on_heal := self._z_heal[pos_xy[:, 1], pos_xy[:, 0]]).any():
                heal_idx = alive_idx[on_heal]
                data[heal_idx, COL_HP] = (data[heal_idx, COL_HP] + config.HEAL_RATE).clamp_max(data[heal_idx, COL_HP_MAX])
                self.grid[1, pos_xy[on_heal, 1], pos_xy[on_heal, 0]] = data[heal_idx, COL_HP].to(self._grid_dt)

            if meta_drain := getattr(config, "METABOLISM_ENABLED", True):
                drain = torch.where(data[alive_idx, COL_UNIT] == 1.0, config.META_SOLDIER_HP_PER_TICK, config.META_ARCHER_HP_PER_TICK)
                data[alive_idx, COL_HP] -= drain.to(self._data_dt)
                self.grid[1, pos_xy[:, 1], pos_xy[:, 0]] = data[alive_idx, COL_HP].to(self._grid_dt)
                if (data[alive_idx, COL_HP] <= 0.0).any():
                    rD, bD = self._apply_deaths(
                        alive_idx[data[alive_idx, COL_HP] <= 0.0],
                        metrics,
                        credit_kills=False,
                    )
                    meta_rd += rD
                    meta_bd += bD

            if self._z_cp_masks and (alive_idx := self._recompute_alive_idx()).numel() > 0:
                pos_xy, teams_alive = self.registry.positions_xy(alive_idx), data[alive_idx, COL_TEAM]
                for cp_mask in self._z_cp_masks:
                    if (on_cp := cp_mask[pos_xy[:, 1], pos_xy[:, 0]]).any():
                        red_on = (on_cp & (teams_alive == 2.0)).sum().item()
                        blue_on = (on_cp & (teams_alive == 3.0)).sum().item()
                        if red_on > blue_on:
                            self.stats.add_capture_points("red", config.CP_REWARD_PER_TICK)
                            metrics.cp_red_tick += config.CP_REWARD_PER_TICK
                        elif blue_on > red_on:
                            self.stats.add_capture_points("blue", config.CP_REWARD_PER_TICK)
                            metrics.cp_blue_tick += config.CP_REWARD_PER_TICK

                        # Individual reward for agents on a contested CP
                        if red_on > 0 and blue_on > 0:
                            winners_on_cp = None
                            if red_on > blue_on:
                                winners_on_cp = on_cp & (teams_alive == 2.0)
                            elif blue_on > red_on:
                                winners_on_cp = on_cp & (teams_alive == 3.0)
                            if winners_on_cp is not None and winners_on_cp.any():
                                winners_idx = alive_idx[winners_on_cp]
                                reward_val = config.PPO_REWARD_CONTESTED_CP
                                individual_rewards.index_add_(
                                    0,
                                    winners_idx,
                                    torch.full_like(winners_idx, reward_val, dtype=self._data_dt),
                                )

        if self._ppo and rec_agent_ids:
            agent_ids = torch.cat(rec_agent_ids)
            team_r_rew = (combat_bd * config.TEAM_KILL_REWARD) + ((combat_rd + meta_rd) * config.PPO_REWARD_DEATH) + metrics.cp_red_tick
            team_b_rew = (combat_rd * config.TEAM_KILL_REWARD) + ((combat_bd + meta_bd) * config.PPO_REWARD_DEATH) + metrics.cp_blue_tick

            current_hp = data[agent_ids, COL_HP]
            hp_reward = (current_hp * config.PPO_REWARD_HP_TICK).to(self._data_dt)
            final_rewards = individual_rewards[agent_ids] + torch.where(torch.cat(rec_teams) == 2.0, team_r_rew, team_b_rew) + hp_reward

            with torch.enable_grad():
                self._ppo.record_step(
                    agent_ids=agent_ids,
                    obs=torch.cat(rec_obs),
                    logits=torch.cat(rec_logits),
                    values=torch.cat(rec_values),
                    actions=torch.cat(rec_actions),
                    rewards=final_rewards,
                    done=(data[agent_ids, COL_ALIVE] <= 0.5)
                )

        self.stats.on_tick_advanced(1)
        metrics.tick = int(self.stats.tick)
        was_dead = (data[:, COL_ALIVE] <= 0.5) if self._ppo is not None else None
        self.respawner.step(self.stats.tick, self.registry, self.grid)
        if was_dead is not None:
            self._ppo_reset_on_respawn(was_dead)
        return vars(metrics)
====[ END war_simulation\engine\tick.py ]============================================================


====[ 21/35 | war_simulation\main.py ]========================================================
from __future__ import annotations

import json
import os
import sys
import argparse
import time
import signal
import traceback
from pathlib import Path
from typing import Optional

from utils.checkpoint import (
    save_checkpoint,
    load_checkpoint,
    apply_engine_state,
    resolve_checkpoint_path,
)

import torch
import numpy as np
try:
    import cv2  # optional; recording becomes no-op if not installed
except Exception:
    cv2 = None

# Local modules
import config
from simulation.stats import SimulationStats
from engine.agent_registry import AgentsRegistry, COL_ALIVE
from engine.tick import TickEngine
from engine.grid import make_grid
from engine.spawn import spawn_symmetric, spawn_uniform_random
from engine.mapgen import add_random_walls, make_zones
from utils.persistence import ResultsWriter
from ui.viewer import Viewer


# =============================================================================
# ðŸ› ï¸ ENV-PARSING UTILITIES
# =============================================================================
# These helpers allow you to override parameters via environment variables.
# Example: FWS_MAX_AGENTS=500 python main.py

def _env_bool(key: str, default: bool) -> bool:
    v = os.getenv(key)
    if v is None:
        return default
    return v.strip().lower() in ("1", "true", "yes", "y", "on")

def _env_float(key: str, default: float) -> float:
    v = os.getenv(key)
    if v is None:
        return float(default)
    try:
        return float(v)
    except Exception:
        return float(default)

def _env_int(key: str, default: int) -> int:
    v = os.getenv(key)
    if v is None:
        return int(default)
    try:
        return int(v)
    except Exception:
        return int(default)

def _env_str(key: str, default: str) -> str:
    v = os.getenv(key)
    if v is None:
        return str(default)
    return str(v)


# =============================================================================
# CLI helpers
# =============================================================================

def _parse_cli(argv: Optional[list[str]] = None) -> argparse.Namespace:
    """
    Minimal CLI parser.

    Your project already supports env-overrides (FWS_...).
    But a user passing CLI flags should override env/config.

    Supported flags:
      --resume PATH
          Resume from a checkpoint file (ckpt_latest.pt / ckpt_000000123.pt)
          or from a folder containing such files.

      --resume-strict / --no-resume-strict
          Strict validates OBS_DIM/NUM_ACTIONS vs current config.
          Non-strict loads best-effort.

      --print-resume-info
          Load checkpoint, print a quick summary, then exit (sanity check).
    """
    p = argparse.ArgumentParser(add_help=True)

    p.add_argument(
        "--resume",
        type=str,
        default="",
        help="Checkpoint file/folder to resume from (overrides FWS_RESUME_PATH / config.RESUME_PATH).",
    )

    strict_grp = p.add_mutually_exclusive_group()
    strict_grp.add_argument("--resume-strict", action="store_true", help="Fail fast on config/shape mismatches.")
    strict_grp.add_argument("--no-resume-strict", action="store_true", help="Best-effort load (more permissive).")

    p.add_argument(
        "--print-resume-info",
        action="store_true",
        help="Load checkpoint, print a quick summary, then exit (sanity check).",
    )

    return p.parse_args(argv)


# =============================================================================
# ------------------------------- helpers ------------------------------------
# =============================================================================

def _seed_all_from_env() -> Optional[int]:
    """
    Optional seeding for reproducible fresh runs.
    On resume, RNG is restored from the checkpoint, so we do NOT reseed.
    """
    seed = os.getenv("FWS_SEED", "").strip()
    if seed == "":
        return None

    try:
        s = int(seed)
    except Exception:
        return None

    np.random.seed(s)
    torch.manual_seed(s)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(s)

    return s


def _config_snapshot(resume_from: str = "") -> dict:
    """Light, serializable snapshot of current config for run metadata."""
    return {
        "summary": config.summary_str() if hasattr(config, "summary_str") else "",
        "RESUME_FROM": str(resume_from or ""),
        "GRID_W": getattr(config, "GRID_WIDTH", None),
        "GRID_H": getattr(config, "GRID_HEIGHT", None),
        "START_PER_TEAM": getattr(config, "START_AGENTS_PER_TEAM", None),
        "MAX_AGENTS": getattr(config, "MAX_AGENTS", None),
        "OBS_DIM": getattr(config, "OBS_DIM", None),
        "NUM_ACTIONS": getattr(config, "NUM_ACTIONS", None),
        "SPAWN_MODE": getattr(config, "SPAWN_MODE", None),
        "DEVICE": str(getattr(config, "TORCH_DEVICE", getattr(config, "DEVICE", "cpu"))),
        "AMP": bool(getattr(config, "USE_AMP", False)),
        "USE_VMAP": bool(getattr(config, "USE_VMAP", False)),
    }


class _SimpleRecorder:
    """
    Lightweight frame recorder (occupancy -> colored pixels).
    This is intentionally simple and best-effort: if OpenCV isn't installed,
    recording becomes a no-op.
    """
    def __init__(self, run_dir: Path, grid: torch.Tensor, fps: int = 60, scale: int = 2) -> None:
        self.enabled = False
        self.writer = None
        self.size = None
        self.path = None
        self.palette = None
        self.grid = grid

        if not getattr(config, "RECORD_VIDEO", False) or cv2 is None:
            return

        h, w = int(grid.size(1)), int(grid.size(2))
        self.size = (w * scale, h * scale)
        self.path = run_dir / "simulation_raw.avi"

        fourcc = cv2.VideoWriter_fourcc(*"MJPG")
        self.writer = cv2.VideoWriter(str(self.path), fourcc, float(fps), self.size)

        if self.writer is not None and self.writer.isOpened():
            self.enabled = True
            print(f"[video] recording â†’ {self.path}")
            self.palette = np.array(
                [
                    [30, 30, 30],    # empty
                    [80, 80, 80],    # wall (if encoded that way)
                    [220, 80, 80],   # red
                    [80, 120, 240],  # blue
                ],
                dtype=np.uint8,
            )
        else:
            print(f"[video] ERROR: could not open writer for {self.path}.")

    def write(self) -> None:
        if not self.enabled:
            return
        occ = self.grid[0].detach().contiguous().to("cpu").numpy().astype(np.uint8)
        frame = self.palette[occ % len(self.palette)]
        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        frame_resized = cv2.resize(frame_bgr, self.size, interpolation=cv2.INTER_NEAREST)
        self.writer.write(frame_resized)

    def close(self) -> None:
        if self.enabled and self.writer is not None:
            self.writer.release()
            print(f"[video] saved â†’ {self.path}")


def _headless_loop(engine: TickEngine, stats: SimulationStats, rw: ResultsWriter, limit: int) -> None:
    from utils.profiler import torch_profiler_ctx, nvidia_smi_summary
    from utils.sanitize import runtime_sanity_check

    with torch_profiler_ctx() as prof:
        try:
            while limit == 0 or stats.tick < limit:
                engine.run_tick()

                rw.write_tick(stats.as_row())
                deaths = stats.drain_dead_log()
                if deaths:
                    rw.write_deaths(deaths)

                if (stats.tick % 500) == 0:
                    runtime_sanity_check(engine.grid, engine.registry.agent_data)

                if prof is not None:
                    prof.step()

                if (stats.tick % 100) == 0:
                    gpu = nvidia_smi_summary() or "-"
                    print(
                        f"Tick {int(stats.tick):7d} | "
                        f"Red {stats.red.score:8.2f} | Blue {stats.blue.score:8.2f} | "
                        f"Elapsed {stats.elapsed_seconds:7.2f}s | GPU {gpu}"
                    )
        except KeyboardInterrupt:
            print("\n[main] Interrupted â€” shutting down gracefully.")


# =============================================================================
# Main
# =============================================================================

def main() -> None:
    torch.set_float32_matmul_precision("high")

    rw: Optional[ResultsWriter] = None
    run_dir: Optional[Path] = None
    recorder: Optional[_SimpleRecorder] = None

    # ------------------------------------------------------------
    # CLI args (override env/config when provided)
    # ------------------------------------------------------------
    args = _parse_cli()

    # Resume path priority:
    #   1) --resume
    #   2) FWS_RESUME_PATH
    #   3) config.RESUME_PATH
    resume_path = str(getattr(args, "resume", "") or "").strip()
    if not resume_path:
        resume_path = _env_str("FWS_RESUME_PATH", "").strip()
    if not resume_path:
        resume_path = str(getattr(config, "RESUME_PATH", "") or "").strip()

    # Strictness priority:
    #   1) explicit CLI flag
    #   2) env FWS_RESUME_STRICT
    #   3) config.RESUME_STRICT (default True if missing)
    if getattr(args, "resume_strict", False):
        resume_strict = True
    elif getattr(args, "no_resume_strict", False):
        resume_strict = False
    else:
        resume_strict = _env_bool("FWS_RESUME_STRICT", bool(getattr(config, "RESUME_STRICT", True)))

    # If user asked for resume but path is wrong, do NOT silently start a fresh run.
    if resume_path:
        try:
            resume_path = str(resolve_checkpoint_path(resume_path))
        except FileNotFoundError as e:
            print(f"[RESUME] ERROR: {e}")
            raise SystemExit(2)

    # --- Checkpoint knobs (env controlled, minimal intrusion) ---
    ckpt_enabled = _env_bool("FWS_CHECKPOINT", False)
    ckpt_every = _env_int("FWS_CHECKPOINT_EVERY_TICKS", 0)
    ckpt_keep_last = _env_int("FWS_CHECKPOINT_KEEP_LAST", 2)
    ckpt_on_exit = _env_bool("FWS_CHECKPOINT_ON_EXIT", True)

    device = getattr(config, "TORCH_DEVICE", getattr(config, "DEVICE", torch.device("cpu")))

    # Only seed for fresh runs (resume restores RNG from ckpt)
    if not resume_path:
        seed = _seed_all_from_env()
        if seed is not None:
            print(f"[seed] {seed}")
        else:
            print("[seed] none (random)")

    print(config.summary_str() if hasattr(config, "summary_str") else "[final_war_sim]")

    try:
        # ------------------------------------------------------------
        # Build world: either from checkpoint or fresh
        # ------------------------------------------------------------
        if resume_path:
            grid, registry, stats, zones, engine_state, meta = load_checkpoint(
                resume_path,
                device=device,
                strict=bool(resume_strict),
            )
            print(f"[RESUME] loaded: {meta.get('path')} (tick={stats.tick}, created={meta.get('created_utc')})")

            # Quick sanity print: if this shows non-zero tick + non-zero scores,
            # you are *really* resuming and not starting fresh.
            try:
                alive_count = int((registry.agent_data[:, COL_ALIVE] > 0.5).sum().item())
            except Exception:
                alive_count = -1

            try:
                r_score = float(getattr(stats.red, "score", 0.0))
                b_score = float(getattr(stats.blue, "score", 0.0))
            except Exception:
                r_score, b_score = 0.0, 0.0

            print(f"[RESUME] sanity: alive={alive_count} red.score={r_score:.3f} blue.score={b_score:.3f}")

            # Optional: just verify the checkpoint loads, then exit.
            if getattr(args, "print_resume_info", False):
                return

        else:
            grid = make_grid(device=device)
            registry = AgentsRegistry(grid)
            stats = SimulationStats()

            add_random_walls(grid)
            zones = make_zones(grid.size(1), grid.size(2), device=device)

            if str(getattr(config, "SPAWN_MODE", "symmetric")).lower() == "uniform":
                print("[UNIFORM_RANDOM_SPAWNING]")
                spawn_uniform_random(registry, grid, per_team=int(config.START_AGENTS_PER_TEAM))
            else:
                print("[SYMMETRIC_SPAWNING]")
                spawn_symmetric(registry, grid, per_team=int(config.START_AGENTS_PER_TEAM))

            engine_state = {}

        print("[INITIATING_TICK_ENGINE]")
        engine = TickEngine(registry, grid, stats, zones=zones)

        # Apply engine continuation state (respawn cooldown + agent_scores)
        apply_engine_state(engine, registry, stats, engine_state)

        # ------------------------------------------------------------
        # Results dir + writer
        # ------------------------------------------------------------
        rw = ResultsWriter()
        run_dir = Path(rw.start(_config_snapshot(resume_from=(meta.get("path", "") if resume_path else ""))))  # uses your existing API
        print(f"[main] Results â†’ {run_dir}")

        # ------------------------------------------------------------
        # Recorder (optional)
        # ------------------------------------------------------------
        recorder = _SimpleRecorder(
            run_dir=run_dir,
            grid=grid,
            fps=int(getattr(config, "TARGET_FPS", 60)),
            scale=2,
        )

        # ------------------------------------------------------------
        # Checkpoint dir (inside this run_dir)
        # ------------------------------------------------------------
        ckpt_dir = run_dir / "checkpoints"

        # ------------------------------------------------------------
        # Manual checkpoint hook (used by UI hotkey)
        # ------------------------------------------------------------
        def _utc_stamp_safe() -> str:
            # Safe for Windows paths (no ":" characters)
            return time.strftime("%Y%m%d_%H%M%S", time.gmtime())

        def _manual_checkpoint(reason: str = "hotkey") -> None:
            t = int(stats.tick)

            base = ckpt_dir / "manual" / f"{_utc_stamp_safe()}_tick{t:09d}"
            manual_dir = base

            # If user hits hotkey multiple times within same second, avoid collisions.
            i = 1
            while manual_dir.exists():
                manual_dir = Path(f"{base}_{i}")
                i += 1

            try:
                save_checkpoint(
                    manual_dir,
                    engine=engine,
                    registry=registry,
                    grid=grid,
                    stats=stats,
                    zones=zones,
                    resume_from=resume_path,
                    keep_last=2,  # irrelevant here; each manual save has its own folder
                )
                print(f"[CKPT] manual ({reason}) tick={t} -> {manual_dir / 'ckpt_latest.pt'}")
            except Exception as e:
                print(f"[CKPT] manual FAILED ({reason}) tick={t}: {e}")

        # Attach onto engine so UI can trigger it without importing checkpoint code
        engine._manual_checkpoint = _manual_checkpoint  # type: ignore[attr-defined]

        # Wrap engine.run_tick so BOTH UI mode and headless mode checkpoint the same way.
        _orig_run_tick = engine.run_tick

        def _run_tick_wrapped() -> None:
            _orig_run_tick()

            # Video recording (best effort, no crash)
            try:
                if recorder is not None:
                    recorder.write()
            except Exception:
                pass

            # Periodic checkpointing (optional)
            if ckpt_enabled and ckpt_every > 0 and (int(stats.tick) % int(ckpt_every)) == 0:
                try:
                    save_checkpoint(
                        ckpt_dir / "periodic",
                        engine=engine,
                        registry=registry,
                        grid=grid,
                        stats=stats,
                        zones=zones,
                        resume_from=resume_path,
                        keep_last=int(ckpt_keep_last),
                    )
                    print(f"[CKPT] periodic tick={int(stats.tick)} -> {ckpt_dir / 'periodic' / 'ckpt_latest.pt'}")
                except Exception as e:
                    print(f"[CKPT] periodic FAILED tick={int(stats.tick)}: {e}")

        engine.run_tick = _run_tick_wrapped  # type: ignore[assignment]

        # ------------------------------------------------------------
        # Graceful exit checkpoint
        # ------------------------------------------------------------
        def _save_on_exit(reason: str) -> None:
            if not ckpt_on_exit:
                return
            try:
                save_checkpoint(
                    ckpt_dir / "on_exit",
                    engine=engine,
                    registry=registry,
                    grid=grid,
                    stats=stats,
                    zones=zones,
                    resume_from=resume_path,
                    keep_last=int(ckpt_keep_last),
                )
                print(f"[CKPT] on_exit ({reason}) tick={int(stats.tick)} -> {ckpt_dir / 'on_exit' / 'ckpt_latest.pt'}")
            except Exception as e:
                print(f"[CKPT] on_exit FAILED ({reason}) tick={int(stats.tick)}: {e}")

        def _handle_sigint(sig, frame) -> None:  # type: ignore[no-untyped-def]
            _save_on_exit("SIGINT")
            raise KeyboardInterrupt()

        signal.signal(signal.SIGINT, _handle_sigint)

        # ------------------------------------------------------------
        # Run: UI or headless
        # ------------------------------------------------------------
        limit = int(getattr(config, "TICK_LIMIT", 0))

        # ------------------------------------------------------------
        # Run: UI or headless
        # ------------------------------------------------------------
        limit = int(getattr(config, "TICK_LIMIT", 0))

        if bool(getattr(config, "ENABLE_UI", True)):
            # Viewer expects a GRID tensor (not the engine) :contentReference[oaicite:4]{index=4}
            # and its run() expects (engine, registry, stats, ...) :contentReference[oaicite:5]{index=5}
            viewer = Viewer(
                grid,
                cell_size=int(getattr(config, "CELL_SIZE", 6)),  # UI scale
                show_grid=bool(getattr(config, "SHOW_GRID", True)),
            )

            viewer.run(
                engine,                 # TickEngine (has hooks like _manual_checkpoint)
                registry,               # AgentsRegistry (agent_data, brains)
                stats,                  # SimulationStats (tick, scores, etc.)
                tick_limit=limit,       # 0 means "run forever"
                target_fps=int(getattr(config, "TARGET_FPS", 60)),
            )
        else:
            _headless_loop(engine, stats, rw, limit)

        _save_on_exit("normal")

    except KeyboardInterrupt:
        # Already handled in _handle_sigint; just fall through
        pass
    except Exception:
        print("[main] CRASH â€” saving crash checkpoint if enabled.")
        try:
            if run_dir is not None:
                ckpt_dir = run_dir / "checkpoints"
                # We may not have engine in scope if crash happened early; best effort:
                # (If crash happened after engine creation, _save_on_exit already covers most.)
        except Exception:
            pass
        traceback.print_exc()
    finally:
        try:
            if recorder is not None:
                recorder.close()
        except Exception:
            pass

        print("[main] Shutdown complete.")


if __name__ == "__main__":
    main()

====[ END war_simulation\main.py ]============================================================


====[ 22/35 | war_simulation\recorder\__init__.py ]===========================================

====[ END war_simulation\recorder\__init__.py ]============================================================


====[ 23/35 | war_simulation\recorder\recorder.py ]===========================================
from __future__ import annotations

import datetime as _dt
from dataclasses import dataclass
from typing import Optional

try:
    import pyarrow as pa  # type: ignore
except Exception:
    pa = None  # graceful fallback


# --------- Arrow schema (if available) ----------
def tick_arrow_schema():
    """
    Parquet/Arrow schema for per-tick agent rows.
    Columns are intentionally simple (analytics-friendly).
    """
    if pa is None:
        return None
    return pa.schema([
        pa.field("tick", pa.int64()),
        pa.field("agent_id", pa.int32()),
        pa.field("team_id", pa.int16()),     # 2 or 3
        pa.field("is_alive", pa.bool_()),
        pa.field("pos_x", pa.int16()),
        pa.field("pos_y", pa.int16()),
        pa.field("hp", pa.float32()),
        pa.field("atk", pa.float32()),
        pa.field("action", pa.int16()),
        pa.field("logits", pa.list_(pa.float32())),  # variable length
    ])


# --------- Lightweight dataclass (for clarity/tests) ----------
@dataclass(frozen=True)
class RunMeta:
    started_utc: str
    grid_h: int
    grid_w: int
    obs_dim: int
    num_actions: int
    commit: Optional[str] = None
    note: Optional[str] = None

    @staticmethod
    def new(grid_h: int, grid_w: int, obs_dim: int, num_actions: int,
            commit: Optional[str] = None, note: Optional[str] = None) -> "RunMeta":
        return RunMeta(
            started_utc=_dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            grid_h=int(grid_h), grid_w=int(grid_w),
            obs_dim=int(obs_dim), num_actions=int(num_actions),
            commit=commit, note=note,
        )

====[ END war_simulation\recorder\recorder.py ]============================================================


====[ 24/35 | war_simulation\recorder\schemas.py ]============================================
# final_war_sim/recorder/schemas.py
from __future__ import annotations

import datetime as _dt
from dataclasses import dataclass
from typing import Optional

try:
    import pyarrow as pa  # type: ignore
except Exception:
    pa = None  # graceful fallback


# --------- Arrow schema (if available) ----------
def tick_arrow_schema():
    """
    Parquet/Arrow schema for per-tick agent rows.
    Columns are intentionally simple (analytics-friendly).
    """
    if pa is None:
        return None
    return pa.schema([
        pa.field("tick", pa.int64()),
        pa.field("agent_id", pa.int32()),
        pa.field("team_id", pa.int16()),     # 2 or 3
        pa.field("is_alive", pa.bool_()),
        pa.field("pos_x", pa.int16()),
        pa.field("pos_y", pa.int16()),
        pa.field("hp", pa.float32()),
        pa.field("atk", pa.float32()),
        pa.field("action", pa.int16()),
        pa.field("logits", pa.list_(pa.float32())),  # variable length
    ])


# --------- Lightweight dataclass (for clarity/tests) ----------
@dataclass(frozen=True)
class RunMeta:
    started_utc: str
    grid_h: int
    grid_w: int
    obs_dim: int
    num_actions: int
    commit: Optional[str] = None
    note: Optional[str] = None

    @staticmethod
    def new(grid_h: int, grid_w: int, obs_dim: int, num_actions: int,
            commit: Optional[str] = None, note: Optional[str] = None) -> "RunMeta":
        return RunMeta(
            started_utc=_dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
            grid_h=int(grid_h), grid_w=int(grid_w),
            obs_dim=int(obs_dim), num_actions=int(num_actions),
            commit=commit, note=note,
        )

====[ END war_simulation\recorder\schemas.py ]============================================================


====[ 25/35 | war_simulation\recorder\video_writer.py ]=======================================
from __future__ import annotations

import os
from typing import Optional

import numpy as np


class VideoWriter:
    """
    Lazy MP4 writer. If ffmpeg is unavailable, falls back to PNG frames.
    Expected frame: HxWx3 uint8 (RGB).
    """
    def __init__(self, run_dir: str, fps: int = 60):
        self.run_dir = os.path.abspath(run_dir)
        self.fps = int(max(1, fps))
        self._mp4_path = os.path.join(self.run_dir, "simulation.mp4")
        self._frames_dir = os.path.join(self.run_dir, "frames_fallback")
        self._writer = None
        self._mode = None  # "mp4" or "pngs"

    def _ensure_writer(self, frame: np.ndarray):
        if self._writer is not None:
            return
        # Try MP4 first
        try:
            import imageio  # type: ignore
            # H.264 sometimes not bundled; let imageio pick available codec
            self._writer = imageio.get_writer(self._mp4_path, fps=self.fps)
            self._mode = "mp4"
            return
        except Exception:
            # Fallback: PNG frames
            os.makedirs(self._frames_dir, exist_ok=True)
            self._writer = True  # sentinel
            self._mode = "pngs"

    def add_frame(self, frame: np.ndarray):
        if frame is None:
            return
        if frame.ndim != 3 or frame.shape[2] != 3:
            raise ValueError(f"VideoWriter expects HxWx3 RGB uint8; got {frame.shape}")
        if frame.dtype != np.uint8:
            frame = frame.astype(np.uint8, copy=False)
        self._ensure_writer(frame)

        if self._mode == "mp4":
            import imageio  # type: ignore
            self._writer.append_data(frame)
        else:
            # frames_fallback/frame_00000001.png
            idx = len(os.listdir(self._frames_dir)) + 1
            path = os.path.join(self._frames_dir, f"frame_{idx:08d}.png")
            try:
                import imageio  # type: ignore
                imageio.imwrite(path, frame)
            except Exception:
                # Last-ditch: numpy save (rare)
                np.save(path.replace(".png", ".npy"), frame)

    def close(self):
        if self._mode == "mp4" and self._writer is not None:
            try:
                self._writer.close()
            except Exception:
                pass
        self._writer = None

====[ END war_simulation\recorder\video_writer.py ]============================================================


====[ 26/35 | war_simulation\rl\__init__.py ]=================================================
# war_simulation/rl/__init__.py
"""
RL utilities.

Note: the live simulation can run without RL components.
Keep this package lightweight to avoid import-side effects.
"""

====[ END war_simulation\rl\__init__.py ]============================================================


====[ 27/35 | war_simulation\rl\ppo_runtime.py ]==============================================
from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, List, Tuple
import math
import torch
import torch.nn.functional as F
from torch import nn, optim
from torch.optim.lr_scheduler import CosineAnnealingLR

from .. import config
from ..engine.agent_registry import AgentsRegistry


@dataclass
class _Buf:
    obs: List[torch.Tensor]
    act: List[torch.Tensor]
    logp: List[torch.Tensor]
    val: List[torch.Tensor]
    rew: List[torch.Tensor]
    done: List[torch.Tensor]


class PerAgentPPORuntime:
    """
    Minimal per-agent PPO runtime.
    """

    def __init__(self, registry: AgentsRegistry, device: torch.device, obs_dim: int, act_dim: int):
        self.registry = registry
        self.device = device
        self.obs_dim = int(obs_dim)
        self.act_dim = int(act_dim)

        self.T        = int(getattr(config, "PPO_WINDOW_TICKS", 64))
        self.epochs   = int(getattr(config, "PPO_EPOCHS", 2))
        self.lr       = float(getattr(config, "PPO_LR", 3e-4))
        self.clip     = float(getattr(config, "PPO_CLIP", 0.2))
        self.ent_coef = float(getattr(config, "PPO_ENTROPY_COEF", 0.01))
        self.vf_coef  = float(getattr(config, "PPO_VALUE_COEF", 0.5))
        self.gamma    = float(getattr(config, "PPO_GAMMA", 0.99))
        self.lam      = float(getattr(config, "PPO_LAMBDA", 0.95))
        self.T_max = int(getattr(config, "PPO_LR_T_MAX", 500_000))
        self.eta_min = float(getattr(config, "PPO_LR_ETA_MIN", 1e-6))

        self._buf: Dict[int, _Buf] = {}
        self._opt: Dict[int, optim.Optimizer] = {}
        self._sched: Dict[int, CosineAnnealingLR] = {}
        self._step = 0

    def _assert_record_shapes(
        self,
        agent_ids: torch.Tensor,
        obs: torch.Tensor,
        logits: torch.Tensor,
        values: torch.Tensor,
        actions: torch.Tensor,
    ) -> None:
        # Device invariants (no silent CPU fallback)
        dev = self.device
        if agent_ids.device != dev or obs.device != dev or logits.device != dev or values.device != dev or actions.device != dev:
            raise RuntimeError(
                f"[ppo] device mismatch: ids={agent_ids.device} obs={obs.device} logits={logits.device} "
                f"values={values.device} actions={actions.device} expected={dev}"
            )
        # Shape invariants
        if agent_ids.dim() != 1:
            raise RuntimeError(f"[ppo] agent_ids must be (B,), got {tuple(agent_ids.shape)}")
        B = int(agent_ids.size(0))
        if obs.dim() != 2 or int(obs.size(0)) != B or int(obs.size(1)) != int(self.obs_dim):
            raise RuntimeError(f"[ppo] obs must be (B,{int(self.obs_dim)}), got {tuple(obs.shape)}")
        if logits.dim() != 2 or int(logits.size(0)) != B or int(logits.size(1)) != int(self.act_dim):
            raise RuntimeError(f"[ppo] logits must be (B,{int(self.act_dim)}), got {tuple(logits.shape)}")
        # Accept value as (B,) or (B,1) but normalize later
        if values.dim() == 2 and (int(values.size(0)) == B and int(values.size(1)) == 1):
            pass
        elif values.dim() == 1 and int(values.size(0)) == B:
            pass
        else:
            raise RuntimeError(f"[ppo] values must be (B,) or (B,1), got {tuple(values.shape)}")
        if actions.dim() != 1 or int(actions.size(0)) != B:
            raise RuntimeError(f"[ppo] actions must be (B,), got {tuple(actions.shape)}")

    def _assert_no_optimizer_sharing(self, aids: List[int]) -> None:
        """
        Defensive check: ensure we never accidentally share the same optimizer object across slots.
        This should always be true under the NO HIVE MIND constraint.
        """
        seen = {}
        for aid in aids:
            opt = self._opt.get(int(aid), None)
            if opt is None:
                continue
            key = id(opt)
            if key in seen and seen[key] != int(aid):
                raise RuntimeError(f"[ppo] optimizer object shared between slots {seen[key]} and {aid} (forbidden).")
            seen[key] = int(aid)

    def reset_agent(self, aid: int) -> None:
        """Hard-reset PPO state for a slot (buffer + optimizer + scheduler).

        Required when a *new* agent respawns into an existing slot so that
        optimizer moments and rollout history are not inherited.
        """
        assert 0 <= int(aid) < int(self.registry.capacity), f"aid out of range: {aid}"
        # Order matters: scheduler holds a ref to optimizer.
        self._sched.pop(int(aid), None)
        self._opt.pop(int(aid), None)
        self._buf.pop(int(aid), None)

    def reset_agents(self, aids: torch.Tensor | List[int]) -> None:
        """Vectorized helper; accepts LongTensor or Python list of slot indices."""
        if aids is None:
            return
        if isinstance(aids, torch.Tensor):
            if aids.numel() == 0:
                return
            lst = aids.to("cpu").tolist()
        else:
            if len(aids) == 0:
                return
            lst = list(aids)
        for a in lst:
            self.reset_agent(int(a))

    def _get_buf(self, aid: int) -> _Buf:
        if aid not in self._buf:
            self._buf[aid] = _Buf([], [], [], [], [], [])
        return self._buf[aid]

    def _get_opt(self, aid: int, model: nn.Module) -> optim.Optimizer:
        if aid not in self._opt:
            self._opt[aid] = optim.Adam(model.parameters(), lr=self.lr)
            self._sched[aid] = CosineAnnealingLR(self._opt[aid], T_max=self.T_max, eta_min=self.eta_min)
        return self._opt[aid]

    @torch.no_grad()
    def record_step(
        self,
        agent_ids: torch.Tensor,
        obs: torch.Tensor,
        logits: torch.Tensor,
        values: torch.Tensor,
        actions: torch.Tensor,
        rewards: torch.Tensor,
        done: torch.Tensor,
    ) -> None:
        """
        Append a single decision step for all agents in this tick.
        """
        # Fail loud on any mismatch instead of silently corrupting PPO buffers.
        self._assert_record_shapes(agent_ids, obs, logits, values, actions)

        logp_a = F.log_softmax(logits, dim=-1).gather(1, actions.view(-1, 1)).squeeze(1)

        for i in range(agent_ids.numel()):
            aid = int(agent_ids[i].item())
            b = self._get_buf(aid)
            b.obs.append(obs[i])
            b.act.append(actions[i])
            b.logp.append(logp_a[i])
            b.val.append(values[i].reshape(1))
            b.rew.append(rewards[i])
            b.done.append(done[i])

        self._step += 1
        if self._step % self.T == 0:
            self._train_window_and_clear()

    def _gae(self, rewards: torch.Tensor, values: torch.Tensor, dones: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        T = rewards.numel()
        adv = torch.zeros_like(rewards)
        last_gae = 0.0
        
        for t in reversed(range(T)):
            mask = 1.0 - float(dones[t].item())
            next_val_t = values[t + 1] if t < T - 1 else 0.0
            
            delta = rewards[t] + self.gamma * next_val_t * mask - values[t]
            last_gae = delta + self.gamma * self.lam * mask * last_gae
            adv[t] = last_gae
        
        ret = adv + values
        if adv.numel() > 1:
            adv = (adv - adv.mean()) / (adv.std(unbiased=False) + 1e-8)
        return adv, ret

    def _policy_value(self, model: nn.Module, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        logits, values = model(obs)
        values = values.squeeze(-1)
        logp = F.log_softmax(logits, dim=-1)
        entropy = -(logp.exp() * logp).sum(-1)
        return logits, values, entropy

    def _train_window_and_clear(self) -> None:
        if len(self._buf) == 0:
            return

        # Defensive check for "no hive mind" optimizer sharing.
        self._assert_no_optimizer_sharing(list(self._buf.keys()))

        for aid, b in list(self._buf.items()):
            if not b.obs: continue
            model = self.registry.brains[aid]
            if model is None:
                self._buf.pop(aid, None)
                continue

            model.train()
            opt = self._get_opt(aid, model)
            
            obs = torch.stack(b.obs)
            act = torch.stack(b.act).long()
            logp_old = torch.stack(b.logp)
            val_old = torch.cat(b.val)
            rew = torch.stack(b.rew)
            done = torch.stack(b.done).bool()

            adv, ret = self._gae(rew, val_old, done)

            with torch.enable_grad():
                for _ in range(self.epochs):
                    logits, values, entropy = self._policy_value(model, obs)
                    logp = F.log_softmax(logits, dim=-1).gather(1, act.view(-1,1)).squeeze(1)
                    ratio = torch.exp(logp - logp_old)
                    surr1, surr2 = ratio * adv, torch.clamp(ratio, 1.0 - self.clip, 1.0 + self.clip) * adv
                    
                    loss_pi = -torch.min(surr1, surr2).mean()
                    loss_v = F.mse_loss(values, ret)
                    loss_ent = -entropy.mean()
                    loss = loss_pi + self.vf_coef * loss_v + self.ent_coef * loss_ent

                    opt.zero_grad(set_to_none=True)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    opt.step()
            
            if aid in self._sched:
                self._sched[aid].step()

            b.obs.clear(); b.act.clear(); b.logp.clear(); b.val.clear(); b.rew.clear(); b.done.clear()
====[ END war_simulation\rl\ppo_runtime.py ]============================================================


====[ 28/35 | war_simulation\simulation\stats.py ]============================================
# war_simulation/simulation/stats.py
# =============================================================================
# SimulationStats: team-scoped scoring + structured death log
#
# Why this file exists:
# - We want "team-level" reward signals (kills, deaths, damage, CP points).
# - We also want a small, stable API that can be serialized into checkpoints
#   and written to CSV/JSON for analysis.
#
# Critical fix:
# - main.py writes summary.json and calls stats.red.as_row() / stats.blue.as_row()
#   in the FINALLY block.
# - Previously TeamCounters did NOT have as_row(), causing a crash during cleanup.
# - This file adds TeamCounters.as_row() in a strict, JSON-friendly way.
# =============================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List
import time

import config

TEAM_RED = "red"
TEAM_BLUE = "blue"


@dataclass
class TeamCounters:
    """
    TeamCounters holds "one teamâ€™s" aggregate numbers.

    Keep this:
    - extremely simple (only Python primitives)
    - easy to serialize (JSON-safe)
    - stable over time (donâ€™t rename keys casually)

    NOTE:
    - main.py expects TeamCounters.as_row() to exist for summary.json.
    """
    score: float = 0.0
    kills: int = 0
    deaths: int = 0
    dmg_dealt: float = 0.0
    dmg_taken: float = 0.0
    cp_points: float = 0.0  # capture points accumulated

    def clone(self) -> "TeamCounters":
        """
        Return an independent copy (safe snapshotting).
        """
        return TeamCounters(
            score=float(self.score),
            kills=int(self.kills),
            deaths=int(self.deaths),
            dmg_dealt=float(self.dmg_dealt),
            dmg_taken=float(self.dmg_taken),
            cp_points=float(self.cp_points),
        )

    def as_row(self) -> Dict[str, float | int]:
        """
        JSON/CSV-friendly representation.

        IMPORTANT:
        - main.py calls this in a finally block: must never throw.
        - Keep keys stable: downstream analysis scripts rely on them.
        """
        return {
            "score": float(self.score),
            "kills": int(self.kills),
            "deaths": int(self.deaths),
            "dmg_dealt": float(self.dmg_dealt),
            "dmg_taken": float(self.dmg_taken),
            "cp_points": float(self.cp_points),
        }


@dataclass
class Snapshot:
    """
    Snapshot is used for "delta since last tick-window" rewards,
    e.g., PPO team signals.
    """
    red: TeamCounters
    blue: TeamCounters
    tick: int


class SimulationStats:
    """
    Team-scoped scoring + dead-agent log.

    Designed for:
    - fast per-tick accounting
    - optional PPO delta signals
    - safe serialization for checkpointing

    Public fields used across the codebase:
    - red, blue: TeamCounters
    - tick: int
    """

    def __init__(self) -> None:
        self.red = TeamCounters()
        self.blue = TeamCounters()
        self.tick = 0

        # High precision timer start point (monotonic)
        self._t0 = time.perf_counter()

        # A small structured log you can dump to JSON/CSV (deaths etc.)
        self._dead_log: List[Dict[str, float | int]] = []

    # -------------------------------------------------------------------------
    # Timing
    # -------------------------------------------------------------------------
    @property
    def elapsed_seconds(self) -> float:
        """
        Elapsed wall-clock time since stats object creation.
        """
        return float(time.perf_counter() - self._t0)

    def on_tick_advanced(self, dt: int) -> None:
        """
        Called by engine after each simulation step.
        """
        self.tick += int(dt)

    # -------------------------------------------------------------------------
    # Internal helpers
    # -------------------------------------------------------------------------
    def _team(self, name: str) -> TeamCounters:
        """
        Map "red"/"blue" string -> the correct TeamCounters.
        """
        return self.red if name == TEAM_RED else self.blue

    # -------------------------------------------------------------------------
    # Points wiring
    # -------------------------------------------------------------------------
    def add_damage_dealt(self, team: str, amount: float) -> None:
        """
        Team dealt damage to opponents.
        """
        t = self._team(team)
        amt = float(amount)
        t.dmg_dealt += amt
        t.score += amt * float(config.TEAM_DMG_DEALT_REWARD)

    def add_damage_taken(self, team: str, amount: float) -> None:
        """
        Team took damage.
        """
        t = self._team(team)
        amt = float(amount)
        t.dmg_taken += amt
        t.score += amt * float(config.TEAM_DMG_TAKEN_PENALTY)

    def add_kill(self, team: str, count: int = 1) -> None:
        """
        Team killed opponents.
        """
        t = self._team(team)
        c = int(count)
        t.kills += c
        t.score += float(c) * float(config.TEAM_KILL_REWARD)

    def add_death(self, team: str, count: int = 1) -> None:
        """
        Team lost agents.
        """
        t = self._team(team)
        c = int(count)
        t.deaths += c
        t.score += float(c) * float(config.TEAM_DEATH_PENALTY)

    def add_capture_points(self, team: str, amount: float) -> None:
        """
        Capture point accumulation.
        """
        t = self._team(team)
        amt = float(amount)
        t.cp_points += amt
        # In your design, CP points directly contribute to score.
        t.score += amt

    # -------------------------------------------------------------------------
    # Structured death log
    # -------------------------------------------------------------------------
    def record_death_entry(
        self,
        agent_id: int,
        team_id_val: float,
        x: int,
        y: int,
        killer_team_id_val: float | int,
    ) -> None:
        """
        Record a single death entry.

        Keep it small:
        - never store tensors here
        - store primitive fields only
        """
        self._dead_log.append(
            {
                "tick": int(self.tick),
                "agent_id": int(agent_id),
                "team": "red" if float(team_id_val) == 2.0 else "blue",
                "x": int(x),
                "y": int(y),
                "killer_team": "red" if float(killer_team_id_val) == 2.0 else "blue",
            }
        )

    def drain_dead_log(self) -> List[Dict[str, float | int]]:
        """
        Return current log and reset buffer (useful for streaming writes).
        """
        buf = self._dead_log
        self._dead_log = []
        return buf

    # -------------------------------------------------------------------------
    # Snapshots / deltas / rows
    # -------------------------------------------------------------------------
    def snapshot(self) -> Snapshot:
        """
        Capture a point-in-time snapshot for delta reward computation.
        """
        return Snapshot(self.red.clone(), self.blue.clone(), int(self.tick))

    def delta_since(self, snap: Snapshot) -> Dict[str, float]:
        """
        Score delta since a previous snapshot.
        """
        return {
            TEAM_RED: float(self.red.score - snap.red.score),
            TEAM_BLUE: float(self.blue.score - snap.blue.score),
        }

    def as_row(self) -> Dict[str, float]:
        """
        A single-row, flat dict (good for CSV).

        NOTE:
        - These keys match what your results writer likely expects.
        """
        return {
            "tick": float(self.tick),
            "elapsed_s": float(self.elapsed_seconds),
            "red_score": float(self.red.score),
            "blue_score": float(self.blue.score),
            "red_kills": float(self.red.kills),
            "blue_kills": float(self.blue.kills),
            "red_deaths": float(self.red.deaths),
            "blue_deaths": float(self.blue.deaths),
            "red_dmg_dealt": float(self.red.dmg_dealt),
            "blue_dmg_dealt": float(self.blue.dmg_dealt),
            "red_dmg_taken": float(self.red.dmg_taken),
            "blue_dmg_taken": float(self.blue.dmg_taken),
            "red_cp_points": float(self.red.cp_points),
            "blue_cp_points": float(self.blue.cp_points),
        }

====[ END war_simulation\simulation\stats.py ]============================================================


====[ 29/35 | war_simulation\ui\__init__.py ]=================================================
# final_war_sim/ui/__init__.py
from .viewer import Viewer

__all__ = ["Viewer"]

====[ END war_simulation\ui\__init__.py ]============================================================


====[ 30/35 | war_simulation\ui\camera.py ]===================================================
from __future__ import annotations

class Camera:
    """
    Screen <-> world transform with integer-friendly grid rendering.
    """
    def __init__(self, cell_pixels: int, world_w: int, world_h: int):
        self.base_cell = max(1, int(cell_pixels))
        self.zoom = 1.0
        self.offset_x = 0.0  # in cells
        self.offset_y = 0.0  # in cells
        self.world_w = int(world_w)
        self.world_h = int(world_h)

    # --- derived ---
    @property
    def cell_px(self) -> int:
        return max(1, int(round(self.base_cell * self.zoom)))

    # --- ops ---
    def pan(self, dx_cells: float, dy_cells: float) -> None:
        self.offset_x = float(min(max(self.offset_x + dx_cells, 0.0), self.world_w - 1))
        self.offset_y = float(min(max(self.offset_y + dy_cells, 0.0), self.world_h - 1))

    def zoom_at(self, factor: float) -> None:
        self.zoom = float(min(max(self.zoom * factor, 0.25), 8.0))

    # --- transforms ---
    def world_to_screen(self, x_cell: int, y_cell: int) -> tuple[int, int]:
        px = int((x_cell - self.offset_x) * self.cell_px)
        py = int((y_cell - self.offset_y) * self.cell_px)
        return px, py

    def screen_to_world(self, px: int, py: int) -> tuple[int, int]:
        x = int(self.offset_x + px / self.cell_px)
        y = int(self.offset_y + py / self.cell_px)
        x = max(0, min(self.world_w - 1, x))
        y = max(0, min(self.world_h - 1, y))
        return x, y

====[ END war_simulation\ui\camera.py ]============================================================


====[ 31/35 | war_simulation\ui\viewer.py ]===================================================
# war_simulation/ui/viewer.py
# =============================================================================
# Viewer: pygame visualization + input + HUD
#
# What this file is responsible for:
# - Rendering the world grid, agents, overlays, minimap, stats panels
# - Handling user input (hotkeys and mouse)
# - Running the sim loop in UI mode (drives engine.run_tick)
#
# Critical fixes in this version:
# 1) SPACE pause was missing. Now SPACE toggles Viewer.paused.
# 2) Duplicate 'S' handler removed (it was present twice).
# 3) "S" is both "pan down" (WASD) AND "save selected brain".
#    We keep your design, but prevent a single S press used for saving
#    from also panning down in the same frame.
# 4) Side panel legend updated to show:
#    - F5: Save simulation checkpoint
# =============================================================================

from __future__ import annotations

import os
import collections
from typing import List, Optional, Dict
import math

import pygame
import torch
import torch.nn as nn
import numpy as np

import config
from .camera import Camera
from engine.agent_registry import (
    COL_ALIVE, COL_TEAM, COL_HP, COL_X, COL_Y, COL_UNIT,
    COL_HP_MAX, COL_VISION, COL_ATK, COL_AGENT_ID
)

# -----------------------------------------------------------------------------
# Visual constants
# -----------------------------------------------------------------------------
FONT_NAME = "consolas"

COLORS = {
    "bg": (20, 22, 28), "hud_bg": (12, 14, 18), "side_bg": (18, 20, 26),
    "grid": (40, 42, 48), "border": (70, 74, 82), "wall": (90, 94, 102),
    "empty": (24, 26, 32),

    "red_soldier": (231, 76, 60),
    "red_archer":  (211, 84, 0),
    "red":         (231, 76, 60),

    "blue_soldier": (52, 152, 219),
    "blue_archer":  (22, 160, 133),
    "blue":         (52, 152, 219),

    "archer_glyph": (245, 230, 90), "marker": (242, 228, 92),
    "text": (230, 230, 230), "text_dim": (180, 186, 194),
    "green": (46, 204, 113), "warn": (243, 156, 18),
    "bar_bg": (38, 42, 48), "bar_fg": (46, 204, 113),
    "graph_red": (231, 76, 60, 150), "graph_blue": (52, 152, 219, 150),
    "graph_grid": (60, 60, 70), "pause_text": (241, 196, 15),
}

OVERLAYS = {
    "heal": (46, 204, 113, 60), "cp": (210, 210, 230, 48),
    "outline_red": (231, 76, 60, 160), "outline_blue": (52, 152, 219, 160),
    "outline_neutral": (160, 160, 170, 120),
    "threat_enemy": (231, 76, 60), "threat_ally": (52, 152, 219),
    "vision_range": (180, 180, 180, 40),
}

RAY_COLORS = {
    "ally": (52, 152, 219), "enemy": (231, 76, 60),
    "wall": (180, 180, 180), "empty": (100, 100, 110),
}


# -----------------------------------------------------------------------------
# Utility helpers
# -----------------------------------------------------------------------------
def _center_window() -> None:
    # Pygame reads this env var on window creation.
    os.environ.setdefault("SDL_VIDEO_CENTERED", "1")


def _get_model_summary(model: nn.Module) -> str:
    """
    A small human-readable summary of the model architecture for the inspector.
    """
    name = model.__class__.__name__.lower()

    # Try to detect transformer-ish models.
    if "transformer" in name:
        try:
            d = getattr(model, "embed_dim", None)
            num_cross = 1 if hasattr(model, "cross_attention") else 0
            num_self = 1 if hasattr(model, "self_attention") else 0
            if d is not None:
                return f"Transformer(d={d}, Cross={num_cross}, Self={num_self})"
            return "TransformerBrain"
        except Exception:
            return "TransformerBrain"

    # For MLPs, attempt to show dims.
    try:
        linears = [m for m in model.modules() if isinstance(m, nn.Linear)]
        dims = [linears[0].in_features] + [m.out_features for m in linears]
        return "â†’".join(map(str, dims))
    except Exception:
        return "Unknown"


def _param_count(model: nn.Module) -> int:
    """
    Trainable parameters only.
    """
    return int(sum(p.numel() for p in model.parameters() if p.requires_grad))


class TextCache:
    """
    Font + render caching.
    Without caching, pygame text rendering becomes a CPU hotspot.
    """
    def __init__(self) -> None:
        self.fonts = {
            13: self._mk_font(13),
            16: self._mk_font(16),
            18: self._mk_font(18),
        }
        self.cache: Dict[tuple, pygame.Surface] = {}

    def _mk_font(self, sz: int) -> pygame.font.Font:
        try:
            return pygame.font.SysFont(FONT_NAME, sz)
        except Exception:
            return pygame.font.Font(None, sz)

    def render(self, text: str, size: int, color, aa: bool = True) -> pygame.Surface:
        key = (text, size, color, aa)
        if key not in self.cache:
            self.cache[key] = self.fonts[size].render(text, aa, color)
        return self.cache[key]


# -----------------------------------------------------------------------------
# Layout
# -----------------------------------------------------------------------------
class LayoutManager:
    def __init__(self, viewer: "Viewer") -> None:
        self.viewer = viewer

    def side_width(self) -> int:
        return max(320, min(420, int(self.viewer.Wpix * 0.27)))

    def world_rect(self) -> pygame.Rect:
        m = self.viewer.margin
        return pygame.Rect(
            m,
            m,
            max(64, self.viewer.Wpix - self.side_width() - 3 * m),
            max(64, self.viewer.Hpix - 126 - 2 * m),
        )

    def side_rect(self) -> pygame.Rect:
        m, side_w = self.viewer.margin, self.side_width()
        return pygame.Rect(
            self.viewer.Wpix - side_w - m,
            m,
            side_w,
            max(64, self.viewer.Hpix - 126 - 2 * m),
        )

    def hud_rect(self) -> pygame.Rect:
        return pygame.Rect(0, self.viewer.Hpix - 126, self.viewer.Wpix, 126)


# -----------------------------------------------------------------------------
# World renderer
# -----------------------------------------------------------------------------
class WorldRenderer:
    def __init__(self, viewer: "Viewer", grid: torch.Tensor, registry) -> None:
        self.viewer, self.grid, self.registry = viewer, grid, registry
        self.cam = viewer.cam

        # Cached pre-rendered background (walls + empty + zone overlays).
        self.static_surf: Optional[pygame.Surface] = None

        # Precomputed zone shapes for fast drawing.
        self._zone_cache = {"heal_tiles": [], "cp_rects": []}

    def build_static_cache(self, engine) -> None:
        """
        Called once at startup (or when zones change) to extract zone masks.
        """
        self.static_surf = None
        self._zone_cache = {"heal_tiles": [], "cp_rects": []}

        zones = getattr(engine, "zones", None)
        if not zones:
            return

        # Heal tiles -> list of (x,y).
        if getattr(zones, "heal_mask", None) is not None:
            ys, xs = torch.nonzero(zones.heal_mask, as_tuple=True)
            self._zone_cache["heal_tiles"] = list(zip(xs.cpu().tolist(), ys.cpu().tolist()))

        # Capture points -> minimal rectangles (cheap to draw).
        for m in getattr(zones, "cp_masks", []):
            ys, xs = torch.nonzero(m, as_tuple=True)
            if xs.numel() > 0:
                self._zone_cache["cp_rects"].append(
                    (xs.min().item(), ys.min().item(), xs.max().item() + 1, ys.max().item() + 1)
                )

    def _draw_static_background(self) -> None:
        """
        Build static_surf for the current world rect size.
        This should only happen when window size or zoom changes.
        """
        wrect = self.viewer.layout.world_rect()

        self.static_surf = pygame.Surface(wrect.size)
        self.static_surf.fill(COLORS["bg"])

        H, W = self.grid.shape[1], self.grid.shape[2]

        # NOTE:
        # This CPU read is expensive (sync). We keep it only in the static cache builder,
        # and we keep dynamic state refresh on a lower frequency elsewhere.
        occ_np = self.grid[0].detach().cpu().numpy()

        # Draw empty/walls
        for y in range(H):
            for x in range(W):
                occ = occ_np[y, x]
                if occ in {0, 1}:
                    color = COLORS["empty"] if occ == 0 else COLORS["wall"]
                    cx, cy = self.cam.world_to_screen(x, y)
                    pygame.draw.rect(self.static_surf, color, (cx, cy, self.cam.cell_px, self.cam.cell_px))

        # Heal zone overlay
        overlay = pygame.Surface(wrect.size, pygame.SRCALPHA)
        for x, y in self._zone_cache["heal_tiles"]:
            cx, cy = self.cam.world_to_screen(x, y)
            pygame.draw.rect(overlay, OVERLAYS["heal"], (cx, cy, self.cam.cell_px, self.cam.cell_px))

        self.static_surf.blit(overlay, (0, 0))

    def draw(self, surf: pygame.Surface, state_data: dict) -> None:
        wrect = self.viewer.layout.world_rect()

        # Rebuild static cache if window resized / zoom changed.
        if self.static_surf is None or self.static_surf.get_size() != wrect.size:
            self._draw_static_background()

        # Blit static background at correct position.
        surf.blit(self.static_surf, wrect.topleft)

        c = self.cam.cell_px

        # Capture point overlay (depends on team occupancy)
        cp_overlay = pygame.Surface(wrect.size, pygame.SRCALPHA)
        for x0, y0, x1, y1 in self._zone_cache["cp_rects"]:
            patch = state_data["occ_np"][y0:y1, x0:x1]
            red_cnt, blue_cnt = (patch == 2).sum(), (patch == 3).sum()

            if red_cnt > blue_cnt:
                b_col, label = OVERLAYS["outline_red"], ("R", COLORS["red"])
            elif blue_cnt > red_cnt:
                b_col, label = OVERLAYS["outline_blue"], ("B", COLORS["blue"])
            else:
                b_col, label = OVERLAYS["outline_neutral"], ("â€“", COLORS["text_dim"])

            cx0, cy0 = self.cam.world_to_screen(x0, y0)
            cx1, cy1 = self.cam.world_to_screen(x1, y1)
            rect = pygame.Rect(cx0, cy0, cx1 - cx0, cy1 - cy0)

            pygame.draw.rect(cp_overlay, b_col, rect, max(1, c // 2))
            lab_surf = self.viewer.text_cache.render(label[0], 13, label[1])
            cp_overlay.blit(lab_surf, lab_surf.get_rect(center=rect.center))

        surf.blit(cp_overlay, wrect.topleft)

        # Agents
        for slot_id in state_data["alive_indices"]:
            entry = state_data["agent_map"].get(slot_id, None)
            if entry is None:
                continue
            x, y, unit, team, _uid, _btype = entry
            color_key = f"{'red' if team == 2.0 else 'blue'}_{'archer' if unit == 2.0 else 'soldier'}"
            color = COLORS[color_key]
            cx, cy = self.cam.world_to_screen(int(x), int(y))
            pygame.draw.rect(surf, color, (wrect.x + cx, wrect.y + cy, c, c))

            # Archer glyph for readability.
            if unit == 2.0 and c > 4:
                pygame.draw.circle(
                    surf,
                    COLORS["archer_glyph"],
                    (wrect.x + cx + c // 2, wrect.y + cy + c // 2),
                    max(2, c // 2 - 1),
                    max(1, c // 6),
                )

        if self.viewer.battle_view_enabled:
            self._draw_hp_bars(surf, wrect, c, state_data)
        if self.viewer.show_brain_types:
            self._draw_brain_labels(surf, wrect, c, state_data)
        if self.viewer.threat_vision_mode:
            self._draw_threat_vision(surf, wrect, c, state_data)
        if self.viewer.show_grid and c >= 6:
            self._draw_grid_lines(surf, wrect, c)

        self._draw_markers(surf, wrect, c, state_data["id_np"])

        if self.viewer.show_rays:
            self._draw_rays(surf, wrect, c, state_data)

    def _draw_hp_bars(self, surf, wrect, c, state_data) -> None:
        if c < 8:
            return

        hp_bar_surf = pygame.Surface(wrect.size, pygame.SRCALPHA)
        for slot_id in state_data["alive_indices"]:
            entry = state_data["agent_map"].get(slot_id, None)
            if entry is None:
                continue
            x, y, _unit, _team, _uid, _btype = entry

            hp_max = float(self.registry.agent_data[slot_id, COL_HP_MAX].item())
            hp = float(self.registry.agent_data[slot_id, COL_HP].item())
            hp_ratio = (hp / hp_max) if hp_max > 0 else 0.0

            cx, cy = self.cam.world_to_screen(int(x), int(y))
            bar_w, bar_h = c, max(1, c // 8)
            bar_y = cy - bar_h - 2

            if 0 <= bar_y < wrect.height and 0 <= cx < wrect.width:
                pygame.draw.rect(hp_bar_surf, (50, 50, 50), (cx, bar_y, bar_w, bar_h))
                pygame.draw.rect(hp_bar_surf, COLORS["green"], (cx, bar_y, bar_w * hp_ratio, bar_h))

        surf.blit(hp_bar_surf, wrect.topleft)

    def _draw_brain_labels(self, surf, wrect, c, state_data) -> None:
        """
        Draw a tiny brain-type character on the agent.
        """
        if c < 8:
            return

        for slot_id in state_data["alive_indices"]:
            entry = state_data["agent_map"].get(slot_id, None)
            if entry is None:
                continue
            x, y, _unit, _team, _uid, btype = entry

            cx, cy = self.cam.world_to_screen(int(x), int(y))
            if 0 <= cx < wrect.width and 0 <= cy < wrect.height:
                font_size = max(10, min(16, c // 2))
                lab_surf = self.viewer.text_cache.render(btype, font_size, COLORS["text"])
                rect = lab_surf.get_rect(center=(wrect.x + cx + c // 2, wrect.y + cy + c // 2))
                surf.blit(lab_surf, rect)

    def _draw_threat_vision(self, surf, wrect, c, state_data) -> None:
        slot_id = self.viewer.selected_slot_id
        if slot_id is None or slot_id not in state_data["agent_map"]:
            return

        my_x, my_y, _unit, my_team, _uid, _btype = state_data["agent_map"][slot_id]
        my_x = float(my_x)
        my_y = float(my_y)

        my_cx, my_cy = self.cam.world_to_screen(int(my_x), int(my_y))
        vision_range = float(self.registry.agent_data[slot_id, COL_VISION].item())
        vision_px = int(vision_range * c)

        overlay = pygame.Surface(wrect.size, pygame.SRCALPHA)
        center_px = (my_cx + c // 2, my_cy + c // 2)
        pygame.draw.circle(overlay, OVERLAYS["vision_range"], center_px, vision_px)

        for other_id, (ox, oy, _ou, o_team, _ouid, _obtype) in state_data["agent_map"].items():
            if slot_id == other_id:
                continue
            dist = float(np.hypot(ox - my_x, oy - my_y))
            if dist <= vision_range:
                o_cx, o_cy = self.cam.world_to_screen(int(ox), int(oy))

                hp_max = float(self.registry.agent_data[other_id, COL_HP_MAX].item())
                hp = float(self.registry.agent_data[other_id, COL_HP].item())
                hp_ratio = (hp / hp_max) if hp_max > 0 else 0.0

                if o_team != my_team:
                    radius = int(c * 0.7 * (0.5 + hp_ratio * 0.5))
                    alpha = int(50 + 150 * hp_ratio)
                    pygame.draw.circle(
                        overlay,
                        (*OVERLAYS["threat_enemy"], alpha),
                        (o_cx + c // 2, o_cy + c // 2),
                        radius,
                    )
                else:
                    pygame.draw.circle(
                        overlay,
                        (*OVERLAYS["threat_ally"], 100),
                        (o_cx + c // 2, o_cy + c // 2),
                        int(c * 0.4),
                        1,
                    )

        surf.blit(overlay, wrect.topleft)

    def _draw_grid_lines(self, surf, wrect, c) -> None:
        ax, ay = self.cam.world_to_screen(0, 0)
        off_x, off_y = (c - (ax % c)) % c, (c - (ay % c)) % c
        x, y = wrect.x + off_x, wrect.y + off_y

        while x < wrect.right:
            pygame.draw.line(surf, COLORS["grid"], (x, wrect.y), (x, wrect.bottom))
            x += c

        while y < wrect.bottom:
            pygame.draw.line(surf, COLORS["grid"], (wrect.x, y), (wrect.right, y))
            y += c

        pygame.draw.rect(surf, COLORS["border"], wrect, 2)

    def _draw_markers(self, surf, wrect, c, id_np) -> None:
        for slot_id in self.viewer.marked:
            pos = np.argwhere(id_np == slot_id)
            if pos.size > 0:
                y, x = pos[0]
                cx, cy = self.cam.world_to_screen(int(x), int(y))
                pygame.draw.rect(
                    surf,
                    COLORS["marker"],
                    (wrect.x + cx, wrect.y + cy, c, c),
                    max(1, c // 8),
                )

    def _draw_rays(self, surf, wrect, c, state_data) -> None:
        slot_id = self.viewer.selected_slot_id
        if slot_id is None or slot_id not in state_data["agent_map"]:
            return

        agent_x, agent_y, _unit, my_team, _uid, _btype = state_data["agent_map"][slot_id]
        agent_x = float(agent_x)
        agent_y = float(agent_y)

        start_pos_screen = (
            wrect.x + self.cam.world_to_screen(int(agent_x), int(agent_y))[0] + c // 2,
            wrect.y + self.cam.world_to_screen(int(agent_x), int(agent_y))[1] + c // 2,
        )

        vision_range = int(float(self.registry.agent_data[slot_id, COL_VISION].item()))
        occ_grid = state_data["occ_np"]
        H, W = occ_grid.shape

        num_rays_to_draw = 32
        for i in range(num_rays_to_draw):
            angle = i * (2 * math.pi / num_rays_to_draw)
            dx, dy = math.cos(angle), math.sin(angle)

            end_x, end_y = agent_x, agent_y
            color = RAY_COLORS["empty"]

            for step in range(1, vision_range + 1):
                test_x = int(round(agent_x + dx * step))
                test_y = int(round(agent_y + dy * step))

                if not (0 <= test_x < W and 0 <= test_y < H):
                    end_x, end_y = test_x, test_y
                    break

                occupant = occ_grid[test_y, test_x]
                if occupant != 0:
                    end_x, end_y = test_x, test_y
                    if occupant == 1:
                        color = RAY_COLORS["wall"]
                    else:
                        hit_team = occupant
                        color = RAY_COLORS["enemy"] if my_team != hit_team else RAY_COLORS["ally"]
                    break
            else:
                end_x = agent_x + dx * vision_range
                end_y = agent_y + dy * vision_range

            end_pos_world = self.cam.world_to_screen(int(end_x), int(end_y))
            end_pos_screen = (wrect.x + end_pos_world[0] + c // 2, wrect.y + end_pos_world[1] + c // 2)
            pygame.draw.line(surf, color, start_pos_screen, end_pos_screen, 1)


# -----------------------------------------------------------------------------
# HUD / Side / Minimap
# -----------------------------------------------------------------------------
class HudPanel:
    def __init__(self, viewer: "Viewer", stats) -> None:
        self.viewer, self.stats = viewer, stats
        self.score_history = collections.deque(maxlen=1000)

    def update(self) -> None:
        # One point per tick: team score delta shown as a graph.
        self.score_history.append(self.stats.red.score - self.stats.blue.score)

    def draw(self, surf: pygame.Surface, state_data: dict) -> None:
        hud = self.viewer.layout.hud_rect()
        surf.fill(COLORS["hud_bg"], hud)

        y, x = hud.y + 8, 12

        pause_str = "[ PAUSED ]" if self.viewer.paused else f"[ {self.viewer.speed_multiplier}x ]"
        surf.blit(
            self.viewer.text_cache.render(
                f"Tick {self.stats.tick} {pause_str}",
                16,
                COLORS["pause_text"] if self.viewer.paused else COLORS["text"],
            ),
            (x, y),
        )

        self._draw_team_stats(surf, y, x, state_data)
        self._draw_score_graph(surf, hud)
        self.viewer.minimap.draw(surf, hud, state_data)

    def _draw_team_stats(self, surf, y: int, x: int, state_data: dict) -> None:
        r_alive = sum(1 for _, _, _, team, _, _ in state_data["agent_map"].values() if team == 2.0)
        b_alive = len(state_data["agent_map"]) - r_alive

        rs_alive = sum(1 for _, _, unit, team, _, _ in state_data["agent_map"].values() if team == 2.0 and unit == 1.0)
        ra_alive = r_alive - rs_alive

        bs_alive = sum(1 for _, _, unit, team, _, _ in state_data["agent_map"].values() if team == 3.0 and unit == 1.0)
        ba_alive = b_alive - bs_alive

        red_str = (
            f"Red  S:{self.stats.red.score:6.1f} CP:{self.stats.red.cp_points:4.1f} "
            f"K:{self.stats.red.kills:3d} D:{self.stats.red.deaths:3d} "
            f"Alive:{r_alive:3d} (S:{rs_alive} A:{ra_alive})"
        )
        blue_str = (
            f"Blue S:{self.stats.blue.score:6.1f} CP:{self.stats.blue.cp_points:4.1f} "
            f"K:{self.stats.blue.kills:3d} D:{self.stats.blue.deaths:3d} "
            f"Alive:{b_alive:3d} (S:{bs_alive} A:{ba_alive})"
        )

        surf.blit(self.viewer.text_cache.render(red_str, 16, COLORS["red"]), (x, y + 24))
        surf.blit(self.viewer.text_cache.render(blue_str, 16, COLORS["blue"]), (x, y + 48))

    def _draw_score_graph(self, surf: pygame.Surface, hud: pygame.Rect) -> None:
        graph_rect = pygame.Rect(hud.right - 540, hud.y + 10, 300, hud.height - 20)
        pygame.draw.rect(surf, COLORS["bg"], graph_rect)

        if len(self.score_history) < 2:
            return

        scores = np.array(self.score_history, dtype=np.float32)
        max_abs_score = float(np.max(np.abs(scores)) or 1.0)
        norm_scores = scores / max_abs_score

        denom = max(1, (len(scores) - 1))
        points = []
        for i, s in enumerate(norm_scores):
            s = float(s)  # numpy scalar -> python float
            px = graph_rect.x + (i / denom) * graph_rect.width
            py = graph_rect.centery - s * (graph_rect.height / 2.2)

            if not (math.isfinite(px) and math.isfinite(py)):
                continue

            points.append((int(px), int(py)))

        if len(points) < 2:
            return

        for i in range(1, 4):
            pygame.draw.line(
                surf,
                COLORS["graph_grid"],
                (graph_rect.x, graph_rect.y + i * graph_rect.height / 4),
                (graph_rect.right, graph_rect.y + i * graph_rect.height / 4),
            )

        red_poly_points = [(p[0], p[1]) for p in points if p[1] < graph_rect.centery]
        if red_poly_points:
            red_poly = [(graph_rect.x, graph_rect.centery)] + red_poly_points + [(red_poly_points[-1][0], graph_rect.centery)]
            pygame.draw.polygon(surf, COLORS["graph_red"], red_poly)

        blue_poly_points = [(p[0], p[1]) for p in points if p[1] > graph_rect.centery]
        if blue_poly_points:
            blue_poly = [(graph_rect.x, graph_rect.centery)] + blue_poly_points + [(blue_poly_points[-1][0], graph_rect.centery)]
            pygame.draw.polygon(surf, COLORS["graph_blue"], blue_poly)

        pygame.draw.aalines(surf, COLORS["text"], False, points)
        pygame.draw.rect(surf, COLORS["border"], graph_rect, 1)
        surf.blit(self.viewer.text_cache.render("Score Lead", 13, COLORS["text_dim"]), (graph_rect.x + 5, graph_rect.y + 2))


class SidePanel:
    def __init__(self, viewer: "Viewer", registry) -> None:
        self.viewer, self.registry = viewer, registry

    def _draw_legend(self, surf: pygame.Surface, x: int, y: int) -> int:
        y += 20
        surf.blit(self.viewer.text_cache.render("Legend & Controls", 18, COLORS["text"]), (x, y))
        y += 30

        legend_items = {
            "Red Soldier": COLORS["red_soldier"],
            "Red Archer": COLORS["red_archer"],
            "Blue Soldier": COLORS["blue_soldier"],
            "Blue Archer": COLORS["blue_archer"],
        }
        for label, color in legend_items.items():
            pygame.draw.rect(surf, color, (x, y, 15, 15))
            surf.blit(self.viewer.text_cache.render(label, 13, COLORS["text_dim"]), (x + 25, y))
            y += 22

        y += 20

        # IMPORTANT:
        # Keep these lines accurate. This is user-facing documentation.
        controls = [
            "WASD / Arrows: Pan Camera",
            "Mouse Wheel: Zoom",
            "Click Agent: Inspect",
            "SPACE: Pause Simulation",
            ". (period): Step 1 tick (while paused)",
            "+ / -: Change Speed",
            "R: Toggle Agent Rays",
            "T: Toggle Threat Vision",
            "B: Toggle HP Bars",
            "N: Toggle Brain Labels",
            "S: Save Selected Brain",
            "Ctrl+S: Save Simulation Checkpoint",
            "F5: Save Simulation Checkpoint",
            "F11: Toggle Fullscreen",
        ]
        for line in controls:
            surf.blit(self.viewer.text_cache.render(line, 13, COLORS["text_dim"]), (x, y))
            y += 18

        return y

    def draw(self, surf: pygame.Surface, state_data: dict) -> None:
        srect = self.viewer.layout.side_rect()
        surf.fill(COLORS["side_bg"], srect)
        pygame.draw.rect(surf, COLORS["border"], srect, 2)

        pad, y = 12, srect.y + 12
        x = srect.x + pad
        surf.blit(self.viewer.text_cache.render("Agent Inspector", 18, COLORS["text"]), (x, y))
        y += 30

        slot_id = self.viewer.selected_slot_id
        if slot_id is None:
            surf.blit(self.viewer.text_cache.render("Click an agent to inspect.", 13, COLORS["text_dim"]), (x, y))
            y += 30

        elif slot_id not in state_data["agent_map"]:
            uid_str = (
                f"ID: {self.viewer.last_selected_uid} (Dead)"
                if self.viewer.last_selected_uid is not None
                else f"Slot: {slot_id} (Dead)"
            )
            surf.blit(self.viewer.text_cache.render(uid_str, 13, COLORS["warn"]), (x, y))
            y += 30

        else:
            _ax, _ay, _u, _t, unique_id, _btype = state_data["agent_map"][slot_id]
            surf.blit(self.viewer.text_cache.render(f"ID: {int(unique_id)}", 16, COLORS["green"]), (x, y))
            y += 24

            agent_data = self.registry.agent_data[slot_id]
            pos = (int(agent_data[COL_X].item()), int(agent_data[COL_Y].item()))
            hp, hp_max = float(agent_data[COL_HP].item()), float(agent_data[COL_HP_MAX].item())
            atk, vision = float(agent_data[COL_ATK].item()), float(agent_data[COL_VISION].item())
            hp_ratio = (hp / hp_max) if hp_max > 0 else 0.0

            unit_val = float(agent_data[COL_UNIT].item())
            unit_name = "Archer" if unit_val == 2.0 else "Soldier"

            brain = self.registry.brains[slot_id]
            brain_name = type(brain).__name__ if brain is not None else "<none>"

            agent_score = self.viewer.agent_scores.get(int(unique_id), 0.0)

            surf.blit(self.viewer.text_cache.render(f"Score: {agent_score:.2f}", 13, COLORS["text_dim"]), (x, y)); y += 18
            surf.blit(self.viewer.text_cache.render(f"Pos: ({pos[0]}, {pos[1]})", 13, COLORS["text_dim"]), (x, y)); y += 18
            surf.blit(self.viewer.text_cache.render(f"Unit: {unit_name}", 13, COLORS["text_dim"]), (x, y)); y += 18
            surf.blit(self.viewer.text_cache.render(f"Brain: {brain_name}", 13, COLORS["text_dim"]), (x, y)); y += 18

            bar_w = srect.width - 2 * pad
            pygame.draw.rect(surf, COLORS["bar_bg"], (x, y, bar_w, 10))
            pygame.draw.rect(surf, COLORS["bar_fg"], (x, y, bar_w * hp_ratio, 10))
            y += 14

            surf.blit(self.viewer.text_cache.render(f"HP: {hp:.2f} / {hp_max:.2f}", 13, COLORS["text_dim"]), (x, y)); y += 20
            surf.blit(self.viewer.text_cache.render(f"Attack: {atk:.2f}", 13, COLORS["text_dim"]), (x, y)); y += 18
            surf.blit(self.viewer.text_cache.render(f"Vision: {vision} cells", 13, COLORS["text_dim"]), (x, y)); y += 22

            if brain is not None:
                surf.blit(self.viewer.text_cache.render(f"Model: {_get_model_summary(brain)}", 13, COLORS["text_dim"]), (x, y)); y += 18
                surf.blit(self.viewer.text_cache.render(f"Params: {_param_count(brain):,}", 13, COLORS["text_dim"]), (x, y)); y += 18

        pygame.draw.line(surf, COLORS["border"], (srect.x, y + 10), (srect.right, y + 10), 2)
        self._draw_legend(surf, x, y)


class Minimap:
    def __init__(self, viewer: "Viewer") -> None:
        self.viewer = viewer
        self.grid_w = viewer.grid.shape[2]
        self.grid_h = viewer.grid.shape[1]

    def draw(self, surf: pygame.Surface, hud_rect: pygame.Rect, state_data: dict) -> None:
        map_w = 200
        map_h = int(map_w * (self.grid_h / self.grid_w))
        map_rect = pygame.Rect(hud_rect.right - map_w - 20, hud_rect.y + (hud_rect.height - map_h) // 2, map_w, map_h)
        surf.fill(COLORS["empty"], map_rect)

        for x, y, _unit, team, _uid, _btype in state_data["agent_map"].values():
            dot_x = map_rect.x + int(float(x) / self.grid_w * map_w)
            dot_y = map_rect.y + int(float(y) / self.grid_h * map_h)
            color = COLORS["red"] if team == 2.0 else COLORS["blue"]
            pygame.draw.rect(surf, color, (dot_x, dot_y, 2, 2))

        if self.viewer.cam.cell_px > 0:
            cam_rect_world = pygame.Rect(
                self.viewer.cam.offset_x,
                self.viewer.cam.offset_y,
                self.viewer.layout.world_rect().width / self.viewer.cam.cell_px,
                self.viewer.layout.world_rect().height / self.viewer.cam.cell_px,
            )
            cam_rect_map = pygame.Rect(
                map_rect.x + (cam_rect_world.x / self.grid_w * map_w),
                map_rect.y + (cam_rect_world.y / self.grid_h * map_h),
                (cam_rect_world.width / self.grid_w * map_w),
                (cam_rect_world.height / self.grid_h * map_h),
            )
            pygame.draw.rect(surf, COLORS["marker"], cam_rect_map, 1)

        pygame.draw.rect(surf, COLORS["border"], map_rect, 1)


# -----------------------------------------------------------------------------
# Input handling
# -----------------------------------------------------------------------------
class InputHandler:
    """
    Processes:
    - continuous key states (WASD/Arrows panning)
    - discrete KEYDOWN events (space pause, F5 save, etc.)
    - mouse picking & zoom
    """

    def __init__(self, viewer: "Viewer") -> None:
        self.viewer = viewer

    def handle(self):
        running = True
        advance_tick = False

        # We will compute continuous pan AFTER processing discrete events,
        # so we can "consume" S if it was used for saving in this frame.
        keys = pygame.key.get_pressed()
        mods = pygame.key.get_mods()
        ctrl_down = bool(mods & pygame.KMOD_CTRL)

        # If user pressed "S" to save brain this frame, we don't also pan down.
        s_consumed_for_save = False

        wrect = self.viewer.layout.world_rect()

        for ev in pygame.event.get():
            if ev.type == pygame.QUIT:
                running = False

            elif ev.type == pygame.VIDEORESIZE:
                # Only allow resize in windowed mode.
                if not self.viewer.fullscreen:
                    self.viewer.Wpix, self.viewer.Hpix = max(800, ev.w), max(520, ev.h)
                    self.viewer.screen = pygame.display.set_mode((self.viewer.Wpix, self.viewer.Hpix), pygame.RESIZABLE)
                    self.viewer.world_renderer.static_surf = None  # force static redraw

            elif ev.type == pygame.KEYDOWN:
                # ----------------------------------------------------------
                # Global quit
                # ----------------------------------------------------------
                if ev.key == pygame.K_ESCAPE:
                    running = False

                # ----------------------------------------------------------
                # Pause toggle (FIX #1)
                # ----------------------------------------------------------
                elif ev.key == pygame.K_SPACE:
                    self.viewer.paused = not self.viewer.paused

                # ----------------------------------------------------------
                # Manual checkpoint hotkey (F5)
                # ----------------------------------------------------------
                elif ev.key == pygame.K_F5:
                    self.viewer.request_full_checkpoint("F5")

                # ----------------------------------------------------------
                # Save (Ctrl+S) -> checkpoint, plain S -> save selected brain
                # ----------------------------------------------------------
                elif ev.key == pygame.K_s:
                    if getattr(ev, "mod", 0) & pygame.KMOD_CTRL:
                        self.viewer.request_full_checkpoint("Ctrl+S")
                    else:
                        s_consumed_for_save = True
                        self.viewer.save_selected_brain()

                # ----------------------------------------------------------
                # Step one tick while paused
                # ----------------------------------------------------------
                elif ev.key == pygame.K_PERIOD and self.viewer.paused:
                    advance_tick = True

                # ----------------------------------------------------------
                # Mark agent for tracking
                # ----------------------------------------------------------
                elif ev.key == pygame.K_m and self.viewer.selected_slot_id is not None:
                    if self.viewer.selected_slot_id in self.viewer.marked:
                        self.viewer.marked.remove(self.viewer.selected_slot_id)
                    elif len(self.viewer.marked) < 10:
                        self.viewer.marked.append(self.viewer.selected_slot_id)

                # ----------------------------------------------------------
                # Toggles
                # ----------------------------------------------------------
                elif ev.key == pygame.K_r:
                    self.viewer.show_rays = not self.viewer.show_rays
                elif ev.key == pygame.K_t:
                    self.viewer.threat_vision_mode = not self.viewer.threat_vision_mode
                elif ev.key == pygame.K_b:
                    self.viewer.battle_view_enabled = not self.viewer.battle_view_enabled
                elif ev.key == pygame.K_n:
                    self.viewer.show_brain_types = not self.viewer.show_brain_types

                # ----------------------------------------------------------
                # Fullscreen toggle
                # ----------------------------------------------------------
                elif ev.key == pygame.K_F11:
                    self.viewer.fullscreen = not self.viewer.fullscreen
                    if self.viewer.fullscreen:
                        self.viewer.screen = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
                    else:
                        self.viewer.screen = pygame.display.set_mode((self.viewer.Wpix, self.viewer.Hpix), pygame.RESIZABLE)
                    self.viewer.world_renderer.static_surf = None

                # ----------------------------------------------------------
                # Speed multiplier
                # ----------------------------------------------------------
                elif ev.key in (pygame.K_EQUALS, pygame.K_PLUS):
                    self.viewer.speed_multiplier = min(16, self.viewer.speed_multiplier * 2)
                elif ev.key == pygame.K_MINUS:
                    self.viewer.speed_multiplier = max(0.25, self.viewer.speed_multiplier / 2)

            elif ev.type == pygame.MOUSEBUTTONDOWN:
                # Left click -> select agent
                if ev.button == 1 and wrect.collidepoint(ev.pos):
                    gx, gy = self.viewer.cam.screen_to_world(ev.pos[0] - wrect.x, ev.pos[1] - wrect.y)
                    slot_id = self.viewer.fast_grid_pick_slot(gx, gy)
                    self.viewer.selected_slot_id = slot_id if slot_id is not None and slot_id >= 0 else None
                    if self.viewer.selected_slot_id is not None:
                        # Reading a single .item() is fine (only on click).
                        self.viewer.last_selected_uid = int(
                            self.viewer.registry.agent_data[self.viewer.selected_slot_id, COL_AGENT_ID].item()
                        )

                # Mouse wheel zoom
                elif ev.button == 4:
                    self.viewer.cam.zoom_at(1.12)
                    self.viewer.world_renderer.static_surf = None
                elif ev.button == 5:
                    self.viewer.cam.zoom_at(1 / 1.12)
                    self.viewer.world_renderer.static_surf = None

        # ---------------------------------------------------------------------
        # Continuous camera pan (WASD + arrows)
        # ---------------------------------------------------------------------
        pan_speed = 10.0 / max(1.0, float(self.viewer.cam.cell_px))
        if keys[pygame.K_a] or keys[pygame.K_LEFT]:
            self.viewer.cam.pan(-pan_speed, 0)
        if keys[pygame.K_d] or keys[pygame.K_RIGHT]:
            self.viewer.cam.pan(pan_speed, 0)
        if keys[pygame.K_w] or keys[pygame.K_UP]:
            self.viewer.cam.pan(0, -pan_speed)

        # Down pan uses S or DOWN, BUT:
        # - do not pan if CTRL is held (because Ctrl+S is checkpoint)
        # - do not pan if we consumed S for "save brain" this frame
        if (keys[pygame.K_s] or keys[pygame.K_DOWN]) and (not ctrl_down) and (not s_consumed_for_save):
            self.viewer.cam.pan(0, pan_speed)

        return running, advance_tick


# -----------------------------------------------------------------------------
# Animation (kept)
# -----------------------------------------------------------------------------
class AnimationManager:
    def __init__(self) -> None:
        self.animations = []

    def add(self, anim_type, pos) -> None:
        self.animations.append([anim_type, pos, 10])

    def update(self) -> None:
        self.animations = [[t, p, l - 1] for t, p, l in self.animations if l > 0]

    def draw(self, surf: pygame.Surface, wrect: pygame.Rect, cam: Camera) -> None:
        c = cam.cell_px
        for anim_type, pos, lifetime in self.animations:
            cx, cy = cam.world_to_screen(pos[0], pos[1])
            alpha = int(255 * (lifetime / 10))
            if anim_type == "damage":
                pygame.draw.circle(
                    surf,
                    (*COLORS["red"], alpha),
                    (wrect.x + cx + c // 2, wrect.y + cy + c // 2),
                    c // 2,
                    2,
                )


# -----------------------------------------------------------------------------
# Viewer (Optimized)
# -----------------------------------------------------------------------------
class Viewer:
    """
    Main speed fixes:
    - Do NOT do grid.detach().cpu() every frame (GPU sync). We refresh state every N frames.
    - Do NOT call .item() for every field for every alive agent each frame.
      We bulk-copy agent columns to CPU when we refresh state.
    - Keep UI smooth at TARGET_FPS for screen recording.
    """

    def request_full_checkpoint(self, reason: str = "hotkey") -> None:
        """
        UI-visible entry point to trigger a full simulation checkpoint.

        main.py attaches:
            engine._manual_checkpoint = callable

        So Viewer can call it without importing checkpoint internals.
        """
        eng = getattr(self, "_engine_ref", None)
        fn = getattr(eng, "_manual_checkpoint", None) if eng is not None else None

        if not callable(fn):
            print("[Viewer] No manual checkpoint hook attached to engine (engine._manual_checkpoint missing).")
            return

        try:
            fn(reason)
        except TypeError:
            # Some older versions might not accept a reason argument.
            fn()

    def __init__(self, grid: torch.Tensor, cell_size: Optional[int] = None, show_grid: bool = True) -> None:
        _center_window()
        pygame.init()
        pygame.display.set_caption("Codex Bellum - Transformer")

        self.grid, self.margin, self.show_grid = grid, 8, show_grid
        self.cam = Camera(int(cell_size or config.CELL_SIZE), grid.shape[2], grid.shape[1])

        H, W = grid.shape[1], grid.shape[2]
        side_min_w, hud_h = 280, 126

        # Prevent too-large windows (use current display bounds).
        try:
            display_info = pygame.display.Info()
            max_w = display_info.current_w - 80
            max_h = display_info.current_h - 120
        except pygame.error:
            max_w, max_h = 1280, 720

        world_px_w, world_px_h = W * self.cam.cell_px, H * self.cam.cell_px
        init_w = min(max_w, max(1280, world_px_w + side_min_w + 3 * self.margin))
        init_h = min(max_h, max(720, world_px_h + hud_h + 2 * self.margin))

        self.Wpix, self.Hpix = int(init_w), int(init_h)
        self.screen = pygame.display.set_mode((self.Wpix, self.Hpix), pygame.RESIZABLE)

        self.text_cache = TextCache()
        self.clock = pygame.time.Clock()

        # Selection / UI state
        self.selected_slot_id: Optional[int] = None
        self.last_selected_uid: Optional[int] = None
        self.marked: List[int] = []

        # Toggles
        self.show_rays = False
        self.paused = False
        self.threat_vision_mode = False
        self.battle_view_enabled = False
        self.show_brain_types = False
        self.fullscreen = False
        self.speed_multiplier = 1.0

        # Per-agent rolling score for inspector display (updated from PPO hook)
        self.agent_scores: Dict[int, float] = collections.defaultdict(float)

        # ---- Performance knobs ----
        self.STATE_REFRESH_EVERY_FRAMES = int(getattr(config, "VIEWER_STATE_REFRESH_EVERY", 2))
        self.PICK_REFRESH_EVERY_FRAMES = int(getattr(config, "VIEWER_PICK_REFRESH_EVERY", 2))

        # Cached CPU state for rendering & picking
        self._cached_state_data = None
        self._cached_id_np = None
        self._cached_occ_np = None
        self._cached_alive_indices: List[int] = []
        self._cached_agent_map: Dict[int, tuple] = {}

        self._last_state_refresh_frame = -10
        self._last_pick_refresh_frame = -10

    def save_selected_brain(self) -> None:
        """
        Saves the selected agent's brain (model parameters) only.
        This is NOT a full simulation checkpoint.
        """
        if self.selected_slot_id is None or not hasattr(self, "registry"):
            return

        brain = self.registry.brains[self.selected_slot_id]
        if not brain:
            return

        tick = self.stats.tick if hasattr(self, "stats") else 0
        uid = self.last_selected_uid if self.last_selected_uid is not None else self.selected_slot_id
        filename = f"brain_agent_{uid}_t_{tick}.pth"

        try:
            torch.save(brain.state_dict(), filename)
            print(f"[Viewer] Saved brain for agent {uid} -> '{filename}'")
        except Exception as e:
            print(f"[Viewer] Error saving brain: {e}")

    # -------------------------------------------------------------------------
    # Fast click picking (uses cached id grid, avoids GPU sync)
    # -------------------------------------------------------------------------
    def fast_grid_pick_slot(self, gx: int, gy: int) -> Optional[int]:
        if self._cached_id_np is None:
            # Fallback: a slow GPU->CPU sync (avoid if possible)
            try:
                v = int(self.grid[2, gy, gx].item())
                return v if v >= 0 else None
            except Exception:
                return None

        if 0 <= gy < self._cached_id_np.shape[0] and 0 <= gx < self._cached_id_np.shape[1]:
            v = int(self._cached_id_np[gy, gx])
            return v if v >= 0 else None

        return None

    # -------------------------------------------------------------------------
    # State refresh (bulk copies)
    # -------------------------------------------------------------------------
    def _refresh_state_cpu(self) -> None:
        """
        One refresh does:
        - grid channels 0 and 2 -> CPU numpy
        - alive indices -> list[int]
        - bulk-copy agent columns for alive -> build agent_map (including brain type)
        """
        with torch.no_grad():
            # This is the expensive sync; we do it only every N frames.
            grid_cpu = self.grid.detach().cpu()
            occ_np = grid_cpu[0].numpy().astype(np.int16, copy=False)
            id_np = grid_cpu[2].numpy().astype(np.int32, copy=False)

            ad = self.registry.agent_data
            alive_mask = ad[:, COL_ALIVE] > 0.5
            alive_idx_t = torch.nonzero(alive_mask).squeeze(1)

            if alive_idx_t.numel() == 0:
                alive_indices: List[int] = []
                agent_map: Dict[int, tuple] = {}
            else:
                alive_indices = alive_idx_t.cpu().tolist()

                # Only columns needed by UI
                cols = [COL_X, COL_Y, COL_UNIT, COL_TEAM, COL_AGENT_ID]
                alive_data = ad.index_select(0, alive_idx_t)[:, cols].detach().cpu().numpy()
                brains = self.registry.brains

                agent_map = {}
                for k, slot_id in enumerate(alive_indices):
                    x, y, unit, team, uid = alive_data[k]
                    br = brains[slot_id]

                    # Tiny label for brain type.
                    if br is None:
                        btype = "?"
                    else:
                        name = type(br).__name__
                        if "Tron" in name:
                            btype = "T"
                        elif "Mirror" in name:
                            btype = "M"
                        elif "Transformer" in name:
                            btype = "Tr"
                        else:
                            btype = "U"

                    agent_map[slot_id] = (float(x), float(y), float(unit), float(team), float(uid), btype)

            self._cached_occ_np = occ_np
            self._cached_id_np = id_np
            self._cached_alive_indices = alive_indices
            self._cached_agent_map = agent_map

            self._cached_state_data = {
                "occ_np": self._cached_occ_np,
                "id_np": self._cached_id_np,
                "alive_indices": self._cached_alive_indices,
                "agent_map": self._cached_agent_map,
            }

    # -------------------------------------------------------------------------
    # PPO score hook (tracks per-agent reward for inspector panel)
    # -------------------------------------------------------------------------
    def _install_score_hook(self, engine, registry) -> None:
        if not hasattr(engine, "_ppo") or engine._ppo is None:
            return

        original_record_step = engine._ppo.record_step

        def record_step_with_score_tracking(*args, **kwargs):
            agent_ids = kwargs.get("agent_ids")
            rewards = kwargs.get("rewards")

            if agent_ids is not None and rewards is not None and agent_ids.numel() > 0:
                with torch.no_grad():
                    slot_ids = agent_ids.detach()
                    uids = registry.agent_data.index_select(0, slot_ids)[:, COL_AGENT_ID].detach().cpu().numpy()
                    r = rewards.detach().cpu().numpy()

                for uid, rv in zip(uids, r):
                    self.agent_scores[int(uid)] += float(rv)

            return original_record_step(*args, **kwargs)

        engine._ppo.record_step = record_step_with_score_tracking

    # -------------------------------------------------------------------------
    # Main UI loop
    # -------------------------------------------------------------------------
    def run(self, engine, registry, stats, tick_limit: int = 0, target_fps: Optional[int] = None) -> None:
        self.registry, self.stats = registry, stats
        self._engine_ref = engine

        self.layout = LayoutManager(self)
        self.world_renderer = WorldRenderer(self, self.grid, registry)
        self.hud_panel = HudPanel(self, stats)
        self.side_panel = SidePanel(self, registry)
        self.input_handler = InputHandler(self)
        self.anim_manager = AnimationManager()
        self.minimap = Minimap(self)

        self.world_renderer.build_static_cache(engine)
        self._install_score_hook(engine, registry)

        running = True
        frame_count = 0

        # Prime cache once so first frame and click picking are fast.
        self._refresh_state_cpu()
        self._last_state_refresh_frame = frame_count
        self._last_pick_refresh_frame = frame_count

        while running:
            # Input first.
            running, advance_tick = self.input_handler.handle()

            # Decide how many sim ticks to run this frame.
            num_ticks_this_frame = 0

            if not self.paused:
                if self.speed_multiplier >= 1:
                    num_ticks_this_frame = int(self.speed_multiplier)
                elif self.speed_multiplier > 0:
                    denom = int(1 / self.speed_multiplier)
                    if denom <= 1 or (frame_count % denom) == 0:
                        num_ticks_this_frame = 1
            elif advance_tick:
                num_ticks_this_frame = 1

            # Execute sim ticks.
            for _ in range(num_ticks_this_frame):
                engine.run_tick()
                self.hud_panel.update()

            # Refresh CPU render state sometimes (big performance win).
            if (frame_count - self._last_state_refresh_frame) >= self.STATE_REFRESH_EVERY_FRAMES:
                self._refresh_state_cpu()
                self._last_state_refresh_frame = frame_count

            if (frame_count - self._last_pick_refresh_frame) >= self.PICK_REFRESH_EVERY_FRAMES:
                self._last_pick_refresh_frame = frame_count

            state_data = self._cached_state_data
            if state_data is None:
                self._refresh_state_cpu()
                state_data = self._cached_state_data

            # Render.
            self.screen.fill(COLORS["bg"])
            self.world_renderer.draw(self.screen, state_data)
            self.hud_panel.draw(self.screen, state_data)
            self.side_panel.draw(self.screen, state_data)
            pygame.display.flip()

            # Rendering FPS cap (sim speed is controlled by speed_multiplier and engine tick rate).
            self.clock.tick(int(target_fps or config.TARGET_FPS))

            frame_count += 1
            if tick_limit > 0 and stats.tick >= tick_limit:
                running = False

        pygame.quit()

====[ END war_simulation\ui\viewer.py ]============================================================


====[ 32/35 | war_simulation\utils\checkpoint.py ]============================================
# utils/checkpoint.py
from __future__ import annotations

"""
Checkpointing utilities for "close program â†’ resume later" continuity.

We store a *full snapshot* of simulation state:
  - grid tensor
  - zones tensor (optional)
  - full agent registry tensor (positions, hp, alive flags, etc.)
  - per-slot brain weights (state_dict per slot)
  - simulation stats (tick, per-team totals, etc.)
  - continuation-only engine state (respawn cooldowns, agent_scores, etc.)
  - RNG states (python / numpy / torch CPU / torch CUDA)

âš ï¸ SECURITY:
  This checkpoint is a Python pickle-based payload saved via torch.save().
  Loading requires torch.load(weights_only=False) on PyTorch 2.6+.
  Only load checkpoints you created / trust.
"""

import os
import time
import random
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, Union, List

import torch
import numpy as np

import config
from engine.agent_registry import AgentsRegistry
from simulation.stats import SimulationStats
from engine.tick import TickEngine


# =============================================================================
# Versioning
# =============================================================================

# Increment this if you make *breaking* format changes.
CHECKPOINT_FORMAT_VERSION: int = 1


# =============================================================================
# PyTorch loader compatibility (PyTorch 2.6+ "weights_only" default changed)
# =============================================================================

def _torch_load_checkpoint(path: Path) -> Any:
    """
    Load a checkpoint payload saved by torch.save(payload, path).

    PyTorch 2.6 changed torch.load default:
      - weights_only=True by default (safer, but rejects arbitrary python objects)
    Our payload contains numpy RNG tuples, python RNG state, lists, dicts, etc.
    Therefore we must load with weights_only=False to resume *full state*.

    SECURITY WARNING:
      weights_only=False can execute arbitrary code during unpickling.
      Only load checkpoints you created / trust.
    """
    try:
        # PyTorch >= 2.6 supports weights_only kwarg
        return torch.load(path, map_location="cpu", weights_only=False)
    except TypeError:
        # Older PyTorch: no weights_only kwarg
        return torch.load(path, map_location="cpu")


# =============================================================================
# RNG capture/restore
# =============================================================================

def _capture_rng_state() -> Dict[str, Any]:
    """
    Capture RNG state for maximum continuity on resume.

    Returns a dict containing:
      - python random module state
      - numpy RNG state (legacy RandomState)
      - torch CPU RNG state
      - torch CUDA RNG state (all devices) if available
    """
    st: Dict[str, Any] = {}

    # Python built-in RNG
    try:
        st["python"] = random.getstate()
    except Exception:
        st["python"] = None

    # NumPy RNG (RandomState)
    # NOTE: np.random.get_state() returns a tuple including numpy arrays -> pickle needed.
    try:
        st["numpy"] = np.random.get_state()
    except Exception:
        st["numpy"] = None

    # Torch CPU RNG
    try:
        st["torch_cpu"] = torch.random.get_rng_state()
    except Exception:
        st["torch_cpu"] = None

    # Torch CUDA RNG (all GPU devices)
    try:
        if torch.cuda.is_available():
            st["torch_cuda"] = torch.cuda.get_rng_state_all()
        else:
            st["torch_cuda"] = None
    except Exception:
        st["torch_cuda"] = None

    return st


def _restore_rng_state(st: Dict[str, Any]) -> None:
    """
    Restore RNG state captured by _capture_rng_state().

    Best-effort:
      - Any failures are swallowed because RNG restore should not crash resume.
    """
    # Python RNG
    try:
        if st.get("python") is not None:
            random.setstate(st["python"])
    except Exception:
        pass

    # NumPy RNG
    try:
        if st.get("numpy") is not None:
            np.random.set_state(st["numpy"])
    except Exception:
        pass

    # Torch CPU RNG
    try:
        if st.get("torch_cpu") is not None:
            torch.random.set_rng_state(st["torch_cpu"])
    except Exception:
        pass

    # Torch CUDA RNG
    try:
        if st.get("torch_cuda") is not None and torch.cuda.is_available():
            torch.cuda.set_rng_state_all(st["torch_cuda"])
    except Exception:
        pass


# =============================================================================
# Pack/unpack registry (agents + brains)
# =============================================================================

def _pack_registry(registry: AgentsRegistry) -> Dict[str, Any]:
    """
    Convert AgentsRegistry into a fully serializable payload.

    We treat registry.agent_data as canonical state and save it as a CPU tensor.
    For brains, we save a per-slot state_dict list, preserving "None" slots.
    """
    payload: Dict[str, Any] = {
        # Capacity helps sanity-check loads (optional but useful).
        "capacity": int(getattr(registry, "capacity", 0)),
        # next_agent_id ensures no ID collisions after resume.
        "next_agent_id": int(getattr(registry, "next_agent_id", 0)),
        # Main agent state tensor
        "agent_data": registry.agent_data.detach().cpu(),
        # Optional generations / metadata if your registry tracks it
        "generations": getattr(registry, "generations", None),
    }

    # Save brains (state_dict per slot)
    brains_sd: List[Optional[Dict[str, Any]]] = []
    if hasattr(registry, "brains"):
        for b in registry.brains:
            if b is None:
                brains_sd.append(None)
            else:
                # state_dict contains tensors -> safe and standard
                brains_sd.append(b.state_dict())
    payload["brains_state_dicts"] = brains_sd

    return payload


def _unpack_registry(
    payload: Dict[str, Any],
    *,
    grid: torch.Tensor,
    device: torch.device,
    strict: bool,
) -> AgentsRegistry:
    """
    Recreate a registry and load saved agent_data + brains.

    Assumptions:
      - AgentsRegistry(grid) builds a registry with the correct shapes.
      - We then overwrite reg.agent_data with checkpoint data.
      - Brain objects either exist per slot, or the registry provides a factory:
            reg._make_brain_for_slot(i)
    """
    # Construct a fresh registry matching runtime code
    reg = AgentsRegistry(grid)

    # --- Restore agent_data tensor (shape must match) ---
    agent_data = payload["agent_data"].to(device)

    if reg.agent_data.shape != agent_data.shape:
        raise RuntimeError(
            "Checkpoint agent_data shape mismatch:\n"
            f"  ckpt={tuple(agent_data.shape)}\n"
            f"  runtime={tuple(reg.agent_data.shape)}\n"
            "This usually means config changed (grid size, max agents, columns) between save and resume."
        )

    # Copy data (in-place) for efficiency
    reg.agent_data.copy_(agent_data)

    # --- Restore optional fields ---
    if payload.get("generations", None) is not None and hasattr(reg, "generations"):
        try:
            reg.generations = payload["generations"]
        except Exception:
            pass

    if "next_agent_id" in payload:
        try:
            reg.next_agent_id = int(payload["next_agent_id"])
        except Exception:
            pass

    # --- Restore brains slot-by-slot ---
    brains_sd = payload.get("brains_state_dicts", None)

    if brains_sd is not None and hasattr(reg, "brains"):
        # Ensure list lengths align
        max_slots = min(len(reg.brains), len(brains_sd))

        for i in range(max_slots):
            sd = brains_sd[i]

            # If checkpoint slot is empty, keep it empty
            if sd is None:
                reg.brains[i] = None
                continue

            # Ensure there is a brain instance to load into
            if reg.brains[i] is None:
                # Try registry-provided slot factory if present
                try:
                    reg.brains[i] = reg._make_brain_for_slot(i)  # type: ignore[attr-defined]
                except Exception:
                    # If no factory exists, we cannot load this slot
                    reg.brains[i] = None

            if reg.brains[i] is None:
                # Best effort: skip if we couldn't build a brain instance
                continue

            # Load weights
            try:
                reg.brains[i].load_state_dict(sd, strict=strict)
            except Exception:
                if strict:
                    raise
                # Non-strict fallback: allow partial key matches
                try:
                    reg.brains[i].load_state_dict(sd, strict=False)
                except Exception:
                    pass

    return reg


# =============================================================================
# Pack/unpack SimulationStats
# =============================================================================

def _pack_stats(stats: SimulationStats) -> Dict[str, Any]:
    """
    Pack SimulationStats into primitives, avoiding heavy/non-serializable objects.

    We store:
      - tick (always)
      - top-level primitive fields from vars(stats)
      - team stats as raw dicts via vars(stats.red) / vars(stats.blue)
    """
    d: Dict[str, Any] = {"tick": int(getattr(stats, "tick", 0))}

    # Store top-level primitives only
    for k, v in vars(stats).items():
        # Skip known heavy / unsafe structures
        if k in ("dead_log",):
            continue
        try:
            if isinstance(v, (int, float, str, bool, type(None))):
                d[k] = v
        except Exception:
            pass

    # Store team stats (best effort)
    if hasattr(stats, "red"):
        try:
            d["red"] = dict(vars(stats.red))
        except Exception:
            d["red"] = {}
    if hasattr(stats, "blue"):
        try:
            d["blue"] = dict(vars(stats.blue))
        except Exception:
            d["blue"] = {}

    return d


def _unpack_stats(payload: Dict[str, Any]) -> SimulationStats:
    """
    Restore SimulationStats from packed payload.

    Best-effort:
      - Missing keys are ignored
      - Unknown keys are ignored if stats object doesn't have them
    """
    st = SimulationStats()

    # Restore tick
    if "tick" in payload:
        try:
            st.tick = int(payload["tick"])
        except Exception:
            pass

    # Helper to fill objects from dict
    def _fill(obj: Any, src: Dict[str, Any]) -> None:
        for k, v in src.items():
            try:
                setattr(obj, k, v)
            except Exception:
                pass

    # Restore team blocks
    if "red" in payload and hasattr(st, "red") and isinstance(payload["red"], dict):
        _fill(st.red, payload["red"])
    if "blue" in payload and hasattr(st, "blue") and isinstance(payload["blue"], dict):
        _fill(st.blue, payload["blue"])

    # Restore remaining top-level fields (skip team dicts)
    for k, v in payload.items():
        if k in ("red", "blue"):
            continue
        try:
            setattr(st, k, v)
        except Exception:
            pass

    return st


# =============================================================================
# Path resolution (UX helper)
# =============================================================================

def resolve_checkpoint_path(path: Union[str, Path]) -> Path:
    """
    Resolve a user-supplied checkpoint path.

    Supports:
      1) Direct file path to *.pt (recommended)
      2) Directory containing ckpt_latest.pt
      3) Directory containing subfolders with ckpt_latest.pt (picks newest)
      4) Fallback to newest ckpt_XXXXXXXXX.pt under a directory

    This makes resume UX nicer on Windows.
    """
    p = Path(path).expanduser()

    # If it's already a file, we are done
    if p.is_file():
        return p

    # If it's a directory, try common patterns
    if p.is_dir():
        direct = p / "ckpt_latest.pt"
        if direct.is_file():
            return direct

        # Search recursively for ckpt_latest.pt and pick the newest
        candidates = list(p.glob("**/ckpt_latest.pt"))
        if candidates:
            candidates.sort(key=lambda q: q.stat().st_mtime, reverse=True)
            return candidates[0]

        # Fallback: find any rotated ckpt_*.pt and pick the highest tick if possible
        rotated = [q for q in p.glob("**/ckpt_*.pt") if q.name != "ckpt_latest.pt"]
        if rotated:
            def _tick_from_name(q: Path) -> int:
                # expects ckpt_000000123.pt â†’ tick 123
                try:
                    return int(q.stem.split("_")[-1])
                except Exception:
                    return -1
            rotated.sort(key=_tick_from_name, reverse=True)
            return rotated[0]

    raise FileNotFoundError(f"Checkpoint not found: {p}")


# =============================================================================
# Engine continuation state application
# =============================================================================

def apply_engine_state(
    engine: TickEngine,
    registry: AgentsRegistry,
    stats: SimulationStats,
    engine_state: Dict[str, Any],
) -> None:
    """
    Apply continuation-only engine state that isn't captured by grid/registry/stats alone.

    Examples:
      - respawn cooldown tables
      - per-agent accumulated scores stored on engine
    """
    if not engine_state:
        return

    # Restore respawner internals
    if "respawn" in engine_state and hasattr(engine, "respawner"):
        try:
            engine.respawner.load_state_dict(engine_state["respawn"])
        except Exception:
            pass

    # Restore per-slot agent scores tensor if present
    if "agent_scores" in engine_state and hasattr(engine, "agent_scores"):
        try:
            t = torch.tensor(engine_state["agent_scores"], device=registry.agent_data.device)
            if engine.agent_scores.shape == t.shape:
                engine.agent_scores.copy_(t)
        except Exception:
            pass


# =============================================================================
# Public API: save_checkpoint / load_checkpoint
# =============================================================================

def save_checkpoint(
    out_dir: Union[str, Path],
    *,
    engine: TickEngine,
    registry: AgentsRegistry,
    grid: torch.Tensor,
    stats: SimulationStats,
    zones: Optional[torch.Tensor] = None,
    resume_from: str = "",
    keep_last: int = 2,
) -> Path:
    """
    Save a checkpoint into out_dir.

    Writes:
      - ckpt_latest.pt (always)
      - optionally rotates the previous ckpt_latest.pt into ckpt_XXXXXXXXX.pt

    Returns:
      Path to ckpt_latest.pt

    Notes:
      - This uses a temp file + atomic replace to reduce chance of partial writes.
    """
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    latest = out_dir / "ckpt_latest.pt"
    tmp = out_dir / "ckpt_tmp.pt"

    # ---- Build payload (pure python dict + tensors on CPU) ----
    payload: Dict[str, Any] = {}

    # Format metadata
    payload["format_version"] = CHECKPOINT_FORMAT_VERSION
    payload["created_utc"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    payload["resume_from"] = str(resume_from or "")
    payload["torch_version"] = getattr(torch, "__version__", "")
    payload["numpy_version"] = getattr(np, "__version__", "")

    # Optional: record config summary for debugging mismatches
    payload["config_summary"] = config.summary_str() if hasattr(config, "summary_str") else ""

    # Core tensors
    payload["grid"] = grid.detach().cpu()
    payload["zones"] = zones.detach().cpu() if zones is not None else None

    # Registry (agents + brains)
    payload["registry"] = _pack_registry(registry)

    # Stats
    payload["stats"] = _pack_stats(stats)

    # Continuation-only engine state
    eng_state: Dict[str, Any] = {}
    if hasattr(engine, "respawner"):
        try:
            eng_state["respawn"] = engine.respawner.state_dict()
        except Exception:
            pass
    if hasattr(engine, "agent_scores"):
        try:
            eng_state["agent_scores"] = engine.agent_scores.detach().cpu().tolist()
        except Exception:
            pass
    payload["engine_state"] = eng_state

    # RNG state
    payload["rng"] = _capture_rng_state()

    # ---- Write to temp first (reduces corruption risk) ----
    try:
        if tmp.exists():
            tmp.unlink()
    except Exception:
        pass

    torch.save(payload, tmp)

    # ---- Rotate old latest (optional) ----
    if keep_last > 0 and latest.exists():
        try:
            tick = int(getattr(stats, "tick", 0))
            rotated = out_dir / f"ckpt_{tick:09d}.pt"

            # Move old latest to rotated name (best effort)
            try:
                latest.replace(rotated)
            except Exception:
                # If replace fails, ignore rotation
                pass

            # Keep only last N rotated (by mtime)
            rotated_files = sorted(out_dir.glob("ckpt_*.pt"), key=lambda p: p.stat().st_mtime)
            if len(rotated_files) > keep_last:
                for f in rotated_files[: len(rotated_files) - keep_last]:
                    try:
                        f.unlink()
                    except Exception:
                        pass
        except Exception:
            pass

    # ---- Promote temp â†’ latest (atomic replace) ----
    try:
        tmp.replace(latest)
    except Exception:
        # As a fallback, try a copy-like behavior (less atomic)
        torch.save(payload, latest)
        try:
            if tmp.exists():
                tmp.unlink()
        except Exception:
            pass

    return latest


def load_checkpoint(
    path: Union[str, Path],
    *,
    device: Union[str, torch.device] = "cpu",
    strict: bool = True,
) -> Tuple[
    torch.Tensor,             # grid
    AgentsRegistry,           # registry
    SimulationStats,          # stats
    Optional[torch.Tensor],   # zones
    Dict[str, Any],           # engine_state
    Dict[str, Any],           # meta
]:
    """
    Load checkpoint and return:
      (grid, registry, stats, zones, engine_state, meta)

    Arguments:
      path:
        - a file path (*.pt), OR
        - a directory (we resolve to ckpt_latest.pt / newest checkpoint)
      device:
        - where to place tensors after loading (grid, zones, registry.agent_data)
      strict:
        - controls brain.load_state_dict(strict=...)
          strict=True will raise on mismatch.

    SECURITY:
      We use torch.load(weights_only=False) on PyTorch 2.6+.
      Only load checkpoints you created / trust.
    """
    # Resolve directory â†’ actual checkpoint file
    ckpt_path = resolve_checkpoint_path(path)

    # Normalize device
    device = torch.device(device)

    # Load payload (CPU first), then move tensors to device
    payload = _torch_load_checkpoint(ckpt_path)

    # Validate payload shape early for clearer errors
    if not isinstance(payload, dict):
        raise RuntimeError(f"Invalid checkpoint payload type: {type(payload)} (expected dict)")

    # Optional version check
    fmt = payload.get("format_version", None)
    if fmt is not None and int(fmt) != CHECKPOINT_FORMAT_VERSION:
        # You can relax this if you add migration logic later
        raise RuntimeError(
            f"Checkpoint format_version mismatch: ckpt={fmt} vs runtime={CHECKPOINT_FORMAT_VERSION}"
        )

    # Minimal metadata (useful for logging)
    meta: Dict[str, Any] = {
        "path": str(ckpt_path),
        "created_utc": payload.get("created_utc"),
        "format_version": payload.get("format_version"),
        "resume_from": payload.get("resume_from", ""),
        "torch_version": payload.get("torch_version", ""),
        "numpy_version": payload.get("numpy_version", ""),
    }

    # Restore tensors
    if "grid" not in payload:
        raise RuntimeError("Checkpoint missing required key: 'grid'")
    grid = payload["grid"].to(device)

    zones = payload.get("zones", None)
    if zones is not None:
        try:
            zones = zones.to(device)
        except Exception:
            zones = None

    # Registry
    if "registry" not in payload:
        raise RuntimeError("Checkpoint missing required key: 'registry'")
    reg_payload = payload["registry"]
    if not isinstance(reg_payload, dict):
        raise RuntimeError(f"Invalid 'registry' payload type: {type(reg_payload)} (expected dict)")

    registry = _unpack_registry(reg_payload, grid=grid, device=device, strict=strict)

    # Stats
    stats_payload = payload.get("stats", {}) or {}
    if not isinstance(stats_payload, dict):
        stats_payload = {}
    stats = _unpack_stats(stats_payload)

    # Engine continuation-only state
    engine_state = payload.get("engine_state", {}) or {}
    if not isinstance(engine_state, dict):
        engine_state = {}

    # RNG restore (best effort)
    rng = payload.get("rng", None)
    if isinstance(rng, dict):
        _restore_rng_state(rng)

    return grid, registry, stats, zones, engine_state, meta

====[ END war_simulation\utils\checkpoint.py ]============================================================


====[ 33/35 | war_simulation\utils\persistence.py ]===========================================
from __future__ import annotations
from dataclasses import dataclass
from multiprocessing import Process, Queue
from typing import Dict, Any, Optional, List
import os, json, csv, time, datetime, queue

# ---- Messages for the writer process ----
@dataclass
class _MsgInit:
    run_dir: str
    config_obj: Dict[str, Any]

@dataclass
class _MsgTickRow:
    row: Dict[str, float]

@dataclass
class _MsgDeaths:
    rows: List[Dict[str, Any]]

@dataclass
class _MsgSaveModel:
    label: str
    state_dict: Dict[str, Any]

class _MsgClose: pass

# ---- Background writer ----
def _writer_loop(q: Queue):
    run_dir = None
    stats_fp = None
    stats_writer = None
    deaths_fp = None
    deaths_writer = None
    try:
        while True:
            try:
                msg = q.get(timeout=0.2)
            except queue.Empty:
                continue
            if isinstance(msg, _MsgInit):
                run_dir = msg.run_dir
                os.makedirs(run_dir, exist_ok=True)
                # config.json
                with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
                    json.dump(msg.config_obj, f, indent=2)

                # open CSVs
                stats_fp = open(os.path.join(run_dir, "stats.csv"), "w", newline="", encoding="utf-8")
                deaths_fp = open(os.path.join(run_dir, "dead_agents_log.csv"), "w", newline="", encoding="utf-8")
                stats_writer = None
                deaths_writer = None

            elif isinstance(msg, _MsgTickRow):
                if stats_writer is None:
                    # header from keys
                    stats_writer = csv.DictWriter(stats_fp, fieldnames=list(msg.row.keys()))
                    stats_writer.writeheader()
                stats_writer.writerow(msg.row)
                stats_fp.flush()

            elif isinstance(msg, _MsgDeaths):
                if not msg.rows:
                    continue
                if deaths_writer is None:
                    deaths_writer = csv.DictWriter(deaths_fp, fieldnames=list(msg.rows[0].keys()))
                    deaths_writer.writeheader()
                deaths_writer.writerows(msg.rows)
                deaths_fp.flush()

            elif isinstance(msg, _MsgSaveModel):
                # save as torch-agnostic JSON (keys and shapes), AND raw .pth if available
                # We cannot import torch here (keep process light). Expect caller to save .pth too if needed.
                meta = {k: (list(v.size()) if hasattr(v, "size") else "tensor") for k, v in msg.state_dict.items()}
                with open(os.path.join(run_dir, f"{msg.label}.state_meta.json"), "w", encoding="utf-8") as f:
                    json.dump(meta, f, indent=2)

            elif isinstance(msg, _MsgClose):
                break
    finally:
        for fp in (stats_fp, deaths_fp):
            try:
                if fp: fp.close()
            except Exception:
                pass

# ---- Public API ----
class ResultsWriter:
    """
    Windows-friendly background writer (multiprocessing.Process).
    Non-blocking calls: init(), write_tick(), write_deaths(), save_model_meta(), close().
    """
    def __init__(self) -> None:
        self.q: Queue = Queue(maxsize=1024)
        self.p: Optional[Process] = None
        self.run_dir: Optional[str] = None

    @staticmethod
    def _timestamp_dir(base: str = "results") -> str:
        ts = datetime.datetime.now().strftime("sim_%Y-%m-%d_%H-%M-%S")
        return os.path.join(base, ts)

    def start(self, config_obj: Dict[str, Any], run_dir: Optional[str] = None) -> str:
        self.run_dir = run_dir or self._timestamp_dir()
        self.p = Process(target=_writer_loop, args=(self.q,), daemon=True)
        self.p.start()
        self.q.put(_MsgInit(run_dir=self.run_dir, config_obj=config_obj))
        return self.run_dir

    def write_tick(self, row: Dict[str, float]) -> None:
        if self.p is None: return
        try: self.q.put_nowait(_MsgTickRow(row=row))
        except queue.Full: pass

    def write_deaths(self, rows: List[Dict[str, Any]]) -> None:
        if self.p is None or not rows: return
        try: self.q.put_nowait(_MsgDeaths(rows=rows))
        except queue.Full: pass

    def save_model_meta(self, label: str, state_dict: Dict[str, Any]) -> None:
        if self.p is None: return
        try: self.q.put_nowait(_MsgSaveModel(label=label, state_dict=state_dict))
        except queue.Full: pass

    def close(self) -> None:
        if self.p is None: return
        try:
            self.q.put(_MsgClose())
            self.p.join(timeout=2.0)
        finally:
            if self.p.is_alive():
                self.p.terminate()
            self.p = None

====[ END war_simulation\utils\persistence.py ]============================================================


====[ 34/35 | war_simulation\utils\profiler.py ]==============================================
from __future__ import annotations
from contextlib import contextmanager
from typing import Optional
import os, shutil, subprocess

import torch

def profiler_enabled() -> bool:
    # opt-in via env var or config flag you can pass down
    return os.getenv("FWS_PROFILE", "0") in {"1", "true", "True"}

@contextmanager
def torch_profiler_ctx(activity_cuda: bool = True, out_dir: str = "prof"):
    """
    Minimal torch.profiler wrapper; creates a chrome trace in out_dir if enabled.
    """
    if not profiler_enabled():
        yield None
        return
    try:
        from torch.profiler import profile, ProfilerActivity
        acts = [ProfilerActivity.CPU]
        if activity_cuda and torch.cuda.is_available():
            acts.append(ProfilerActivity.CUDA)
        os.makedirs(out_dir, exist_ok=True)
        with profile(
            activities=acts,
            schedule=torch.profiler.schedule(wait=2, warmup=2, active=6, repeat=1),
            on_trace_ready=torch.profiler.tensorboard_trace_handler(out_dir),
            record_shapes=False,
            with_stack=False,
            with_flops=False,
        ) as prof:
            yield prof
    finally:
        pass

def nvidia_smi_summary() -> Optional[str]:
    """
    Returns a one-line nvidia-smi util/mem summary if nvidia-smi exists; else None.
    """
    exe = shutil.which("nvidia-smi")
    if exe is None:
        return None
    try:
        out = subprocess.check_output(
            [exe, "--query-gpu=utilization.gpu,memory.used,memory.total,power.draw", "--format=csv,noheader,nounits"],
            stderr=subprocess.DEVNULL,
            timeout=1.0,
        ).decode("utf-8", errors="ignore").strip()
        # Possibly multiple GPUs: return first line
        return out.splitlines()[0] if out else None
    except Exception:
        return None

====[ END war_simulation\utils\profiler.py ]============================================================


====[ 35/35 | war_simulation\utils\sanitize.py ]==============================================
from __future__ import annotations
import torch
import config

def assert_finite_tensor(t: torch.Tensor, name: str) -> None:
    if not torch.isfinite(t).all():
        bad = (~torch.isfinite(t)).sum().item()
        raise RuntimeError(f"{name} contains {bad} non-finite values")

def assert_grid_ok(grid: torch.Tensor) -> None:
    if grid.ndim != 3 or grid.size(0) != 3:
        raise RuntimeError(f"grid shape must be (3,H,W), got {tuple(grid.shape)}")
    if grid.device.type != config.TORCH_DEVICE.type:
        # allow different indices; we only care same *type*
        pass
    assert_finite_tensor(grid, "grid")
    # occupancy must be in {0,1,2,3} (float allowed)
    occ = grid[0]
    if not ((occ >= 0.0) & (occ <= 3.0)).all():
        raise RuntimeError("grid[0] occupancy out of range [0..3]")

def assert_agent_data_ok(data: torch.Tensor) -> None:
    if data.ndim != 2 or data.size(1) < 6:
        raise RuntimeError(f"agent_data must be (N,>=6), got {tuple(data.shape)}")
    assert_finite_tensor(data, "agent_data")
    alive = data[:, 0]
    if not ((alive >= 0.0) & (alive <= 1.0)).all():
        raise RuntimeError("alive flag out of range [0..1]")
    team = data[:, 1]
    ok = (team == 0.0) | (team == 2.0) | (team == 3.0)  # allow 0 for empty rows
    if not ok.all():
        raise RuntimeError("team_id must be 0.0/2.0/3.0")

def runtime_sanity_check(grid: torch.Tensor, agent_data: torch.Tensor) -> None:
    """
    Call this occasionally in long runs to catch corruption early.
    """
    assert_grid_ok(grid)
    assert_agent_data_ok(agent_data)

====[ END war_simulation\utils\sanitize.py ]============================================================
